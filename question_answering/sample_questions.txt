Explain why, values are almost never pushed or popped on the stack

The stack is used to allocate local variables. The stack is grown and shrunk on procedure call or return, respectively. Objects on the stack are addressed relative to the stack pointer and are primarily scalars (single variables) rather than arrays. The stack is used for activation records, not as a stack for evaluating expressions. Hence,  values are almost never pushed or popped on the stack.

Crosscutting Issues: The Role of Compilers 11% 12% 21% Program, lucas, level 1 Compiler lucas, level 0 optimimcf, level 3 zation level mcf, level 2 100% 76% 76% 84% 100% mcf, level 0 0% 40% 80% % of unoptimized instructions executed Branches/Calls Loads/Stores FIGURE 2.26 Change in instruction count for the programs lucas and mcf from the SPEC2000 as compiler optimization levels vary. Level 0 is the same as unoptimized code. Level 1 includes local optimizations, code scheduling, and local register allocation. Level 2 includes global optimizations, loop transformations (software pipelining), and global register allocation. Level 3 adds procedure integration. These experiments were performed on the Alpha compilers. The Impact of Compiler Technology on the Architects Decisions The interaction of compilers and high-level languages significantly affects how programs use an instruction set architecture. There are two important questions: How are variables allocated and addressed? How many registers are needed to allocate variables appropriately? To address these questions, we must look at the three separate areas in which current high-level languages allocate their data: n The stack is used to allocate local variables. The stack is grown and shrunk on procedure call or return, respectively. Objects on the stack are addressed relative to the stack pointer and are primarily scalars (single variables) rather than arrays. The stack is used for activation records, not as a stack for evaluating expressions. Hence, values are almost never pushed or popped on the stack. The global data area is used to allocate statically declared objects, such as global variables and constants. A large percentage of these objects are arrays or other aggregate data structures

Explain why, caches needed to be blocking and to cause all the functional units to stall

Early VLIWs operated in lock-step; there was no hazard detection hardware at all. This structure dictated that a stall in any functional unit pipeline must cause the entire processor to stall, since all the functional units must be kept synchronized. Although a compiler may be able to schedule the deterministic functional units to prevent stalls, predicting which data accesses will encounter a cache stall and scheduling them is very difficult. Hence,  caches needed to be blocking and to cause all the functional units to stall.

Expand them when they are read into the cache or are decoded. We will see techniques to reduce code size increases in both Sections 4.7 and 4.8. Early VLIWs operated in lock-step; there was no hazard detection hardware at all. This structure dictated that a stall in any functional unit pipeline must cause the entire processor to stall, since all the functional units must be kept synchronized. Although a compiler may be able to schedule the deterministic functional units to prevent stalls, predicting which data accesses will encounter a cache stall and scheduling them is very difficult. Hence, caches needed to be blocking and to cause all the functional units to stall. As the issue rate and number of memory references becomes large, this synchronization restriction becomes unacceptable. In more recent processors, the functional units operate more independently, and the compiler is used to avoid hazards at issue time, while hardware checks allow for unsynchronized execution once instructions are issued

Explain why refresh is another reason for variability of memory latency and hence cache miss penalty

Earlier sections presented main memory as if operated like a Swiss train, consistently delivering the goods exactly according to schedule. Refresh belies that myth, for some accesses take much longer than others do. Hence, refresh is another reason for variability of memory latency and hence cache miss penalty.

Since the memory matrix in a DRAM is conceptually square, the number of steps in a refresh is usually the square root of the DRAM capacity. DRAM designers try to keep time spent refreshing to be less than 5% of the total time. Earlier sections presented main memory as if operated like a Swiss train, consistently delivering the goods exactly according to schedule. Refresh belies that myth, for some accesses take much longer than others do. Thus, refresh is another reason for variability of memory latency and hence cache miss penalty. Amdahl suggested a rule of thumb that memory capacity should grow linearly with CPU speed to keep a balanced system, so that a 1000 MIPS processor should have 1000 megabytes of memory. CPU designers rely on DRAMs to supply that demand: in the past they expected a four-fold improvement in capacity every three years, or 55% per year. Unfortunately, the performance of DRAMs is growing at a much slower rate. Figure 5.30 shows a performance improvement in row access time, which is related to latency, of about 5% per year. The CAS or Data Transfer Time, which is related to bandwidth, is growing at more than twice that rate.

Explain why operating systems allow blocks to be placed anywhere in main memory

The miss penalty for virtual memory involves access to a rotating magnetic storage device and is therefore quite high. Given the choice of lower miss rates or a simpler placement algorithm, operating systems designers normally pick lower miss rates because of the exorbitant miss penalty. Hence, operating systems allow blocks to be placed anywhere in main memory.

We are now ready to answer the four memory-hierarchy questions for virtual memory. Q1: Where can a block be placed in main memory? The miss penalty for virtual memory involves access to a rotating magnetic storage device and is therefore quite high. Given the choice of lower miss rates or a simpler placement algorithm, operating systems designers normally pick lower miss rates because of the exorbitant miss penalty. Thus, operating systems allow blocks to be placed anywhere in main memory. According to the terminology in Both paging and segmentation rely on a data structure that is indexed by the page or segment number. This data structure contains the physical address of the block. For segmentation, the offset is added to the segments physical address to obtain the final physical address. For paging, the offset is simply concatenated to this physical page address (see Figure 5.35). Virtual address Virtual page number Page table Page offset Main memory This data structure, containing the physical page addresses, usually takes the form of a page table. Indexed by the virtual page number, the size of the table is the number of pages in the virtual address space. Given a 32-bit virtual address, 4-KB pages, and 4 bytes per page table entry, the size of the page table would be (232/212) 22 = 222 or 4 MB

What is a stateless protocol

A stateless protocol is a communication protocol in which the receiver must not retain session state from previous requests.Examples of stateful protocols include the Transmission Control Protocol (TCP) and the File Transfer Protocol (FTP).

A stateless protocol is a communication protocol in which the receiver must not retain session state from previous requests. The sender transfers relevant session state to the receiver in such a way that every request can be understood in isolation, that is without reference to session state from previous requests retained by the receiver.In contrast, a stateful protocol is a communication protocol in which the receiver may retain session state from previous requests.In computer networks, examples of stateless protocols include the Internet Protocol (IP), which is the foundation for the Internet, and the Hypertext Transfer Protocol (HTTP), which is the foundation of the World Wide Web. Examples of stateful protocols include the Transmission Control Protocol (TCP) and the File Transfer Protocol (FTP).Stateless protocols improve the properties of visibility, reliability, and scalability. Visibility is improved because a monitoring system does not have to look beyond a single request in order to determine its full nature. Reliability is improved because it eases the task of recovering from partial failures. Scalability is improved because not having to store session state between requests allows the server to quickly free resources and further simplifies implementation.The disadvantage of stateless protocols is that they may decrease network performance by increasing the repetitive data sent in a series of requests, since that data cannot be left on the server and reused.

Explain why, a single cache would present a structural hazard for loads and stores, leading to stalls

The data cache cannot supply all the memory needs of the processor, the processor also needs instructions. Although a single cache could try to supply both, it can be a bottleneck. For example, when a load or store instruction is executed, the pipelined processor will simultaneously request both a data word and an instruction word. Hence,  a single cache would present a structural hazard for loads and stores, leading to stalls.

A write miss is very similar to a read miss, since the 21264 allocates a block on a read or a write miss. We have seen how it works, but the data cache cannot supply all the memory needs of the processor: the processor also needs instructions. Although a single cache could try to supply both, it can be a bottleneck. For example, when a load or store instruction is executed, the pipelined processor will simultaneously request both a data word and an instruction word. Hence, a single cache would present a structural hazard for loads and stores, leading to stalls. One simple way to conquer this problem is to divide it: one cache is dedicated to instructions and another to data. Separate caches are found in most recent processors, including the Alpha 21264. Hence, it has a 64-KB instruction cache as well as the 64-KB data cache. The CPU knows whether it is issuing an instruction address or a data address, so there can be separate ports for both, thereby doubling the bandwidth between the memory hierarchy and the CPU. Separate caches also offer the opportunity of optimizing each cache separately: different capacities, block sizes, and associativities may lead to better performance. (In contrast to the instruction caches and data caches of the 21264, the terms unified or mixed are applied to caches that can contain either instructions or data.) Instruction cache Unified cache 8.16 3.82 1.36 0.61 0.30 0.02 caches. Separating instructions and data removes misses due to conflicts between instruction blocks and data blocks, but the split also fixes the cache space devoted to each type. Which is more important to miss rates? A fair comparison of separate instruction and data caches to unified caches requires the total cache size to be the same. For example, a separate 16-KB instruction cache and 16-KB data cache should be compared to a 32-KB unified cache. Calculating the average miss rate with separate instruction and data caches necessitates knowing the percentage of memory references to each cache. Figure 2.32 on page 149 suggests the split is 100%/(100% + 37% + 10%) or about 78% instruction references to (37% + 10%)/(100% + 37% + 10%) or about 22% data references. Splitting affects performance beyond what is indicated by the change in miss rates, as we shall see shortly.

Explain why, storage systems are typically held to a higher standard of dependability than the rest of the computer

After 15 years of doubling processor performance every 18 months, processor performance is not the problem it once was. Many would find highly dependable systems much more attractive than faster versions of todays systems with todays level of unreliability. Although it is frustrating when a program crashes, people become hysterical if they lose their data. Hence,  storage systems are typically held to a higher standard of dependability than the rest of the computer.

This shift in focus from computation to communication and storage of information emphasizes reliability and scalability as well as cost-performance. To reflect the increasing importance of I/O, the third edition of this book has twice as many I/O chapters as the first edition and half as many on instruction set architecture. This chapter covers storage I/O and the next covers communication I/O. Although two chapters cannot fully vindicate I/O, they may at least atone for some of the sins of the past and restore some balance. Types of Storage Devices Does Performance Matter? After 15 years of doubling processor performance every 18 months, processor performance is not the problem it once was. Many would find highly dependable systems much more attractive than faster versions of todays systems with todays level of unreliability. Although it is frustrating when a program crashes, people become hysterical if they lose their data. Hence, storage systems are typically held to a higher standard of dependability than the rest of the computer. Because of traditional demands placed on storageand because a new century needs new challengesthis chapter defines reliability, availability, and dependability and shows how to improve them. Dependability is the bedrock of storage, yet it also has its own rich performance theoryqueueing theorythat balances throughput versus response time

Explain why, each message must have at least 1 bit in addition to the data 

For one machine to get data from the other, it must first send a request containing the address of the data it desires from the other node. When a request arrives, the machine must send a reply with the data. Hence,  each message must have at least 1 bit in addition to the data to determine whether the message is a new request or a reply to an earlier request.

FIGURE 8.4 A simple network connecting two machines. For one machine to get data from the other, it must first send a request containing the address of the data it desires from the other node. When a request arrives, the machine must send a reply with the data. Hence, each message must have at least 1 bit in addition to the data to determine whether the message is a new request or a reply to an earlier request. The network must distinguish between information needed to deliver the message, typically called the header or the trailer depending on where it is relative to the data, and the payload, which contains the data. Figure 8.5 shows the format of messages in our simple network. This example shows a single-word payload, but messages in some interconnection networks can include hundreds of words. Interconnection networks involve normally software. Even this simple example invokes software to translate requests and replies into messages with the appropriate headers. An application program must usually cooperate with the operating system to send a message to another machine, since the network will be shared with all the processes running on the two machines, and the operating system cannot allow messages for one process to be received by another. Thus, the messaging software must have some way to distinguish between processes; this distinction may be included in an expanded header. Although hardware support can reduce the amount of work, most is done by software
