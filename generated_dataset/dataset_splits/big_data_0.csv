,Question,Answer,Context
0,"Explain why, the SRD method has improved the strength of the predictive Symmetrizing Ranked Data 61 Histogram # Boxplot 235 3 1 0 1 0 2 0 2 0 3220 134 116 82 103 113 112 161 + 172 304 5 669 may represent up to 14 counts Skewness = 1","3. Note: The stem-and-leaf displays turn into histograms because the sample size is too large, 2,000. Regardless, information can still be obtained about the shape of the distributions. only to further the SRD method by treating ordinal data, which assumes recoded numeric values, as interval data. out generating scatterplots, the reliable correlation coefficients between the two pairs of variables, the raw variables HI_BALANCE and RECENCY_ MOS, and the reexpressed variables, via the SRD method, rHI_BALANCE and rRECENCY_MOS are -0.6412 and -0.10063, respectively (see Tables 5.1 and 5.2). Hence, the SRD method has improved the strength of the predictive Symmetrizing Ranked Data 61 Histogram # Boxplot 235 3 1 0 1 0 2 0 2 0 3220 134 116 82 103 113 112 161 + 172 304 5 669 may represent up to 14 counts Skewness = 1.","2. The stem-and-leaf displays and the box-and-whiskers plots for RECENCY_MOS and rRECENCY_MOS are in Figures 5.4 and 5.5, respectively. RECENCY_MOS and rRECENCY_MOS have skewness values of 0.0621 and -0.0001, respectively.3. Note: The stem-and-leaf displays turn into histograms because the sample size is too large, 2,000. Regardless, information can still be obtained about the shape of the distributions. only to further the SRD method by treating ordinal data, which assumes recoded numeric values, as interval data. out generating scatterplots, the reliable correlation coefficients between the two pairs of variables, the raw variables HI_BALANCE and RECENCY_ MOS, and the reexpressed variables, via the SRD method, rHI_BALANCE and rRECENCY_MOS are -0.6412 and -0.10063, respectively (see Tables 5.1 and 5.2). Hence, the SRD method has improved the strength of the predictive Symmetrizing Ranked Data 61 Histogram # Boxplot 235 3 1 0 1 0 2 0 2 0 3220 134 116 82 103 113 112 161 + 172 304 5 669 may represent up to 14 counts Skewness = 1.0888 FIgure 5.2 Histogram and boxplot of HI_BALANCE. # Boxplot 3.25 3 0 9 0 33 88 186 299 0.25 379 + 385 308 187 68 30 –2.75 25 may represent up to 9 counts Skewness = 0.0096 FIgure 5.3 Histogram and boxplot for rHI_BALANCE. (-0.06412))/abs(-0.06421)), where abs = the absolute value function, which ignores the negative sign. In sum, the paired-variable (rHI_BALANCE, rRE- CENCY_MOS) is not necessarily the correct one pair; but it has more predic- tive power than the original pair-variable, and offer more potential in the model building process. Since the fatal night of April 15, 1912, when the White Star liner Titanic hit an iceberg and went down in the mid-Atlantic, fascination with the disaster has never abated. In recent years, interest in the Titanic has increased 62 Statistical and Machine-Learning Data Mining Histogram # Boxplot 625 2 0 5 0 7 0 39 91 131 325 196 301 371 453 282 114 25 8 may represent up to 10 counts Skewness = 0.0621 FIgure 5.4 Histogram and boxplot for RECENCY_MOS. # Boxplot 3.25 3 0 10 0 33 86 187 299 380 + 385 300 183 90 31 11 0 –3.25 2 0 may represent up to 9 counts Skewness = –0.0001 FIgure 5.5 Histogram and boxplot for rRECENCY_MOS. Correlation Coefficient between HI_BALANCE and RECENCY_MOS Pearson Correlation Coefficients, N = 2,000 Prob > r under H0: Rho = 0 HI_BALANCE RECENCY_MOS HI_BALANCE 1.00000 –0.06412 0.0041 RECENCY_MOS –0.06412 1.00000 0.0041 dramatically because of the discovery of the wreck site by Dr. Robert Ballard. of information about the sinking is savored. I believe that the SRD method can satisfy the appetite of the Titanic aficionado. I build a preliminary Titanic model to identify survivors so when Titanic II sails it will know beforehand who will be most likely to survive an iceberg hitting with outcome odds Symmetrizing Ranked Data 63 Table 5.2 Correlation Coefficient between rHI_BALANCE and rRECENCY_MOS Pearson Correlation Coefficients, N = 2,000 Prob > r under H0: Rho = 0 RHI_BALANCE RECENCY_MOS rRHI_BALANCE 1.00000 -0.10063 Rank for Variable rHI_BALANCE <.0001 rRECENCY_MOS -0.10063 1.00000 Rank for Variable <.0001 RECENCY_MOS of 2.0408e-12 to 1.* The Titanic model application, detailed in the remaining sections of the chapter, shows clearly the power of the SRD data mining technique, worthy of inclusion in every data miner’s toolkit. There were 2,201 passengers and crew aboard the Titanic. Only 711 persons survived, resulting in a 32.2% survival rate. For all persons, their basic demographic variables are known: GENDER (female, male); CLASS (first, second, third, crew); and AGE (adult, child). The passengers fall into 14 pat- terns of GENDER-CLASS-AGE (Table 5.3). Also, Table 5.3 includes pattern cell size (N), number of survivors within each cell (S), and the Survival Rate (format is %). informative, nominal (GENDER), and ordinal (CLASS and AGE), building a Titanic model has been a challenge for the best of academics and practitioners [2–6]. The SRD method is an original and valuable a data mining method that belongs with the Titanic modeling literature. In the following sections, I present the building of a Titanic model. AGE_, CLASS_AGE_, and CLASS_GENDER_ To see what the data look like, I generate stem-and-leaf displays and box-and- whisker plots for CLASS_, AGE_, and GENDER_ (Figures 5.6, 5.7, and 5.8, respectively). Also, I bring out the interaction variables CLASS_AGE_ and CLASS_GENDER_, which I created in the scales of measurement Section 5.2. The graphics of CLASS_AGE and CLASS_GENDER_ are in Figures 5.9 and 5.10, respectively. Regarding the coding of ordinal values for the interac- tion variables, I use the commonly known refrain during a crisis, “Women and children, first.” Survival rates for women and children are 74.35%, and 52.29%, respectively (Table 5.4), and bear out the refrain. Statistical and Machine-Learning Data Mining Table 5.3 Titanic Dataset Survival Pattern GENDER CLASS AGE N S Rate 1 Male First Adult 175 57 32.5 2 Male First Child 5 5 100.0 3 Male Second Adult 168 14 8.3 4 Male Second Child 11 11 100.0 5 Male Third Adult 462 75 16.2 6 Male Third Child 48 13 27.1 7 Male Crew Adult 862 192 22.3 8 Female First Adult 144 140 97.2 9 Female First Child 1 1 100.0 10 Female Second Adult 93 80 86.0 11 Female Second Child 13 13 100.0 12 Female Third Adult 165 76 46.1 13 Female Third Child 31 14 45.2 14 Female Crew Adult 23 20 87.0 Total 2,201 711 32.2 Histogram # Boxplot 4.1 325 3.5 285 2.9 2.3 706 1.7 1.1 885 may represent up to 19 counts Skewness = 1.0593 FIgure 5.6 Histogram and boxplot of CLASS_. rCLASS_, rAGE_, rCLASS_AGE_, and rCLASS_GENDER_ The stem-and-leaf displays and box-and-whisker plots for the symme- trized-ranked variables rCLASS_, rAGE_, rGENDER_, rCLASS_AGE_, and rCLASS_GENDER_ are in Figures 5.11 through 5.15, respectively. 65 Histogram # Boxplot 2.025 2092 1.925 1.825 1.725 1.625 1.525 1.425 1.325 1.225 1.125 1.025 109 may represent up to 44 counts Skewness = –4.1555 FIgure 5.7 Histogram and boxplot for AGE_. # Boxplot 1.025 470 0.925 0.825 0.725 0.625 0.525 0.425 0.325 0.225 0.125 0.025 1731 may represent up to 37 counts Skewness = 1.3989 FIgure 5.8 Histogram and boxplot for GENDER_. skewness for the original and SRD variables. The CLASS_, CLASS_AGE_, and CLASS_GENDER_ variables have been reexpressed, rendering signifi- cance shifts in the corresponding skewed distributions to almost symmet- ric distributions: Skewness values decrease significantly in the direction of zero. Although AGE_ and GENDER_ are moot variables with null (useless) graphics because the variables assume only two values, I include them as an object lesson. Statistical and Machine-Learning Data Mining Histogram # Boxplot 8.25 24 6 319 5.25 261 79 627 2.25 885 may represent up to 19 counts Skewness = 0.8948 FIgure 5.9 Histogram and boxplot for CLASS_AGE_. # Boxplot 8.25 145 0 106 23 196 4.75 180 862 510 1.25 179 may represent up to 18 counts Skewness = 1.1935 FIgure 5.10 Histogram and boxplot for CLASS_GENDER_. Women and Children by SURVIVED Row Pct Frequency No Yes Total Child 52 57 109 47.71 52.29 Female 109 316 425 25.65 74.35 Total 161 373 534 5.5.2.4 Building a Preliminary Titanic Model Reflecting on the definition of ordinal and interval variables, I know that the symmetrized-ranked variables are not ordinal variables. However, the scale property of the reexpressed variables rCLASS_, rCLASS_AGE_, and rCLASS_GENDER_ is not obvious. The variables are not on a ratio scale Symmetrizing Ranked Data 67 Histogram # Boxplot 1.15 706 0.45 285 + 325 –0.25 –0.95 885 may represent up to 19 counts Skewness = 0.3854 FIgure 5.11 Histogram and boxplot for rCLASS_. # Boxplot 0.15 2092 + –0.95 –2.05 109 may represent up to 44 counts Skewness = –4.1555 FIgure 5.12 Histogram and boxplot for rAGE_. symmetrized-ranked variable as an approximate interval variable . dependent variable SURVIVED, which assumes 1 = yes and 0 = no. The preliminary Titanic model is built by the SAS procedure LOGISTIC, and 68 Statistical and Machine-Learning Data Mining Histogram # Boxplot 1.35 470 –0.35 1731 may represent up to 37 counts Skewness = 1.3989 FIgure 5.13 Histogram and boxplot for rGENDER_. # Boxplot 2.7 24 6 319 0.9 261 79 627 –0.9 885 may represent up to 19 counts Skewness = 0.4592 FIgure 5.14 Histogram and boxplot for rCLASS_AGE_. rCLASS_AGE_ and rCLASS_GENDER_ is detailed in Table 5.6. survivors are correctly classified among those predicted survivors. The classification matrix for the preliminary model (Table 5.7) I believe yields a clearer display of the predictiveness of a binary (yes/no) classification model. tested with respect to the three-way interaction variables, which are required because the Titanic data only have two ordinal variables (CLASS and AGE) Symmetrizing Ranked Data 69 Histogram # Boxplot 1.9 145 106 23 196 180 862 510 –1.9 179 may represent up to 18 counts Skewness = 0.0551 FIgure 5.15 Histogram and boxplot for rCLASS_GENDER_. Comparison of Skewness Values for Original and Symmetrized-Ranked Variables Effect of the Symmetrized-Ranked Variable Skewness Value Method (Yes, No, Null) CLASS_ 1.0593 Yes rCLASS_ 0.3854 AGE_ -4.1555 Null. Binary variables render two parallel lines. -4.1555 GENDER_ 1.3989 Null. Binary variables render two parallel lines. 1.3989 CLASS_AGE_ 0.9848 Yes rCLASS_AGE_ 0.4592 CLASS_ 1.1935 Yes GENDER_ rCLASS_ 0.0551 GENDER_ and one nominal class (GENDER). To build on the preliminary model is going beyond the objectives of introducing the SRD method; the statistical data mining SRD method offers promise in improving the predictive power of the data and providing the data miner with a starting point for more applications of this utile method. Perhaps the data miner will continue with the preliminary model with the SRD data mining approach to finalize the Titanic model. Statistical and Machine-Learning Data Mining Table 5.6 Preliminary Titanic Model The LOGISTIC Procedure: Analysis of Maximum Likelihood Estimates Standard Wald Parameter DF Estimate Error Chi-Square Pr > ChiSq Intercept 1 -0.8690 0.0533 265.4989 <.0001 rCLASS_AGE_ 1 0.4037 0.0581 48.1935 <.0001 rCLASS_GENDER_ 1 1.0104 0.0635 252.9202 <.0001 Table 5.7 Classification Table for the Preliminary Titanic Model Predicted Predicted Deceased Survivors Total Actual deceased 1,199 291 1,490 Actual survivors 291 420 711 Total 1,490 711 2,201 5.6 Summary I introduced a new statistical data mining method, the SRD method, and added it to the paradigm of simplicity and desirability for good model-build- ing practice. The method uses two basic statistical tools, symmetrizing and ranking variables, yielding new reexpressed variables with likely improved predictive power. First, I detailed Steven’s scales of measurement to provide a framework for the new symmetric reexpressed variables. Accordingly, I defined the newly created SRD variables on an approximate interval scale.Then, I provided a quick review of the simplest of EDA elements, the stem- and-leaf display and the box-and-whiskers plot, as both are needed for pre- senting the underpinnings of the SRD method. Last, I illustrated the new method with two examples, which showed improved predictive power of the symmetric reexpressed variables over the raw variables. It was my intent that the examples are the starting point for more applications of the SRD method. 1. Stevens, S.S., On the theory of scales of measurement, American Association for the Advancement of Science, 103(2684), 677–680, 1946."
1,"Explain why, the sought-after objective is met","There are three elements of the GenIQ Model: (1) parse tree; (2) original data- set (with X1 and X2 replaced by XX1 and XX2, respectively*), to which the GenIQ Model score, the GenIQ predicted variable GenIQvar, is appended; and (3) the GenIQ Model equation, actually computer code defining the model. From the parse tree, the GenIQ Model selects the following functions: addition, subtraction, division, and the cosine. Also, GenIQ selects the two original variables XX1 and XX2 and the number 3, which in the world of GP is considered a variable. the RESPONSE variable, 5 R’s followed by 5 N’s. This is tantamount to a max- imized decile table. Hence, the sought-after objective is met.","2. X1 + X2 is a result of the variable selection of GenIQ, albeit it is a singleton best subset. ing a LR model. Consequently, I showed the features of the GenIQ Model as described in Section 24.4: It (1) data mines for new variables, (2) performs variable selection, and (3) specifies the model to optimize the decile table (actually, the upper four deciles in this illustration). I reconsider the dataset in Figure 24.7. I seek a GenIQ Model that maximizes the full decile table. From the modeling session of the first illustration, I obtain such a model in Figure 24.10. Before discussing the model, I explain how the model is obtained. GenIQ modeling produces, in a given session, about 5 to 10 equivalent models. Some models perform better, say, in the top four deciles (like the previous GenIQ Model). Some models perform better all the way from the top to the bottom deciles, the full decile table. In addition, some models have the desired discreteness of predicted scores. And, some mod- els have undesired clumping of or gaps among the GenIQ predicted scores. 397 GenIQ Model Tree XX2 + XX2 + XX2 RESPONSE is R Cos + XX2 + + 3 XX2 GenIQ Model - Computer Code Table 1. Scored, Ranked Data by GenIQvar x2 = 3; x3 = XX1; ID XX1 XX2 Response GenIQvar x2 = x2 + x3; 1 45 5 R 0.86740 If x1 NE 0 Then x1 = x2/x1; Else x1 = 1; x2 = XX2; 2 35 21 R 0.57261 x3 = XX2; 5 6 10 R 0.11528 x2 = x3 – x2; 3 31 38 R 0.05905 x3 = XX2; 4 30 30 R –0.53895 x2 = x2 + x3; 9 16 13 N –0.86279 If x1 NE 0 Then x1 = x2/x1; Else x1 = 1; 10 12 30 N –0.95241 x1 = Cos(x1); 6 45 37 N –0.96977 GenlQvar = x1; 7 30 10 N –0.99381 8 23 30 N –0.99833 FIgUre 24.10 The GenIQ Model maximizes the decile table. desired characteristics. It is important to note that the offering by GenIQ of equivalent models is not spurious data mining: There is no rerunning the data through various variable selections until the model satisfies the given objective.There are three elements of the GenIQ Model: (1) parse tree; (2) original data- set (with X1 and X2 replaced by XX1 and XX2, respectively*), to which the GenIQ Model score, the GenIQ predicted variable GenIQvar, is appended; and (3) the GenIQ Model equation, actually computer code defining the model. From the parse tree, the GenIQ Model selects the following functions: addition, subtraction, division, and the cosine. Also, GenIQ selects the two original variables XX1 and XX2 and the number 3, which in the world of GP is considered a variable. the RESPONSE variable, 5 R’s followed by 5 N’s. This is tantamount to a max- imized decile table. Hence, the sought-after objective is met. Note this illus- tration implies the weakness of statistical regression modeling: Statistical * The renaming of the X variables to XX is a necessary task as GenIQ uses X-named variables as an intermediate variable for defining the computer code of the model. Statistical and Machine-Learning Data Mining modeling offers only one model unless spurious rerunning the data through various variable selections is conducted, versus the several models of vary- ing decile performances offered by genetic modeling . With respect for statistical lineage, I maintained the statistical regression paradigm, which dictates fitting the data to model, is old, as it was developed and tested within the small data setting of yesterday and has been shown untenable with big data of today. I further maintained the new machine- learning genetic paradigm, which lets the data define the model, is espe- cially effective with big data; consequently, genetic models outperform the originative statistical regression models. I presented a comparison of genetic versus statistical LR, showing great care and completeness. My comparison showed the promise of the GenIQ Model as a flexible, any-size data alterna- tive method to the statistical regression models. 1. Harlow, L.L., Mulaik, S.A., & Steiger, J.H. (Eds.), What If There Were No Significance Testing? Erlbaum, Mahwah, NJ, 1997.2. Koza, J.R, Genetic Programming: On the Programming of Computers by Means of Natural Selection, MIT Press, Cambridge, MA, 1992. Data Reuse: A Powerful Data Mining Effect of the GenIQ Model 25.1 Introduction The purpose of this chapter is to introduce the concept of data reuse, a powerful data mining effect of the GenIQ Model. Data reuse is appending new variables, which are found when building a GenIQ Model, to the original dataset. As the new variables are reexpressions of the original variables, the correlations among the original variables and the GenIQ data-mined vari- ables are expectedly high. In the context of statistical modeling, the issue of such high correlations is known as multicollinearity. One effect of multi- collinearity is unstable regression coefficients, an unacceptable result. In contrast, multicollinearity is a nonissue for the GenIQ Model because it has no coefficients. The benefit of data reuse is apparent: The original dataset is enhanced with the addition of new, predictive-full, GenIQ data-mined vari- ables. I provide two illustrations of data reuse as a powerful data mining technique. Data reuse is appending new variables, which are found when building a GenIQ Model, to the original dataset. As the new variables are reexpres- sions of the original variables, the correlations among the original variables and the GenIQ data-mined variables are expectedly high. In the context of statistical modeling, the issue of such high correlations is known as mul- ticollinearity. The effects of multicollinearity are inflated standard errors of the regression coefficients, unstable regression coefficients, lack of valid declaration of the importance of the predictor variables, and an indetermi- nate regression equation when the multicollinearity is severe. The simplest solution of “guess and check,” although inefficient, to the multicollinearity problem is deleting suspect variables from the regression model. 400 Statistical and Machine-Learning Data Mining Multicollinearity is a nonissue for the GenIQ Model because it has no coeffi- cients. The benefit of data reuse is apparent: The original dataset is enhanced with the addition of new, predictive-full, GenIQ data-mined variables. The concept and benefit of data reuse are best explained by illustration. I provide two such illustrations. To illustrate data reuse, I build an ordinary least squares (OLS) regression model with dependent variable PROFIT, predictor variables XX1 and XX2, and the data in Table 25.1. Profit_est = 2.42925 + 0.16972*XX1 - 0.06331*XX2 (25.1) The assessment of the Profit_est model is made in terms of decile table per- formance. The individuals in Table 25.1 are scored and ranked by Profit_est."
2,"Explain why, Overfit is clean of noise and is ready for building a well-fitted model","4. The Cum Lift at the fifth score group is 119, after which the Cum Lifts range between [100, 102/103]. d P r ob Table 27.3 lem Quasi 20-tile Analysis , N Select ew S Number of Tiles 20 olut CUM io Random_ CUM n NUMBER OF NUMBER OF Split RATE CUM% of LIFT N.TILE INDIVIDUALS Random_Split Random_Split RATE (%) (%) SAMPLE (%) Top 56 40 71.43 71.43 04.00 143 2 23 15 65.22 69.62 05.64 139 3 16 8 50.00 66.32 06.79 133 4 33 20 60.61 64.84 09.14 130 5 44 19 43.18 59.30 12.29 119 6 503 240 47.71 50.67 48.21 101 423 424 Statistical and Machine-Learning Data Mining Table 27.4 RANDOM_SPLIT Decile Table Predicted Random_ Cum Random_ Split Rate Random_ Cum Min Max Decile Split % Split Rate % Lift score score Top 62 50.49 50.41 101 −1.26 1.33 2nd 62 50.49 50.41 101 −1.27 −1.26 3rd 61 49.67 50.27 101 −1.38 −1.27 4th 62 50.49 50.31 101 −1.54 −1.38 5th 61 49.67 50.16 100 −1.60 −1.54 6th 60 48.86 49.93 100 −3.07 −1.60 7th 62 50.49 50.00 100 −3.19 −3.07 8th 61 49.67 50.00 100 −3.28 −3.19 9th 62 50.49 50.05 100 −4.71 −3.28 Bottom 61 49.67 50.00 100 −13.44 −4.71 Overfit is now clean of noise. To test the soundness of the last assertion, I rerun GenIQ with clean Overfit data. The resultant decile table, in Table 27.4, displays Cum Lifts within [100, 101]. Hence, Overfit is clean of noise and is ready for building a well-fitted model.","3. Within these N-tiles, there are 170 (56 + 23 + 16 + 33 + 44) individuals.4. The Cum Lift at the fifth score group is 119, after which the Cum Lifts range between [100, 102/103]. d P r ob Table 27.3 lem Quasi 20-tile Analysis , N Select ew S Number of Tiles 20 olut CUM io Random_ CUM n NUMBER OF NUMBER OF Split RATE CUM% of LIFT N.TILE INDIVIDUALS Random_Split Random_Split RATE (%) (%) SAMPLE (%) Top 56 40 71.43 71.43 04.00 143 2 23 15 65.22 69.62 05.64 139 3 16 8 50.00 66.32 06.79 133 4 33 20 60.61 64.84 09.14 130 5 44 19 43.18 59.30 12.29 119 6 503 240 47.71 50.67 48.21 101 423 424 Statistical and Machine-Learning Data Mining Table 27.4 RANDOM_SPLIT Decile Table Predicted Random_ Cum Random_ Split Rate Random_ Cum Min Max Decile Split % Split Rate % Lift score score Top 62 50.49 50.41 101 −1.26 1.33 2nd 62 50.49 50.41 101 −1.27 −1.26 3rd 61 49.67 50.27 101 −1.38 −1.27 4th 62 50.49 50.31 101 −1.54 −1.38 5th 61 49.67 50.16 100 −1.60 −1.54 6th 60 48.86 49.93 100 −3.07 −1.60 7th 62 50.49 50.00 100 −3.19 −3.07 8th 61 49.67 50.00 100 −3.28 −3.19 9th 62 50.49 50.05 100 −4.71 −3.28 Bottom 61 49.67 50.00 100 −13.44 −4.71 Overfit is now clean of noise. To test the soundness of the last assertion, I rerun GenIQ with clean Overfit data. The resultant decile table, in Table 27.4, displays Cum Lifts within [100, 101]. Hence, Overfit is clean of noise and is ready for building a well-fitted model. Note: The decile table in Table 27.4 is a smart decile table, which does not require my generating a corresponding quasi N-tile analysis. Overfitting, a problem akin to model inaccuracy, is as old as model build- ing itself because it is part of the modeling process. An overfitted model is one that approaches reproducing the training data on which the model is built—by capitalizing on the idiosyncrasies of the training data. I introduced a new solution, based on the data-mining feature of the GenIQ Model, to the old problem of overfitting. I illustrated, with a real case study, how the GenIQ Model identifies the complexity of the idiosyncrasies and instructs for deletion of the individuals that contribute to the complexity in the data to produce a dataset clean of noise. The clean dataset is ready for building a well-fitted model. The Importance of Straight Data: Revisited 28.1 Introduction The purpose of this chapter is to revisit examples discussed in Chapters 4 and 9, in which the importance of straight data is illustrated. I posited the solutions to the examples without explanation as the material needed to understand the solution was not introduced at that point. ered. Thus, for completeness, I detail the posited solutions in this chapter. The solution uses the data mining feature, straightening data, of the GenIQ Model. I start with the example in Chapter 9 and conclude with the example in Chapter 4. From Chapter 4, Section 4.2, there are five reasons why it is important to straighten data: 1. The straight-line (linear) relationship between two continuous vari- ables, say X and Y, is as simple as it gets. As X increases (decreases) in its values, so does Y increase (decrease) in its values, in which case it is said that X and Y are positively correlated. Or, as X increases (decreases) in its values, so does Y decrease (increase) in its values, in which case it is said that X and Y are negatively correlated. As an example of this setting of simplicity (and everlasting importance), Einstein’s E and m have a perfect positive linear relationship. on within the data. The class of linear data is the desirable element for good model-building practice. eties of the statistical linear model, require linear relationships between a dependent variable and (a) each predictor variable in a model and 425 426 Statistical and Machine-Learning Data Mining (b) all predictor variables considered jointly, regarding them as an array of predictor variables that have a multivariate distribution. yielding good predictions with nonstraight data, in fact do better with straight data. are theoretical reasons for symmetry and straightness going hand in hand. that symmetric data have values that are in correspondence in size and shape on opposite sides of a dividing line or middle value of the data. The iconic symmetric data profile in statistics is bell shaped. I envision an underlying positively sloped straight line running through the 10 points in the PROFIT-INCOME smooth plot in Figure 9.2, even though the smooth trace reveals four severe kinks. Based on the general association test with a TS (test statistic) value of 6, which is almost equal to the cutoff score 7, as presented in Chapter 2, I conclude there is an almost noticeable straight-line relationship between PROFIT and INCOME. The correlation coefficient for the relationship is a reliable rPROFIT, INCOME of 0.763. Notwithstanding these indicators of straightness, the relationship could use some straightening, but clearly, the bulging rule does not apply. nonlinearities, is the GenIQ procedure, a machine-learning, genetic-based data mining method. I use the GenIQ Model to reexpress INCOME. The genetic structure, which represents the reexpressed INCOME variable, labeled gINCOME, is defined in Equation (9.3): gINCOME = sin(sin(sin(sin(INCOME)))*INCOME) + log(INCOME) (9.3) The structure uses the nonlinear reexpressions of the trigonometric sine function (four times) and the log (to base 10) function to loosen the “kinky” PROFIT-INCOME relationship. The relationship between PROFIT and INCOME (via gINCOME) has indeed been smoothed out as the smooth trace reveals no serious kinks in Figure 9.3. Based on TS equal 6, which again is almost equal to the cutoff score of 7, I conclude there is an almost noticeable straight-line PROFIT-gINCOME relationship, a nonrandom scatter about an underlying positively sloped straight line. The correlation coefficient for the reexpressed relationship is a reliable rPROFIT, gINCOME of 0.894. 427 Visually, the effectiveness of the GenIQ procedure in straightening the data is obvious: the sharp peaks and valleys in the original PROFIT smooth plots versus the smooth wave of the reexpressed smooth plot. Quantitatively, the gINCOME-based relationship represents a noticeable improvement of 7.24% (= (0.894 - 0.763)/0.763) increase in correlation coefficient “points” over the INCOME-based relationship. toid that states a dollar-unit variable is often reexpressed with the log function. Thus, it is not surprising that the genetically evolved structure gINCOME uses the log function. With respect to logging the PROFIT vari- able, I concede that PROFIT could not benefit from a log reexpression, no doubt due to the “mini” in the dataset (i.e., the small size of the data), so I chose to work with PROFIT, not log of PROFIT, for the sake of simplicity (another EDA [exploratory data analysis] mandate, even for instructional purposes). The GenIQ Model for gINCOME is in Figure 28.1. Sin Sin Sin Income Sin × Profit + Income Log Income GenIQ Model Computer Code x1 = INCOME; x1 = Log (x1); x2 = INCOME; x2 = sin (x2); x2 = sin (x2); x2 = sin (x2); x3 = INCOME; x2 = x2 + x3; x2 = sin (x2); x1 = x1 + x2; GenIQvar = x1; gINCOME = GenIQvar; FIguRE 28.1 GenIQ Model for gINCOME. Statistical and Machine-Learning Data Mining 28.4 Restatement of Section 4.6 “Data Mining the Relationship of (xx3, yy3)” Recall, I data mine for the underlying structure of the paired variables (xx3, yy3) using a machine-learning approach under the discipline of evolution- ary computation, specifically genetic programming (GP). The fruits of my data mining work yield the scatterplot in Figure 4.3 of Chapter 4. The data mining work is not an expenditure of preoccupied time (i.e., not waiting for time- consuming results) or mental effort as the GP-based data mining (GP-DM) is a machine-learning adaptive intelligent process, which is quite effective for straightening data. The data mining software used is the GenIQ Model, which renames the data-mined variable with the prefix GenIQvar. Data- mined (xx3, yy3) is relabeled (xx3, GenIQvar(yy3)). The GenIQ Model for GenIQvar(yy3) is in Figure 28.2. xx3 Cos + xx3 GenIQ(yy3) + xx3 GenIQ Model Computer Code x1 = 0.6550772; 0.655 x2 = xx3; If x1 ne 0 then x1 = x2/x1; Else x1 = 1; x2 = xx3; x3 = xx3; x2 = x2 + x3; x2 = Cos (x2); x1 = x1 + x2; GenIQvar (yy3) = x1; FIguRE 28.2 GenIQ Model for GenlQvar(yy3). 429 28.5 Summary The purpose of this chapter is to revisit examples discussed in Chapters 2, 4, and 9, in which I posited the solutions to the examples without explana- tion, as the material needed to understand the solution was not introduced at that point. At this point, the background required has been more than extensively covered. Thus, for completeness, I detailed the posited solutions. Model. Now, I have completed the illustrations of why it is important to straighten data. 29 The GenIQ Model: Its Definition and an Application* 29.1 Introduction Using a variety of techniques, regression modelers build the everyday mod- els that maximize expected response and profit based on the results of mar- keting programs, solicitations, and the like. Standard techniques include the statistical methods of classical discriminant analysis (DA), as well as logis- tic and ordinary regression. A recent addition to the regression modelers’ arsenal is the machine-learning (ML) method of artificial neural networks (ANNs). Another newcomer is the GenIQ Model, a ML alternative to the statistical ordinary least squares model and the logistic regression model (LRM), is the focus of this chapter and is presented in full detail. zation techniques provide the estimation of all models. Then, I introduce genetic modeling, the ML optimization approach that serves as the engine for the GenIQ Model. As the ubiquitous marketing objectives are to maximize expected response and profit for developing marketing strategies, I demon- strate how the GenIQ Model serves to meet those objectives. Actual case studies explicate further the potential of the GenIQ Model. Whether in business or model building, optimization is central to the deci- sion-making process. In both theory and practice, an optimization tech- nique involves selecting the best (or most favorable) condition within a given environment. To distinguish among available choices, an objective function (also known as a fitness function) must be predetermined. The choice, which * This chapter is based on an article in Journal of Targeting, Measurement and Analysis for Marketing, 9, 3, 2001. Used with permission. 432 Statistical and Machine-Learning Data Mining corresponds to the extreme value* of the objective function, is the best out- come that constitutes the details of the solution to the problem. lem. For example, in marketing, one such problem is to predict sales. The least squares regression technique is a model formulated to address sales prediction. The regression problem is formulated in terms of finding the regression equation such that the prediction errors (the difference between actual and predicted sales) are small.† The objective function is the prediction error, making the best equation the one that minimizes that prediction error. own decision problem. The GenIQ Model addresses problems pertaining to direct, database, or telemarketing solicitations; marketing-mix optimization programs; business-intelligent offerings; customer relationship management (CRM) campaigns; Web- or e-mail-based broadcasts; and the like and uses genetic modeling as the optimization technique for its solution. Just as Darwin’s principle of the survival of the fittest§,¶ explains tendencies in human biology, regression modelers can use the same principle to predict the best solution to an optimization problem.** Each genetic model has an associated fitness function value that indicates how well the model solves, or “fits,” the problem. A model with a high fitness value solves the problem * If the optimization problem seeks to minimize the objective function, then the extreme value is the smallest; if it seeks to maximize, then the extreme value is the largest.† The definition of small (technically called mean squared error) is the average of the squared differences between actual and predicted values."
3,Explain why a well-fitted model can be built on the clean version of Overfit,"3. If Overfit has unacceptable noise, building a model is possible: The decile table has Cum Lifts outside [98, 102] in the upper deciles, say, top to third. Overfit has idiosyncrasies causing substantial overfit- ting. Overfit is cleaned by deleting the individuals in the deciles with Cum Lifts outside [98, 102]. Hence,a well-fitted model can be built on the clean version of Overfit.","The decile table has Cum Lifts within [98, 102] in the upper deciles, say, top to third. Overfit is almost clean of idiosyncrasies. Highly probable accurate predictions result when building a model with Overfit data.3. If Overfit has unacceptable noise, building a model is possible: The decile table has Cum Lifts outside [98, 102] in the upper deciles, say, top to third. Overfit has idiosyncrasies causing substantial overfit- ting. Overfit is cleaned by deleting the individuals in the deciles with Cum Lifts outside [98, 102]. As a result, a well-fitted model can be built on the clean version of Overfit. Accurate predictions result when building a model with the clean Overfit data.* For unequal splits (e.g., 60%-40%), the approach is the same. 419 Table 27.1 Overfit Data: Variables, and Type # Variable Type 1 RANDOM_SPLIT Num 2 REQUESTE Num 3 INCOME Num 4 TERM Num 5 APPTYPE Char 6 ACCOMMOD Num 7 CHILDREN Num S MOVES5YR Num 9 MARITAL Num 10 EMPLOYME Num 11 DIRECTDE Char 12 CONSOLID Num 13 NETINCOM Num 14 EMPLOY_1 Char 15 EMAIL Num 16 AGE Num 17 COAPP Num 18 GENDER Num 19 INCOMECO Num 20 COSTOFLI Num 21 PHCHKHL Char 22 PWCHKHL Char 23 PMCHKHL Char 24 NOCITZHL Char 25 EMPFLGHL Char 26 PFSFLGHL Char 27 NUMEMPLO Num 28 BANKAFLG Char 29 EMPFLGML Char 30 PFSFLGML Char 31 CIVILSML Char 32 TAXHL Num 33 TAXML Num 34 NETINCML Num 35 LIVLOANH Num 36 LIVCOSTH Num 37 CARLOAN Num 38 CARCOST Num 39 EDLOAN Num 40 EDCOST Num ( Continued) 420 Statistical and Machine-Learning Data Mining Table 27.1 (Continued) Overfit Data: Variables, and Type # Variable Type 41 OTLOAN Num 42 OTCOST Num 43 CCLOAN Num 44 CCCOST Num 45 EMLFLGHL Char 46 PHONEH Num 47 PHONEW Num 48 PHONEC Num 49 REQCONSR Num 50 TIMEEMPL Num 51 AGECOAPP Num 52 APPLIEDY Num 53 GBCODE Num 27.3.1 RaNDOM_SPlIT genIQ Model The GenIQ Model consists of two components: a tree display and computer code. Using the Overfit dataset, I build a GenIQ Model with the dependent variable RANDOM_SPLIT. The RANDOM_SPLIT GenIQ Model tree display (Figure 27.4) identifies the complexity of the idiosyncrasies (noise) in Overfit. The RANDOM_SPLIT GenIQ Model computer code is in Figure 27.5. The decile analysis from the RANDOM_SPLIT GenIQ Model* built on the Overfit data indicates Overfit has unacceptable noise."
4,Explain why targeting with a descriptive profile does not provide any assurance that the best customers will respond to a new campaign,"3. The success of a solicitation based on look-alike profiles, in terms of the actual responses obtained, depends on the disparity of the defined look-alike group and target group and the tenability of the look-alike assumption. Marketers typically attempt to improve the effectiveness of their campaigns by targeting their best customers. However, targeting only the best custom- ers (responders) based on their characteristics is sure to fail. The descriptive profile, which represents responders without reference to nonresponders, is a nonpredictive profile. Hence,targeting with a descriptive profile does not provide any assurance that the best customers will respond to a new campaign.","* The sample look-alike rate sometimes needs to be adjusted to equal the incidence of the target group in the population. This incidence is rarely known and must be estimated. 299 nonLOOK-ALIKE (700) 70.0% LOOK-ALIKE (300) 30.0% Total 1000 GENDER female male 100.0% 66.7% 0.0% 33.3% 100 900 CHILDREN no yes 100.0% 40.0% 0.0% 60.0% 400 500 INCOME [55000,175000) [176000,250000] 100.0% 0.0% 0.0% 100.0% 200 300 Figure 17.7 Look-alike tree. It is instructive to discuss a noticeable characteristic of look-alike trees. In general, upper segment rates in a look-alike tree are quite large and often reach 100%. Similarly, lower segment rates are quite small and often fall to 0%. I observe these patterns in Table 17.5. There is one segment with a 100% look-alike rate and three segments with a 0% look-alike rate. 1. It is easier to identify an individual who looks like someone with predefined characteristics (e.g., gender, and children) than someone who behaves in a particular manner (e.g., responds to a solicitation). rates to the extent the defined look-alike group differs from the target 300 Table 17.5 Gains Chart for Look-Alike Tree Defined by GENDER, CHILDREN, and INCOME Number of Cumulative Segment Cumulative Cumulative Segmenta Size of Segment Responses Responses Response Rate Response Rate Lift 4 INCOME, [176000,250000) S CHILDREN, yes tat GENDER, male 300 300 300 100.0% 100.0% 333 isti 2 CHILDREN, no ca GENDER, male 400 0 300 0.0% 42.9% 143 l an 1 GENDER, female 100 0 300 0.0% 37.5% 125 d M 3 INCOME a [55000,175000) ch CHILDREN, yes ine GENDER, male 200 0 300 0.0% 30.0% 100 -Lear 1,000 300 30.0% nin a Segments are ranked by look-alike rates. ata Mi ning Identifying Your Best Customers 301 group. Care should be exercised in defining the look-alike group because it is easy to include individuals inadvertently unwanted.3. The success of a solicitation based on look-alike profiles, in terms of the actual responses obtained, depends on the disparity of the defined look-alike group and target group and the tenability of the look-alike assumption. Marketers typically attempt to improve the effectiveness of their campaigns by targeting their best customers. However, targeting only the best custom- ers (responders) based on their characteristics is sure to fail. The descriptive profile, which represents responders without reference to nonresponders, is a nonpredictive profile. Therefore, targeting with a descriptive profile does not provide any assurance that the best customers will respond to a new campaign. The value of the descriptive profile lies in its definition of the salient characteristics of the target group, which are used to develop an effective marketing strategy. describes responders with regard to responsiveness, that is, in terms of vari- ables that discriminate between responders and nonresponders. The predic- tive profile is used for finding responders to a new campaign, after which the descriptive profile is used to communicate effectively with those customers. and interesting predictive profiles. With an illustration, I presented the gains chart as the standard report of the predictive power of the tree-based predic- tive profiles, for example, the expected response rates on implementing the profiles in a solicitation. alike profiling, a reliable method when actual response information is not available. A look-alike profile is a predictive profile based on a group of indi- viduals who look like the individuals in a target group, thus serving as sur- rogate responders. I addressed a warning that the look-alike rates are biased estimates of target response rates because the look-alike profiles are about surrogate responders, not actual responders. 18 Assessment of Marketing Models* 18.1 Introduction Marketers use decile analysis to assess their models in terms of classifica- tion or prediction accuracy. The uninformed marketers do not know that additional information from the decile analysis can be extracted to supple- ment the model assessment. The purpose of this chapter is to present two additional concepts of model assessment—precision and separability—and to illustrate these concepts by further use of the decile analysis. response and profit models and illustrate the basic measures of accuracy. Lift. The discussion of Cum Lift is set in the context of a decile analysis, which is the usual approach marketers use to evaluate the performance of response and profit models. I provide a systematic procedure for conducting a decile analysis with an illustration. separability. Last, I provide guidelines for using all three measures in assess- ing marketing models. How well does a response model correctly classify individuals as responder and nonresponder? The traditional measure of accuracy is the proportion of total correct classifications (PTCC), which is calculated from a simple cross tabulation. on the validation sample consisting of 100 individuals with a 15% response * This chapter is based on an article with the same title in Journal of Targeting, Measurement and Analysis for Marketing, 7, 3, 1998. Used with permission. 304 Statistical and Machine-Learning Data Mining Table 18.1 Classification Results of Response Model Predicted Nonresponder Responder Total Actual Nonresponder 74 11 85 Responder 2 13 15 Total 76 24 100 rate. The “Total” column indicates there are 85 actual nonresponders and 15 actual responders in the sample. The “Total” row indicates the model predicts 76 nonresponders and 24 responders. The model correctly classi- fies 74 nonresponders and 13 responders. Accordingly, the PTCC is (74 + 13)/100 = 87%. situation. For example, if the assessment criterion imposes a penalty for misclassifications,* then PTCC must be either modified or discarded for a more relevant measure. els: Cum Lift. They use response models to identify those individuals most likely to respond to a solicitation. They create a solicitation list of the most likely individuals to obtain an advantage over a random selection of indi- viduals. The Cum Lift indicates how many more responses can be expected with a selection based on a model over the expected responses from a random selection (no model). Specifically, the Cum Lift is an index of the expected response rate with a selection based on a model compared with the expected response rate based on a random selection. Before I illustrate the calculation of the Cum Lift, I provide a companion exposition of accuracy for the profit model. How well does a profit model correctly predict an individual’s profit value? There are several measures of prediction accuracy, all of which are based on the concept of error, namely, actual profit minus predicted profit. The mean squared error (MSE) is by far the most popular measure, but it is flawed, thus necessitating three alternative measures. I briefly review the four error measures.* For example, there is a $2 loss if a responder is classified as a nonresponder and a $4 loss if a nonresponder is classified as a responder. 305 1. MSE is the mean of individual squared errors. It gives greater impor- tance to larger errors and tends to underestimate the predictive accuracy of the model. This point is illustrated in the discussion that follows. bias in the estimates. Percentage error for an individual is the error divided by the actual profit multiplied by 100. gards the sign of the error."
5,Explain why variable selec- tion methods without data mining capability cannot generate the enhanced best subset,"‡ Absolute r value means that the sign is ignored. For example, if r = -0.23, then absolute r = +0.23. Statistical and Machine-Learning Data Mining Second, these methods cannot identify structure in the data. They find the best subset of variables without “digging” into the data, a feature that is necessary for finding important variables or structures. Hence,variable selec- tion methods without data mining capability cannot generate the enhanced best subset.","† For example, consider two TS values: 1.934056 and 1.934069. These values are equal when rounding occurs at the third place after the decimal point: 1.934.‡ Absolute r value means that the sign is ignored. For example, if r = -0.23, then absolute r = +0.23. Statistical and Machine-Learning Data Mining Second, these methods cannot identify structure in the data. They find the best subset of variables without “digging” into the data, a feature that is necessary for finding important variables or structures. Therefore, variable selec- tion methods without data mining capability cannot generate the enhanced best subset. The following illustration clarifies this weakness. Consider the complete set of variables, X1, X2, …, X10. Any of the variable selection meth- ods in current use will only find the best combination of the original vari- ables (say X1, X3, X7, X10), but can never automatically transform a variable (say transform X1 to log X1) if it were needed to increase the information con- tent (predictive power) of that variable. Furthermore, none of these methods can generate a reexpression of the original variables (perhaps X3/X7) if the constructed variable were to offer more predictive power than the original component variables combined. In other words, current variable selection methods cannot find the enhanced best subset that needs to include trans- formed and reexpressed variables (possibly X1, X3, X7, X10, log X1, X3/X7). A subset of variables without the potential of new variables offering enhanced predictive power clearly limits the modeler in building the best model. cussed next.Transformed variables with a preferred shape. A variable selection procedure should have the ability to transform an individual variable, if necessary, to induce a symmetric distribution. Symmetry is the preferred shape of an individual variable. For example, the workhorses of statistical measures— the mean and variance—are based on the symmetric distribution. A skewed distribution produces inaccurate estimates for means, variances, and related statistics, such as the correlation coefficient. Analyses based on a skewed distribution provide typically questionable findings. Symmetry facilitates the interpretation of the effect of the variable in an analysis. A skewed distri- bution is difficult to examine because most of the observations are bunched together at either end of the distribution. linear relationships. A linear or straight-line relationship is the preferred shape when considering two variables. A straight-line relationship between independent and dependent variables is an assumption of the popular statis- tical linear model. (Remember, a linear model is defined as a sum of weighted variables, such as Y = b0 + b1*X1 + b2*X2 + b3*X3.)* Moreover, a straight-line relationship among all the independent variables considered jointly is also a desirable property [4]. Straight-line relationships are easy to interpret: A unit of increase in one variable produces an expected constant increase in a second variable."
6,Explain why calculating the average income from a database of 2 mil- lion individuals requires heavy-duty lifting (number crunching),"† Tukey, via his groundbreaking EDA book, put the concept of “reexpression” in the forefront of EDA data mining tools; yet, he never provided any definition. I assume he believed that the term is self-explanatory. Tukey’s first mention of reexpression is in a question on page 61 of his work: “What is the single most needed form of re-expression?” I, for one, would like a definition of reexpression, and I provide one further in the book. 9 observations and learning from the variables. Although it depends on who is analyzing the data, a sample size greater than 50,000 individuals can be con- sidered big. Hence,calculating the average income from a database of 2 mil- lion individuals requires heavy-duty lifting (number crunching).","There are two sides to big data. On one side is classical statistics that con- siders big as simply not small. Theoretically, big is the sample size after which asymptotic properties of the method “kick in” for valid results. On the other side is contemporary statistics that considers big in terms of lifting * Authors Weiss and Indurkhya and I use the general concept of “big” data. However, we stress different characteristics of the concept.† Tukey, via his groundbreaking EDA book, put the concept of “reexpression” in the forefront of EDA data mining tools; yet, he never provided any definition. I assume he believed that the term is self-explanatory. Tukey’s first mention of reexpression is in a question on page 61 of his work: “What is the single most needed form of re-expression?” I, for one, would like a definition of reexpression, and I provide one further in the book. 9 observations and learning from the variables. Although it depends on who is analyzing the data, a sample size greater than 50,000 individuals can be con- sidered big. Thus, calculating the average income from a database of 2 mil- lion individuals requires heavy-duty lifting (number crunching). In terms of learning or uncovering the structure among the variables, big can be consid- ered 50 variables or more. Regardless of which side the data analyst is work- ing, EDA scales up for both rows and columns of the data table. There are three distinguishable characteristics of data size: condition , location , and population. Condition refers to the state of readiness of the data for analysis. Data that require minimal time and cost to clean, before reliable analysis can be performed, are said to be well conditioned; data that involve a substantial amount of time and cost are said to be ill conditioned. Small data are typically clean and therefore well conditioned. ates data flowing continuously from all directions at unprecedented speed and volume, and these data usually require cleansing. They are considered “dirty” mainly because of the merging of multiple sources. The merging pro- cess is inherently a time-intensive process, as multiple passes of the sources must be made to get a sense of how the combined sources fit together. Because of the iterative nature of the process, the logic of matching individual records across sources is at first “fuzzy,” then fine-tuned to soundness; until that point, unexplainable, seemingly random, nonsensical values result. Thus, big data are ill conditioned. small data, big data reside in relational databases consisting of a set of data tables. The link among the data tables can be hierarchical (rank or level dependent) or sequential (time or event dependent). Merging of multiple data sources, each consisting of many rows and columns, produces data of even greater number of rows and columns, clearly suggesting bigness. teristics in common and related to the study under consideration. Small data ideally represent a random sample of a known population that is not expected to encounter changes in its composition in the near future. The data are collected to answer a specific problem, permitting straightforward answers from a given problem-specific method. In contrast, big data often represent multiple, nonrandom samples of unknown populations, shifting in composition within the short term. Big data are “secondary” in nature; that is, they are not collected for an intended purpose. They are available from the hydra of marketing information, for use on any post hoc problem, and may not have a straightforward solution. big data per se. However, he did predict that the cost of computing, in 10 Statistical and Machine-Learning Data Mining both time and dollars, would be cheap, which arguably suggests that he knew big data were coming. Regarding the cost, clearly today’s PC bears this out. The data size discussion raises the following question: “How large should a sample be?” Sample size can be anywhere from folds of 10,000 up to 100,000. over 15 years and a statistics instructor who analyzes deceivingly simple cross tabulations with the basic statistical methods as my data mining tools, I have observed that the less-experienced and -trained data analyst uses sam- ple sizes that are unnecessarily large. I see analyses performed on and mod- els built from samples too large by factors ranging from 20 to 50. Although the PC can perform the heavy calculations, the extra time and cost in getting the larger data out of the data warehouse and then processing them and thinking about it are almost never justified. Of course, the only way a data analyst learns that extra big data are a waste of resources is by performing small versus big data comparisons, a step I recommend. The term data mining emerged from the database marketing community sometime between the late 1970s and early 1980s. Statisticians did not under- stand the excitement and activity caused by this new technique since the discovery of patterns and relationships (structure) in the data is not new to them. They had known about data mining for a long time, albeit under various names, such as data fishing, snooping, and dredging, and most dis- paraging, “ransacking” the data. Because any discovery process inherently exploits the data, producing spurious findings, statisticians did not view data mining in a positive light. you have a hammer in hand, you tend eventually to start seeing nails.” The statistical version of this maxim is, “Simply looking for something increases the odds that something will be found.” Therefore, looking * Abraham Maslow brought to the world of psychology a fresh perspective with his concept of “humanism,” which he referred to as the “third force” of psychology after Pavlov’s “behaviorism” and Freud’s “psychoanalysis.” Maslow’s hammer is frequently used without any- body seemingly knowing the originator of this unique pithy statement expressing a rule of conduct. Maslow’s Jewish parents migrated from Russia to the United States to escape from harsh conditions and sociopolitical turmoil. He was born Brooklyn, New York, in April 1908 and died from a heart attack in June 1970. 11 for structure typically results in finding structure. All data have spuri- ous structures, which are formed by the “forces” that make things come together, such as chance. The bigger the data, the greater are the odds that spurious structures abound. Thus, an expectation of data mining is that it produces structures, both real and spurious, without distinction between them. digm. They define data mining as any process that finds unexpected structures in data and uses the EDA framework to ensure that the process explores the data, not exploits it (see Figure 1.1). Note the word unexpected, which suggests that the process is exploratory rather than a confirmation that an expected structure has been found. By finding what one expects to find, there is no longer uncertainty regarding the existence of the structure. make adjustments to minimize the number of spurious structures identified. analyses that search for interesting structure, such as adjusting the overall alpha level/type I error rate or inflating the degrees of freedom [13, 14]. In data mining, the statistician has no explicit analytical adjustments available, only the implicit adjustments affected by using the EDA paradigm itself. The steps discussed next outline the data mining/EDA paradigm. As expected from EDA, the steps are defined by soft rules. response to a future mail campaign. The following represent the steps that need to be taken: Obtain the database that has similar mailings to the future mail campaign. up to 100,000. calculations to determine interesting or noticeable structures. are not necessarily the results and should not be declared signifi- cant findings. structures form natural groups? Do the groups make sense; is there consistency among the structures within a group? Try more techniques. Repeat the many exploratory passes with several fresh samples drawn from the database. Check for con- sistency across the multiple passes. If results do not behave in a 12 Statistical and Machine-Learning Data Mining similar way, there may be no structure to predict response to a future mailing, as chance may have infected your data. If results behave similarly, then assess the variability of each structure and each group. ing response to a future mailing. Coined by Samuel in 1959, the term machine learning (ML) was given to the field of study that assigns computers the ability to learn without being explicitly programmed [15]. In other words, ML investigates ways in which the computer can acquire knowledge directly from data and thus learn to solve problems. It would not be long before ML would influence the statisti- cal community. assumptions of classical statistics [16]. They developed the automatic inter- action detection (AID) regression tree, a methodology without assumptions. sional patterns and relationships in data and serves as an assumption-free, nonparametric alternative to regression prediction and classification anal- yses. Many statisticians believe that AID marked the beginning of an ML approach to solving statistical problems. There have been many improve- ments and extensions of AID: THAID, MAID, CHAID (chi-squared auto- matic interaction detection), and CART, which are now considered viable and quite accessible data mining tools. CHAID and CART have emerged as the most popular today. intensive techniques that need the PC machine, a necessary condition for an ML method. However, they are not true ML methods because they use explicitly statistical criteria (e.g., chi squared and the F-tests), for the learn- ing. A genuine ML method has the PC itself learn via mimicking the way humans think. Thus, I must use the term quasi. Perhaps a more appropriate and suggestive term for AID-type procedures and other statistical problems using the PC machine is statistical ML. been developing algorithms to automate the induction process, which pro- vided another alternative to regression analysis. In 1979, Quinlan used the well-known concept learning system developed by Hunt et al. to implement one of the first intelligent systems—ID3—which was succeeded by C4.5 and C5.0 [17, 18]. These algorithms are also considered data mining tools but have not successfully crossed over to the statistical community. 13 The interface of statistics and ML began in earnest in the 1980s. ML researchers became familiar with the three classical problems facing statisticians: regression (predicting a continuous outcome variable), clas- sification (predicting a categorical outcome variable), and clustering (gen- erating a few composite variables that carry a large percentage of the information in the original variables). They started using their machinery (algorithms and the PC) for a nonstatistical, assumption-free nonparamet- ric approach to the three problem areas. At the same time, statisticians began harnessing the power of the desktop PC to influence the classical problems they know so well, thus relieving themselves from the starchy parametric road. neural networks, support vector machines, fuzzy logic, genetic algorithms and programming, information retrieval, knowledge acquisition, text pro- cessing, inductive logic programming, expert systems, and dynamic pro- gramming. All areas have the same objective in mind but accomplish it with their own tools and techniques. Unfortunately, the statistics community and the ML subgroups have no real exchanges of ideas or best practices. They create distinctions of no distinction. In the spirit of EDA, it is incumbent on data analysts to try something new and retry something old. They can benefit not only from the computational power of the PC in doing the heavy lifting of big data but also from the ML ability of the PC in uncovering structure nestled in big data. In the spirit of trying something old, statistics still has a lot to offer.Thus, today’s data mining can be defined in terms of three easy concepts: 1. descriptive and noninferential parts of classical statistical machin- ery as indicators. The parts include sum of squares, degrees of freedom, F-ratios, chi-square values, and p values, but exclude infer- ential conclusions. environment. However, because small data are a component of big data, they are not excluded. cessing unit, having the ability to learn without being explicitly programmed and the intelligence to find structure in the data."
7,Explain why data mining is defined today as a ll of statistics and EDA for big and small data with the power of PC for the lifting of data and learning the structures within the data,"Thus, today’s data mining can be defined in terms of three easy concepts: 1. descriptive and noninferential parts of classical statistical machin- ery as indicators. The parts include sum of squares, degrees of freedom, F-ratios, chi-square values, and p values, but exclude infer- ential conclusions. environment. However, because small data are a component of big data, they are not excluded. cessing unit, having the ability to learn without being explicitly programmed and the intelligence to find structure in the data.Moreover, the PC is essential for big data, as it can always do what it is explicitly programmed to do. Statistical and Machine-Learning Data Mining In view of these terms, the following data mining mnemonic can be formed: Data Mining = Statistics + Big Data + Machine Learning and Lifting Hence,data mining is defined today as a ll of statistics and EDA for big and small data with the power of PC for the lifting of data and learning the structures within the data.","† Tukey, via his groundbreaking EDA book, put the concept of “reexpression” in the forefront of EDA data mining tools; yet, he never provided any definition. I assume he believed that the term is self-explanatory. Tukey’s first mention of reexpression is in a question on page 61 of his work: “What is the single most needed form of re-expression?” I, for one, would like a definition of reexpression, and I provide one further in the book. 9 observations and learning from the variables. Although it depends on who is analyzing the data, a sample size greater than 50,000 individuals can be con- sidered big. Thus, calculating the average income from a database of 2 mil- lion individuals requires heavy-duty lifting (number crunching). In terms of learning or uncovering the structure among the variables, big can be consid- ered 50 variables or more. Regardless of which side the data analyst is work- ing, EDA scales up for both rows and columns of the data table. There are three distinguishable characteristics of data size: condition , location , and population. Condition refers to the state of readiness of the data for analysis. Data that require minimal time and cost to clean, before reliable analysis can be performed, are said to be well conditioned; data that involve a substantial amount of time and cost are said to be ill conditioned. Small data are typically clean and therefore well conditioned. ates data flowing continuously from all directions at unprecedented speed and volume, and these data usually require cleansing. They are considered “dirty” mainly because of the merging of multiple sources. The merging pro- cess is inherently a time-intensive process, as multiple passes of the sources must be made to get a sense of how the combined sources fit together. Because of the iterative nature of the process, the logic of matching individual records across sources is at first “fuzzy,” then fine-tuned to soundness; until that point, unexplainable, seemingly random, nonsensical values result. Thus, big data are ill conditioned. small data, big data reside in relational databases consisting of a set of data tables. The link among the data tables can be hierarchical (rank or level dependent) or sequential (time or event dependent). Merging of multiple data sources, each consisting of many rows and columns, produces data of even greater number of rows and columns, clearly suggesting bigness. teristics in common and related to the study under consideration. Small data ideally represent a random sample of a known population that is not expected to encounter changes in its composition in the near future. The data are collected to answer a specific problem, permitting straightforward answers from a given problem-specific method. In contrast, big data often represent multiple, nonrandom samples of unknown populations, shifting in composition within the short term. Big data are “secondary” in nature; that is, they are not collected for an intended purpose. They are available from the hydra of marketing information, for use on any post hoc problem, and may not have a straightforward solution. big data per se. However, he did predict that the cost of computing, in 10 Statistical and Machine-Learning Data Mining both time and dollars, would be cheap, which arguably suggests that he knew big data were coming. Regarding the cost, clearly today’s PC bears this out. The data size discussion raises the following question: “How large should a sample be?” Sample size can be anywhere from folds of 10,000 up to 100,000. over 15 years and a statistics instructor who analyzes deceivingly simple cross tabulations with the basic statistical methods as my data mining tools, I have observed that the less-experienced and -trained data analyst uses sam- ple sizes that are unnecessarily large. I see analyses performed on and mod- els built from samples too large by factors ranging from 20 to 50. Although the PC can perform the heavy calculations, the extra time and cost in getting the larger data out of the data warehouse and then processing them and thinking about it are almost never justified. Of course, the only way a data analyst learns that extra big data are a waste of resources is by performing small versus big data comparisons, a step I recommend. The term data mining emerged from the database marketing community sometime between the late 1970s and early 1980s. Statisticians did not under- stand the excitement and activity caused by this new technique since the discovery of patterns and relationships (structure) in the data is not new to them. They had known about data mining for a long time, albeit under various names, such as data fishing, snooping, and dredging, and most dis- paraging, “ransacking” the data. Because any discovery process inherently exploits the data, producing spurious findings, statisticians did not view data mining in a positive light. you have a hammer in hand, you tend eventually to start seeing nails.” The statistical version of this maxim is, “Simply looking for something increases the odds that something will be found.” Therefore, looking * Abraham Maslow brought to the world of psychology a fresh perspective with his concept of “humanism,” which he referred to as the “third force” of psychology after Pavlov’s “behaviorism” and Freud’s “psychoanalysis.” Maslow’s hammer is frequently used without any- body seemingly knowing the originator of this unique pithy statement expressing a rule of conduct. Maslow’s Jewish parents migrated from Russia to the United States to escape from harsh conditions and sociopolitical turmoil. He was born Brooklyn, New York, in April 1908 and died from a heart attack in June 1970. 11 for structure typically results in finding structure. All data have spuri- ous structures, which are formed by the “forces” that make things come together, such as chance. The bigger the data, the greater are the odds that spurious structures abound. Thus, an expectation of data mining is that it produces structures, both real and spurious, without distinction between them. digm. They define data mining as any process that finds unexpected structures in data and uses the EDA framework to ensure that the process explores the data, not exploits it (see Figure 1.1). Note the word unexpected, which suggests that the process is exploratory rather than a confirmation that an expected structure has been found. By finding what one expects to find, there is no longer uncertainty regarding the existence of the structure. make adjustments to minimize the number of spurious structures identified. analyses that search for interesting structure, such as adjusting the overall alpha level/type I error rate or inflating the degrees of freedom [13, 14]. In data mining, the statistician has no explicit analytical adjustments available, only the implicit adjustments affected by using the EDA paradigm itself. The steps discussed next outline the data mining/EDA paradigm. As expected from EDA, the steps are defined by soft rules. response to a future mail campaign. The following represent the steps that need to be taken: Obtain the database that has similar mailings to the future mail campaign. up to 100,000. calculations to determine interesting or noticeable structures. are not necessarily the results and should not be declared signifi- cant findings. structures form natural groups? Do the groups make sense; is there consistency among the structures within a group? Try more techniques. Repeat the many exploratory passes with several fresh samples drawn from the database. Check for con- sistency across the multiple passes. If results do not behave in a 12 Statistical and Machine-Learning Data Mining similar way, there may be no structure to predict response to a future mailing, as chance may have infected your data. If results behave similarly, then assess the variability of each structure and each group. ing response to a future mailing. Coined by Samuel in 1959, the term machine learning (ML) was given to the field of study that assigns computers the ability to learn without being explicitly programmed [15]. In other words, ML investigates ways in which the computer can acquire knowledge directly from data and thus learn to solve problems. It would not be long before ML would influence the statisti- cal community. assumptions of classical statistics [16]. They developed the automatic inter- action detection (AID) regression tree, a methodology without assumptions. sional patterns and relationships in data and serves as an assumption-free, nonparametric alternative to regression prediction and classification anal- yses. Many statisticians believe that AID marked the beginning of an ML approach to solving statistical problems. There have been many improve- ments and extensions of AID: THAID, MAID, CHAID (chi-squared auto- matic interaction detection), and CART, which are now considered viable and quite accessible data mining tools. CHAID and CART have emerged as the most popular today. intensive techniques that need the PC machine, a necessary condition for an ML method. However, they are not true ML methods because they use explicitly statistical criteria (e.g., chi squared and the F-tests), for the learn- ing. A genuine ML method has the PC itself learn via mimicking the way humans think. Thus, I must use the term quasi. Perhaps a more appropriate and suggestive term for AID-type procedures and other statistical problems using the PC machine is statistical ML. been developing algorithms to automate the induction process, which pro- vided another alternative to regression analysis. In 1979, Quinlan used the well-known concept learning system developed by Hunt et al. to implement one of the first intelligent systems—ID3—which was succeeded by C4.5 and C5.0 [17, 18]. These algorithms are also considered data mining tools but have not successfully crossed over to the statistical community. 13 The interface of statistics and ML began in earnest in the 1980s. ML researchers became familiar with the three classical problems facing statisticians: regression (predicting a continuous outcome variable), clas- sification (predicting a categorical outcome variable), and clustering (gen- erating a few composite variables that carry a large percentage of the information in the original variables). They started using their machinery (algorithms and the PC) for a nonstatistical, assumption-free nonparamet- ric approach to the three problem areas. At the same time, statisticians began harnessing the power of the desktop PC to influence the classical problems they know so well, thus relieving themselves from the starchy parametric road. neural networks, support vector machines, fuzzy logic, genetic algorithms and programming, information retrieval, knowledge acquisition, text pro- cessing, inductive logic programming, expert systems, and dynamic pro- gramming. All areas have the same objective in mind but accomplish it with their own tools and techniques. Unfortunately, the statistics community and the ML subgroups have no real exchanges of ideas or best practices. They create distinctions of no distinction. In the spirit of EDA, it is incumbent on data analysts to try something new and retry something old. They can benefit not only from the computational power of the PC in doing the heavy lifting of big data but also from the ML ability of the PC in uncovering structure nestled in big data. In the spirit of trying something old, statistics still has a lot to offer.Thus, today’s data mining can be defined in terms of three easy concepts: 1. descriptive and noninferential parts of classical statistical machin- ery as indicators. The parts include sum of squares, degrees of freedom, F-ratios, chi-square values, and p values, but exclude infer- ential conclusions. environment. However, because small data are a component of big data, they are not excluded. cessing unit, having the ability to learn without being explicitly programmed and the intelligence to find structure in the data.Moreover, the PC is essential for big data, as it can always do what it is explicitly programmed to do. Statistical and Machine-Learning Data Mining In view of these terms, the following data mining mnemonic can be formed: Data Mining = Statistics + Big Data + Machine Learning and Lifting Thus, data mining is defined today as a ll of statistics and EDA for big and small data with the power of PC for the lifting of data and learning the structures within the data. Explicitly referring to big and small data implies the process works equally well on both.Again, in the spirit of EDA, it is prudent to parse the mnemonic equation. focuses on the rows of the data table and uses the capacity of the PC in terms of MIPS (million instructions per second), the speed in which explicitly pro- grammed steps are executed. Calculating the average income of 1 million individuals is an example of PC lifting. PC to find the structure within the columns without being explicitly pro- grammed. Learning is more demanding on the PC than lifting in the same way that learning from books is always more demanding than merely lifting the books. Identifying structure, such as the square root of (a2 + b2), is an example of PC learning. are subpopulations or clusters), the PC has to learn the rows and their rela- tionships to each other to identify the row structures. Thus, when lifting and learning of the rows are required in addition to learning within the columns, the PC must work exceptionally hard but can yield extraordinary results."
8,Explain why there is a 99% (and of course 95%) confidence level that there is an association between RS and HI,"4. TS equals 8 (= 10 - 1 - 1), which is greater than or equal to the 95% and 99% confidence cutoff scores 7 and 8, respectively. Cutoff Scores for General Association Test (95% and 99% Confidence Levels) N 95% 99% 8–9 6 — 10–11 7 8 12–13 9 10 14–15 10 11 16–17 11 12 18–19 12 14 20–21 14 15 22–23 15 16 24–25 16 17 26–27 17 19 28–29 18 20 30–31 19 21 32–33 21 22 34–35 22 24 36–37 23 25 38–39 24 26 40–41 25 27 42–43 26 28 44–45 27 30 46–47 29 31 48–49 30 32 50–51 31 33 28 Statistical and Machine-Learning Data Mining Symbol is Value of Slice. 0 0.028 0.026 e 2 1 ons 3 Resp 0.024 4 9 5 0.022 6 8 7 0.020 6000 8000 10000 12000 14000 16000 18000 20000 22000 24000 26000 28000 Household Income Figure 2.6 General association test for smoothed RS-HI scatterplot. Hence,there is a 99% (and of course 95%) confidence level that there is an association between RS and HI.","3. The line segment formed by points 4 and 5 in Figure 2.6 is the only segment that crosses the medial line. Accordingly, m = 1.4. TS equals 8 (= 10 - 1 - 1), which is greater than or equal to the 95% and 99% confidence cutoff scores 7 and 8, respectively. Cutoff Scores for General Association Test (95% and 99% Confidence Levels) N 95% 99% 8–9 6 — 10–11 7 8 12–13 9 10 14–15 10 11 16–17 11 12 18–19 12 14 20–21 14 15 22–23 15 16 24–25 16 17 26–27 17 19 28–29 18 20 30–31 19 21 32–33 21 22 34–35 22 24 36–37 23 25 38–39 24 26 40–41 25 27 42–43 26 28 44–45 27 30 46–47 29 31 48–49 30 32 50–51 31 33 28 Statistical and Machine-Learning Data Mining Symbol is Value of Slice. 0 0.028 0.026 e 2 1 ons 3 Resp 0.024 4 9 5 0.022 6 8 7 0.020 6000 8000 10000 12000 14000 16000 18000 20000 22000 24000 26000 28000 Household Income Figure 2.6 General association test for smoothed RS-HI scatterplot.Thus, there is a 99% (and of course 95%) confidence level that there is an association between RS and HI. The RS smooth trace line in Figure 2.5 sug- gests that the observed relationship between RS and Hi appears to be poly- nomial to the third power. Accordingly, the linear (HI), quadratic (HI2), and cubed (HI3) Household Income terms should be tested in the Response model. It should be clear that an analysis based on the uncritical use of the coeffi- cient correlation is problematic. The strength of a relationship between two variables cannot simply be taken as the calculated r value itself. The testing for the linearity assumption, which is made easy by the simple scatterplot or smoothed scatterplot, is necessary for a “thorough-and-ready” analysis. If the observed relationship is linear, then the r value can be taken at face value for the strength of the relationship at hand. If the observed relationship is not linear, then the r value must be disregarded or used with extreme caution. tionship, its scatter can be tested for randomness or for a noticeable general Two Basic Data Mining Methods for Variable Assessment 29 association by the proposed nonparametric method. If the former is true, then it is concluded there is no association between the variables. If the lat- ter is true, then the predictor variable is reexpressed to reflect the observed relationship and therefore tested for inclusion in the model. 1. Anscombe, F.J., Graphs in statistical analysis, American Statistician, 27, 17–22, 1973.2. Tukey, J.W., Exploratory Data Analysis, Addison-Wesley, Reading, MA, 1997."
9,Explain why YEAR is measured on an interval scale,"a. For example, WEIGHT has a natural 0 at no weight. Thus, it makes sense to say that my beagle Matzi weighing 26 pounds is twice as heavy as my dachshund Dappy weighing 13 pounds. YEAR 0 is arbitrary, and it is not sensible to say that the year 2000 is twice as old as the year 1000. Hence,YEAR is measured on an interval scale.","iv. Interval scales do not have a true zero point; therefore, it is not possible to make statements about how many times happier one score is than another. mon example of interval data is the Fahrenheit scale for temperature. Equal differences on this scale represent equal differences in temperature, but a temperature of 30° is not twice as warm as a temperature of 15°. That is, 30° - 20° = 20° - 10°, but 20°/10° is not equal to 2. That is, 20° is not twice as hot as 10°. has an absolute zero. Thus, a temperature of 300 K is twice as high as a temperature of 150 K. 5. What is a true zero? Some scales of measurement have a true or natural zero.a. For example, WEIGHT has a natural 0 at no weight. Thus, it makes sense to say that my beagle Matzi weighing 26 pounds is twice as heavy as my dachshund Dappy weighing 13 pounds. YEAR 0 is arbitrary, and it is not sensible to say that the year 2000 is twice as old as the year 1000. Thus, YEAR is measured on an interval scale. Statistical and Machine-Learning Data Mining 6. Note: Some data analysts unfortunately make no distinction between interval or ratio data, calling them both continuous. Moreover, most data analysts put their heads in the sand to treat ordinal data, which assumes numerical values, as interval data. In both situations, this is not correct technically. The stem-and-leaf display is a graphical presentation of quantitative data to assist in visualizing the relative density and shape of a distribution. A salient feature of the display is that it retains the original data to at least two significant digits and puts the data in order. A basic stem-and-leaf display is drawn with two columns separated by a vertical line. The left column con- tains the stems, and the right column contains the leaves. Typically, the leaf contains the last digit of the number, and the stem contains all of the other digits. In the case of very large numbers, the data values may be rounded to a particular place value (e.g., the hundredths place) that will be used for the leaves. The remaining digits to the left of the rounded place value are used as the stem. The stem-and-leaf display is also useful for highlighting outliers and finding the mode. The display is most useful for datasets of moderate “EDA” size (around 250 data points), after which the stem-and-leaf display becomes a histogram, rotated counterclockwise 90°, and all the digits of the leaves are represented by a *. The box-and-whiskers plot provides a detailed visual summary of various features of a distribution. The box stretches from the bottom horizontal line, the lower hinge, defined as the 25th percentile, to the top horizontal line, the upper hinge, defined as the 75th percentile. The box is completed by adding the vertical lines on both ends of the hinges. The horizontal line within the box is the median. The + represents the mean.The H-spread is defined as the difference between the hinges, and a step is defined as 1.5 times the H-spread. Inner fences are one step beyond the hinges."
10,"Explain why X0 = log X, and Y0 = log Y","Corresponding to values ½, -½, and -1, they are called the square root of X, negative reciprocal square root of X, and negative reciprocal of X, respec- tively. Similarly, for the Y variables, they are called square root of Y, nega- tive reciprocal square root of Y, and negative reciprocal of Y, respectively. defined as log to base 10. Hence,X0 = log X, and Y0 = log Y.","When the X-Y relationship has a bulge similar to any one of the four shapes, 106 Statistical and Machine-Learning Data Mining Y up Y up X down X up 2 1 3 4 Y down Y down X down X up FIguRe 8.3 The bulging rule. “rung” in the ladder, are used to straighten out the bulge. Most data have bulges. However, when kinks or elbows characterize the data, then another approach is required, which is discussed further in the chapter. Going up-ladder of powers means reexpressing a variable by raising it to a power p greater than 1. (Remember that a variable raised to the power of 1 is still that variable; X1 = X, and Y1 = Y). The most common p values used are 2 and 3. Sometimes values higher up-ladder and in-between values like 1.33 are used. Accordingly, starting at p = 1, the data miner goes up-ladder, resulting in reexpressed variables, for X and Y, as follows: Starting at X1: X2, X3, X4, X5, … Starting at Y1: Y2, Y3, Y4, Y5, … Some variables reexpressed going up-ladder have special names. cubed, respectively. Similarly, for the Y variables, they are called Y squared and Y cubed, respectively. it to a power p that is less than 1. The most common p-values are ½, 0, -½, and -1. Sometimes, values lower down-ladder and in-between values like 0.33 are used. Also, for negative powers, the reexpressed variable now sports a negative sign (i.e., is multiplied by -1); the reason for this is theo- retical and beyond the scope of this chapter. Accordingly, starting at p = 1, the data miner goes down-ladder, resulting in reexpressed variables for X and Y, as follows: Starting at X1: X1/2, X0, X–1/2, –X1, Starting at Y1: Y1/2, Y0, Y–1/2, –Y1, Logistic Regression: The Workhorse of Response Modeling 107 Some reexpressed variables going down-ladder have special names.Corresponding to values ½, -½, and -1, they are called the square root of X, negative reciprocal square root of X, and negative reciprocal of X, respec- tively. Similarly, for the Y variables, they are called square root of Y, nega- tive reciprocal square root of Y, and negative reciprocal of Y, respectively. defined as log to base 10. Thus, X0 = log X, and Y0 = log Y. 8.6.2 bulging Rule The bulging rule states the following: 1. If the data have a shape similar to that in the first quadrant, then the data miner tries reexpressing by going up-ladder for X, Y, or both.2. If the data have a shape similar to that shown in the second quad- rant, then the data miner tries reexpressing by going down-ladder for X or up-ladder for Y. 3. If the data have a shape similar to that in the third quadrant, then the data miner tries reexpressing by going down-ladder for X, Y, or both."
11,Explain why the correlation coefficient on aggregated data serves as a gross indicator of the strength of the original X-Y relationship at hand,"In Chapter 2, I referred to a “linear assumption,” in that the underlying relationship between X and Y is linear. The second assumption is an implicit one: The (X, Y) data points are at the individual level. When the (X, Y) points are analyzed at an aggregate level, such as in the logit plot and other plots presented in this chapter, the correlation coefficient based on “big” points tends to produce a “big” r value, which serves as a gross estimate of the individual-level r value. The aggregation of data diminishes the idiosyn- crasies of the individual (X, Y) points, thereby increasing the resolution of the relationship, for which the r value also increases. Hence,the correlation coefficient on aggregated data serves as a gross indicator of the strength of the original X-Y relationship at hand.","4. If the data have a shape similar to that in the fourth quadrant, then the data miner tries reexpressing by going up-ladder for X or down- ladder for Y. Reexpressing is an important, yet fallible, part of EDA detective work. deterioration of information. Here is why: Reexpression (going down too far) has the potential to squeeze the data so much that its values become indistinguishable, resulting in a loss of information. Expansion (going up too far) can potentially pull apart the data so much that the new far- apart values lie within an artificial range, resulting in a spurious gain of information. and soundness. Data miners can always go to the extremes of the ladder by exerting their will to obtain a little more straightness, but they must be mindful of a consequential loss of information. Sometimes, it is evi- dent when one has gone too far up/down on the ladder; there is power p, after which the relationship either does not improve noticeably or inex- plicably bulges in the opposite direction due to a corruption of informa- tion. I recommend using discretion to avoid overstraightening and its potential deterioration of information. In addition, I caution that extreme reexpressions are sometimes due to the extreme values of the original variables. Thus, always check the maximum and minimum values of the 108 Statistical and Machine-Learning Data Mining original variables to make sure they are reasonable before reexpressing the variables. The correlation coefficient measures the strength of the straight-line or linear relationship between two variables, X and Y, discussed in detail in Chapter 2. However, there is an additional assumption to consider.In Chapter 2, I referred to a “linear assumption,” in that the underlying relationship between X and Y is linear. The second assumption is an implicit one: The (X, Y) data points are at the individual level. When the (X, Y) points are analyzed at an aggregate level, such as in the logit plot and other plots presented in this chapter, the correlation coefficient based on “big” points tends to produce a “big” r value, which serves as a gross estimate of the individual-level r value. The aggregation of data diminishes the idiosyn- crasies of the individual (X, Y) points, thereby increasing the resolution of the relationship, for which the r value also increases. Thus, the correlation coefficient on aggregated data serves as a gross indicator of the strength of the original X-Y relationship at hand. There is a drawback of aggregation: It often produces r values without noticeable differences because the power of the distinguishing individual-level information is lost. Returning to the LGT_TXN logit plot for FD1_OPEN, whose bulging relation- ship is in need of straightening, I identify its bulge as the type in quadrant 2 in Figure 8.3. According to the bulging rule, I should try going up-ladder for LGT_TXN or down-ladder for FD1_OPEN. LGT_TXN cannot be reexpressed because it is the explicit dependent variable as defined by the logistic regres- sion framework. Reexpressing it would produce grossly illogical results.To go down-ladder for FD1_OPEN, I use the powers ½, 0, -½, -1, and -2. to base 10 of FD1_OPEN, labeled FD1_LOG; the negative reciprocal root of FD1_OPEN, labeled FD1_RPRT; the negative reciprocal of FD1_OPEN, labeled FD1_RCP; and the negative reciprocal square of FD1_OPEN, labeled FD1_RSQ. The corresponding LGT_TXN logit plots for these reexpressed variables and the original FD1_OPEN (repeated here for convenience) are in Figure 8.4. FD1_RPRT do an equal job of straightening the data. I could choose any of them but decide to do a little more detective work by looking at the numerical Logistic Regression: The Workhorse of Response Modeling 109 Plot of LGT_TXN*FD1_OPEN Plot of LGT_TXN*FD1_SQRT LGT_TXN LGT_TXN –1.0 –1.0 X X –1.5 X –1.5 X –2.0 –2.0 X X –2.5 –2.5 1 2 3 1.00 1.25 1.50 1.75 FD1_OPEN FD1_SQRT Plot of LGT_TXN*FD1_LOG Plot of LGT_TXN*FD1_RPRT LGT_TXN –1.0 LGT_TXN –1.0 X X –1.5 X –1.5 X –2.0 –2.0 X X –2.5 –2.5 0.00 0.25 0.50 0.75 –1.0 –0.8 –0.6 –0.4 FD1_LOG FD1_RPRT Plot of LGT_TXN*FD1_RCP Plot of LGT_TXN*FD1_RSQ –1.0 LGT_TXN –1.0 LGT_TXN X X –1.5 X –1.5 X –2.0 –2.0 X X –2.5 –2.5 –1.000 –0.833 –0.667 –0.500 –0.333 –1.0 –0.5 0.0 FD1_RCP FD1_RSQ FIguRe 8.4 Logit plots for FD1_OPEN and its reexpressed variables. variable—to support my choice of the best reexpressed variable. The larger the correlation coefficient, the more effective the reexpressed variable is in straightening the data. Thus, the reexpressed variable with the largest cor- relation coefficient is declared the best reexpressed variable, with exceptions guided by the data miner’s own experience with these visual and numerical indictors in the context of the problem domain. reexpressed variable are ranked in descending order in Table 8.8. The corre- lation coefficients of the reexpressed variables represent noticeable improve- ments in straightening the data over the correlation coefficient for the original variable FD1_OPEN (r = 0.907). FD1_RSQ has the largest correlation 110 Statistical and Machine-Learning Data Mining Table 8.8 Correlation Coefficients between LGT_TXN and Reexpressed FD1_OPEN FD1_RSQ FD1_RCP FD1_RPRT FD1_LOG FD1_SQRT FD1_OPEN 0.998 0.988 0.979 0.960 0.937 0.907 Table 8.9 Correlation Coefficients between LGT_TXN and Reexpressed FD2_OPEN FD2_RSQ FD2_RCP FD2_RPRT FD2_LOG FD2_SQRT FD2_OPEN 0.995 0.982 0.968 0.949 0.923 0.891 coefficient (r = 0.998), but it is slightly greater than that for FD1_RCP (r = 0.988) and therefore not worthy of notice. an 8.9% (= (0.988 - 0.907)/0.907) improvement in straightening the data over the original relationship with FD1_OPEN. I prefer FD1_RCP over FD1_RSQ and other extreme reexpressions down-ladder (defined by power p less than -2) because I do not want to unwittingly select a reexpression that might be too far down-ladder, resulting in loss of information. Thus, I go back one rung to power -1, hoping to get the right balance between straightness and minimal loss of information. The scenario for FD2_OPEN is virtually identical to the one presented for FD1_OPEN. This is not surprising as FD1_OPEN and FD2_OPEN share a large amount of information. The correlation coefficient between the two variables is 0.97, meaning the two variables share 94.1% of their variation. Table 8.9). The relationship between the LGT_TXN and investment, depicted in the plot in Figure 8.5, is somewhat straight with a negative slope and a slight bulge in the middle for investment values 3, 4 and 5. I identify the bulge of the type in quadrant 3 in Figure 8.3. Thus, going down-ladder for powers ½, 0, -½, -1, and -2 results in the square root of investment, labeled INVEST_SQRT; the log to base 10 of investment, labeled INVEST_LOG; the negative reciprocal root of investment, labeled INVEST_RPRT; the negative reciprocal of investment, labeled INVEST_RCP; and the negative reciprocal square of INVESTMENT, labeled INVEST_RSQ. The corresponding LGT_TXN logit plots for these reexpressed variables and the original INVESTMENT are in Figure 8.5. 111 Plot of LGT_TXN*INVESTMENT Plot of LGT_TXN*INVEST_SQRT LGT_TXN LGT_TXN –1.75 X –1.75 X –1.80 X –1.80 X –1.85 –1.85 –1.90 –1.90 –1.95 X –1.95 X –2.00 X X –2.00 X X –2.05 X –2.05 X 1 2 3 4 5 6 1.0 1.5 2.0 2.5 INVESTMENT INVEST_SQRT Plot of LGT_TXN*INVEST_LOG Plot of LGT_TXN*INVEST_RPRT LGT_TXN LGT_TXN –1.75 X –1.75 X –1.80 X –1.80 X –1.85 –1.85 –1.90 –1.90 –1.95 X –1.95 X –2.00 X X –2.00 X X –2.05 X –2.05 X 0.00 0.25 0.50 0.75 1.00 –1.00 –0.75 –0.50 –0.25 INVEST_LOG INVEST_RPRT Plot of LGT_TXN*INVEST_RCP Plot of LGT_TXN*INVEST_RSQ LGT_TXN LGT_TXN –1.75 X –1.75 X –1.80 X –1.80 X –1.85 –1.85 –1.90 –1.90 –1.95 X –1.95 X –2.00 X X –2.00 XX –2.05 X –2.05 X –1.00 –0.75 –0.50 –0.25 0.00 –1.00 –0.75 –0.50 –0.25 0.00 INVEST_RCP INVEST_RSQ FIguRe 8.5 Logit plots for INVESTMENT and its reexpressed variables. INVEST_LOG has the largest correlation coefficient, which supports the sta- tistical factoid that claims a variable in dollar units should be reexpressed with the log function. The correlation coefficients for INVEST_LOG and INVEST_SQRT in Table 8.10 are -0.978 and -0.966, respectively; admittedly, the correlation coefficients do not reflect a noticeable difference. My choice of the best reexpression for investment is INVEST_LOG because I prefer the statistical factoid to my visual choice. Only if a noticeable difference between correlation coefficients for INVEST_LOG and INVEST_SQRT existed would I sway from being guided by the factoid. INVEST_LOG represents an improvement of 3.4% (= (0.978 - 0.946)/0.946; disregarding the negative sign) 112 Statistical and Machine-Learning Data Mining Table 8.10 Correlation Coefficients between LGT_TXN and Reexpressed INVESTMENT INVEST_log INVEST_sqrt INVEST_rprt INVESTMENT INVEST_rcp INVEST_rsq –0.978 –0.966 –0.950 0.946 –0.917 –0.840 in straightening the data over the relationship with the original variable INVESTMENT (r = -0.946). I describe two plotting techniques for uncovering the correct reexpression when the bulging rule does not apply. After discussing the techniques, I return to the next variable for reexpression, MOS_OPEN. The relation- ship between LGT_TXN and MOS_OPEN is interesting and offers an excellent opportunity to illustrate the data mining flexibility of the EDA methodology. qualitatively or to account quantitatively for the relationship in a logit plot."
12,Explain why this is an incipient subset of important variables for predicting TXN_ADD,"From Equation (8.8), the G/df value for each remaining variable in Table 8.12 is greater than 4. Thus, these five variables, each important predic- tors of TXN_ADD, form a starter subset for predicting TXN_ADD. I have not forgotten about FD_TYPE; it is discussed in another section. set; the output is in Table 8.13. From Equation (8.9), the five-variable subset has a G/df value of 40.21(= 201.031/5), which is greater than 4. Hence,this is an incipient subset of important variables for predicting TXN_ADD.","4. Insert the 45° line in the plot. The line serves as a reference for visual assessment of the importance of a structure for predicting response, thereby confirming that the structure under consideration is the cor- rect one. Smooth points on the line imply that the mean predicted response and the mean actual response are equal, and there is great certainty that the structure is the correct one. The tighter the smooth points “hug” the 45° line, the greater the certainty of the structure is. Conversely, the greater the scatter about the line, the lesser the certainty of the structure. Statistical and Machine-Learning Data Mining 8.9 Reexpressing MOS_OPEN The relationship between LGT_TXN and MOS_OPEN in Figure 8.6 is not straight in the full range of MOS_OPEN values from one to six but is straight between values one and five. The LGT_TXN logit plot for MOS_OPEN shows a check mark shape with vertex at MOS_OPEN = 5 as LGT_TXN jumps at MOS_OPEN = 6. Clearly, the bulging rule does not apply. an organization of variables and functions that renders the ideal straight- line relationship between LGT_TXN and the MOS_OPEN structure. It will implicitly account for the jump in logit of in LGT_TXN at MOS_OPEN = 6. TXN_ADD response model. ted logit plot (Figure 8.7), which is based on the logistic regression analysis on TXN_ADD with MOS_OPEN. The LRM, from which the predicted logits are obtained, is defined in Equation (8.4): Logit(TXN_ADD) = -1.24 - 0.17*MOS _OPEN (8.4) It is acknowledged that MOS_OPEN has six distinct values. The fitted logit plot does not reflect the shape of the relationship in the original LGT_TXN logit plot in Figure 8.6. The predicted point at mos_open = 6 is way too low. structure as it does not produce the shape in the original logit plot. –1.5 X X X LGT_TXN –2.0 X –2.5 X 1 2 3 4 5 6 MOS_OPEN FIguRe 8.6 Logit plot of MOS_OPEN. 115 –1.0 X –1.5 X X d_LGT_TXN X icte –2.0 X edpr X –2.5 1 2 3 4 5 6 MOS_OPEN FIguRe 8.7 Fitted logit plot for MOS_OPEN. I generate the TXN_ADD smooth predicted-versus-actual plot for MOS_ OPEN (Figure 8.8), which depicts both the structure under consideration and the reference variable. The smooth predicted values are based on the LRM previously defined in Equation (8.4) and restated here in Equation (8.5) for convenience. (8.5) There are six smooth points, each labeled by the corresponding six values of MOS_OPEN. The scatter about the 45° line is wild, implying MOS_OPEN is not a good predictive structure, especially when MOS_OPEN equals 1, 5, 6, and 4, as their corresponding smooth points are not close to the 45° line. board to jump into LGT_TXN at MOS_OPEN = 6. MOS_OPEN = 1, as the farthest point from the line, strikes me as inexplicable. MOS_OPEN = 4 may be within an acceptable distance from the line. the corresponding smooth points are close to the line. Two good predic- tions of a possible six predictions results in a poor 33% accuracy rate. Thus, MOS_OPEN is not good structure for predicting TXN_ADD. As before, the implication is that MOS_OPEN alone is not the correct structure to reflect the original relationship between LGT_TXN and MOS_OPEN in Figure 8.6. Statistical and Machine-Learning Data Mining Legend: 1–6 are values of MOS_OPEN 0.20 1 0.18 2 0.16 0.14 4 3 0.12 5 d_Prob_TXN 0.10 6 0.08 dicte pre 0.06 0.04 0.02 0.05 0.10 0.15 0.20 0.25 0.30 mean_TXN_ADD FIguRe 8.8 Plot of smooth predicted versus actual for MOS_OPEN. structure, defined as MOS_DUM = 1 if MOS_OPEN = 6; MOS_DUM = 0 if MOS_OPEN not equal to 6. ing of the predicted logits from regressing TXN_ADD on the structure con- sisting of MOS_OPEN and MOS_DUM. The LRM is defined in Equation (8.6): Logit(TXN_ADD) = -0.62 - 0.38*MOS_OPEN + 1.16*MOS_DUM (8.6) This fitted plot accurately reflects the shape of the original relationship between TXN_ADD and MOS_OPEN in Figure 8.6. The implication is that MOS_OPEN and MOS_DUM make up the correct structure of the informa- tion carried in MOS_OPEN. The definition of the structure is the right side of the equation itself. predicted-versus-actual plot in Figure 8.10, consisting of mean predicted logits of TXN_ADD against mean MOS_OPEN. The predicted logits come Logistic Regression: The Workhorse of Response Modeling 117 –1.0 X X –1.5 X d LGT_TXN X icte –2.0 ed X pr –2.5 X 1 2 3 4 5 6 MOS_OPEN FIguRe 8.9 Fitted logit plot for MOS_OPEN and MOS_DUM. 0.275 0.250 1 0.225 0.200 2 0.175 d Prob_TXN 0.150 icte 0.125 ed 63 pr 0.100 4 0.075 5 0.05 0.10 0.15 0.20 0.25 0.30 mean TXN_ADD FIguRe 8.10 Plot of smooth predicted versus actual for MOS_OPEN and MOS_DUM. variable pair MOS_OPEN and MOS_DUM. MOS_OPEN is used as the refer- ence variable. The smooth points hug the 45° line nicely. The implication is that the MOS_OPEN structure defined by MOS_OPEN and MOS_DUM is again confirmed, and the two-piece structure is an important predictive of TXN_ADD. Statistical and Machine-Learning Data Mining 8.10 Assessing the Importance of Variables The classic approach for assessing the statistical significance of a variable considered for model inclusion is the well-known null hypothesis signifi- cance testing procedure, which is based on the reduction in prediction error (actual response minus predicted response) associated with the variable in question. The statistical apparatus of the formal testing procedure for logistic regression analysis consists of: The log likelihood (LL) function, the G statistic, degrees of freedom (df), and the p value. The procedure uses the apparatus within a theoretical framework with weighty and untenable assumptions. From a purist point of view, this could cast doubt on findings that actually have statistical significance. Even if findings of statistical sig- nificance are accepted as correct, they may not be of practical importance or have noticeable value to the study at hand. For the data miner with a prag- matic slant, the limitations and lack of scalability inherent in the classic sys- tem cannot be overlooked, especially within big data settings. In contrast, the data mining approach uses the LL units, the G statistic, and degrees of freedom in an informal data-guided search for variables that suggest a noticeable reduction in prediction error. One point worth noting is that the informality of the data mining approach calls for suitable change in termi- nology, from declaring a result as statistically significant to one worthy of notice or noticeably important. like to comment on the objectivity of the classic approach as well as degrees of freedom. The classic approach is so ingrained in the analytic community that no viable alternative occurs to practitioners, especially an alternative based on an informal and sometimes highly individualized series of steps. tive as it is based on sound probability theory and statistical mathematical machinery. However, the settings of the testing machinery defined by model builders could affect the results. The settings include the levels of rejecting a variable as significant when, in fact, it is not, or accepting a variable as not significant when, in fact, it is. Determining the proper sample size is also a subjective setting as it depends on the amount budgeted for the study. Last, the allowable deviation of violations of test assumptions is set by the model builder’s experience. Therefore, by acknowledging the subjective nature of the classic approach, the model builder can be receptive to the alternative data mining approach, which is free of theoretical ostentation and math- ematical elegance. typically described as a generic measure of the number of independent pieces of information available for analysis. To ensure accurate results, this concept is accompanied by the mathematical adjustment “replace N with N -1.” The concept of degrees of freedom gives a deceptive impression of simplicity in Logistic Regression: The Workhorse of Response Modeling 119 counting the pieces of information. However, the principles used in count- ing are not easy for all but the mathematical statistician. To date, there is no generalized calculus for counting degrees of freedom. Fortunately, the counting already exists for many analytical routines. Therefore, the correct degrees of freedom are readily available; computer output automatically provides them, and there are lookup tables in older statistics textbooks. For the analyses in the following discussions, the counting of degrees of free- dom is provided. In data mining, the assessment of the importance of a subset of variables for predicting response involves the notion of a noticeable reduction in predic- tion error due to the subset of variables and is based on the ratio of the G sta- tistic to the degrees of freedom, G/df. The degrees of freedom is defined as the number of variables in the subset. The G statistic is defined, in Equation (8.7), as the difference between two LL quantities, one corresponding to a model without the subset of variables and the other corresponding to a model with the subset of variables. There are two points worth noting: First, the LL units are multiplied by a factor of -2, a mathematical necessity; second, the term subset is used to imply there is always a large set of variables available from which the model builder considers the smaller subset, which can include a single variable. assessing the likelihood that the variables have some predictive power. In brief, the larger the average G value per degrees of freedom (G/df), the more important the variables are in predicting response. If X is the only variable considered for inclusion into the model, the G statis- tic is defined in Equation (8.8): G = -2LL(model with intercept only) - -2LL(model with X) (8.8) The decision rule for declaring X an important variable in predicting response is as follows: If G/df* is greater than the standard G/df value 4, then X is an important predictor variable and should be considered for inclu- sion in the model. Note that the decision rule only indicates that the variable * Obviously, G/df equals G for a single-predictor variable with df = 1. Statistical and Machine-Learning Data Mining has some importance, not how much importance. The decision rule implies that a variable with a greater G/df value has a greater likelihood of some importance than a variable with a smaller G/df value, not that it has greater importance. When subset A consisting of k variables is the only subset considered for model inclusion, the G statistic is defined in Equation (8.9): G = -2LL(model with intercept) - -2LL(model with A(k) variables) (8.9) The decision rule for declaring subset A important in predicting response is as follows: If G/k is greater than the standard G/df value 4, then subset A is an important subset of the predictor variable and should be considered for inclusion in the model. As before, the decision rule only indicates that the subset has some importance, not how much importance. Let subsets A and B consist of k and p variables, respectively. The number of variables in each subset does not have to be equal. If they are equal, then all but one variable can be the same in both subsets. The G statistics for A and B are defined in Equations (8.10) and (8.11), respectively: G(k) = -2LL(model with intercept) - -2LL(model with “A” variables) (8.10) G(p) = -2LL(model with intercept) - -2LL(model with “B” variables) (8.11) The decision rule for declaring which of the two subsets is more impor- tant (i.e., greater likelihood of having some predictive power) in predicting response is as follows: 1. If G(k)/k is greater than G(p)/p, then subset A is the more impor- tant predictor variable subset; otherwise, B is the more important subset. subsets are to be regarded tentatively of comparable importance. the decision about which subset is better. by the more important subset. Of course, this rule assumes that G(k)/k and G(p)/p are greater than the standard G/df value 4. 121 8.11 Important Variables for Case Study The first step in variable assessment is to determine the baseline LL value for the data under study. The LRM for TXN_ADD without variables produces two essential bits of information in Table 8.11: 1. The baseline for this case study is -2LL equals 3606.488. Logit(TNX_ADD = 1) = -1.9965 (8.12) There are interesting bits of information in Table 8.11 that illustrate two use- ful statistical identities: 1. Exponentiation of both sides of Equation (8.12) produces odds of response equal to 0.1358. Recall that exponentiation is the mathe- matical operation of raising a quantity to a power. The exponentia- tion of a logit is the odds; consequently, the exponentiation of -1.9965 is 0.1358. See Equations (8.13) to (8.15). (8.13) Odds(TNX_ADD = 1) = Exp(-1.9965) (8.14) Odds (TNX_ADD = 1) = 0.1358 (8.15) Table 8.11 The LOGISTIC Procedure for TXN_ADD Response Profile TXN_ADD COUNT 1 589 0 4,337 –2LL = 3,606.488 Parameter Wald Variable Estimate Standard Error Chi-Square Pr > Chi-Square INTERCEPT –1.9965 0.0439 2,067.050 0.0 LOGIT = 1.9965 ODDS = EXP (.19965) = .1358 ODDS 0.1358 PROB(TXN_ADD =1)= = = 0..119 1+ ODDS 1+ 0.1358 122 Statistical and Machine-Learning Data Mining 2. Probability of (TNX_ADD = 1), hereafter the probability of RESPONSE, is easily obtained as the ratio of odds divided by 1 + odds. The implication is that the best estimate of RESPONSE—when no information is known or no variables are used—is 11.9%, namely, the average response of the mailing. With the LL baseline value 3,606.488, I assess the importance of the five vari- ables: MOS_OPEN and MOS_DUM, FD1_RCP, FD2_RCP, and INVEST_LOG. model, I perform a logistic regression analysis on TXN_ADD with MOS_ OPEN and MOS_DUM; the output is in Table 8.12. From Equation (8.9), the G value is 107.022 (= 3,606.488 - 3,499.466). The degree of freedom is equal to the number of variables; df is 2. Accordingly, G/df equals 53.511, which is greater than the standard G/df value of 4. Thus, MOS_OPEN and MOS_DUM as a pair are declared important predictor variables of TXN_ADD.From Equation (8.8), the G/df value for each remaining variable in Table 8.12 is greater than 4. Thus, these five variables, each important predic- tors of TXN_ADD, form a starter subset for predicting TXN_ADD. I have not forgotten about FD_TYPE; it is discussed in another section. set; the output is in Table 8.13. From Equation (8.9), the five-variable subset has a G/df value of 40.21(= 201.031/5), which is greater than 4. Thus, this is an incipient subset of important variables for predicting TXN_ADD. The “mystery” in building a statistical model is that the true subset of variables defining the true model is not known. The model builder can be most productive by seeking to find the best subset of variables that defines the final model as an “intelliguess” of the true model. The final model reflects more of the model builder’s effort given the data at hand than an estimate of the Table 8.12 G and df for Predictor Variables Variable -2LL G df p-value INTERCEPT 3,606.488 MOS_OPEN + MOS_DUM 3,499.466 107.023 2 0.0001 FD1_RCP 3,511.510 94.978 1 0.0001 FD2_RCP 3,503.993 102.495 1 0.0001 INV_LOG 3,601.881 4.607 1 0.0001 Logistic Regression: The Workhorse of Response Modeling 123 Table 8.13 Preliminary Logistic Model for TXN_ADD with Starter Subset Intercept Intercept and All All Only Variables Variables -2LL 3,606.488 3,405.457 201.031 with 5 df (p = 0.0001) Parameter Standard Wald Pr > Variable Estimate Error Chi-Square Chi-Square INTERCEPT 0.9948 0.2462 16.3228 0.0001 FD2_RCP 3.6075 0.9679 13.8911 0.0002 MOS_OPEN -0.3355 0.0383 76.8313 0.0001 MOS_DUM 0.9335 0.1332 49.0856 0.0001 INV_LOG -0.7820 0.2291 11.6557 0.0006 FD1_RCP -2.0269 0.9698 4.3686 0.0366 true model itself. The model builder’s attention has been drawn to the most noticeable, unavoidable collection of predictor variables, whose behavior is known to the extent the logit plots uncover their shapes and their relation- ships to response. of predictor variables consists of variables whose contributions to the predic- tions of the model are often unpredictable and unexplainable. Sometimes, the most important variable in the mix drops from the top, in that its contri- bution in the model is no longer as strong as it was individually. Other times, the least-unlikely variable rises from the bottom, in that its contribution in the model is stronger than it was individually. In the best of times, the vari- ables interact with each other such that their total effect on the predictions of the model is greater than the sum of their individual effects. bilities), it is impossible for the model builder to assess the unique contribution of a variable. In practice, the model builder can assess the relative importance of a variable, specifically, its importance with respect to the presence of the other variables in the model. The Wald chi-square—as posted in logistic regression analysis output—serves as an indicator of the relative importance of a variable, as well as for selecting the best subset. This is discussed in the next section. The decision rules for finding the best subset of important variables consist of the following steps: 1. to be important are probably important; let experience (the model 124 Statistical and Machine-Learning Data Mining builders and others) in the problem domain be the rule. If there are many variables from which to choose, rank the variables based on the correlation coefficient r (between response variable and candi- date predictor variable). One to two handfuls of the experience-based variables, the largest r-valued variables, and some small r-valued vari- ables form the initial subset. The last variables are included because small r values may falsely exclude important nonlinear variables. tionship.) Categorical variables require special treatment as the cor- relation coefficient cannot be calculated. (I illustrate with FD_TYPE how to include a categorical variable in a model in the last section.) 2. For the variables in the initial subset, generate logit plots and straighten the variables as required. The most noticeable handfuls of original and reexpressed variables form the starter subset. the Wald cutoff value of 4 from the model. This results in the first incipient subset of important variables.Perform another logistic regression analysis on the incipient subset. Delete one or two variables with Wald chi-square values less than the Wald cutoff value of 4 from the model. The model builder can create an illusion of important variables appearing and disappearing with the deletion of different variables. The Wald chi-square values can exhibit “bouncing” above and below the Wald cutoff value as the variables are deleted. The bouncing effect is due to the correlation between the “included” variables and the “deleted” variables. A greater correlation implies greater bouncing (unreliability) of Wald chi-square values. implies the greater the uncertainty of declaring important variables. chi-square values. This step often results in different subsets as the model builder deletes judicially different pairings of variables. subsets using the decision rule in Section 8.10.4. I perform a logistic regression on TXN_ADD with the five-variable subset MOS_OPEN and MOS_DUM, FD1_RCP, FD2_RCP, and INVEST_LOG; the output is in Table 8.13. FD1_RCP has the smallest Wald chi-square value, 4.3686. FD2_RCP, which has a Wald chi-square of 13.8911, is highly correlated Logistic Regression: The Workhorse of Response Modeling 125 with FD1_RCP (rFD1_RCP, FD2_RCP = 0.97), thus rendering their Wald chi-square values unreliable. However, without additional indicators for either variable, I accept their “face” values as an indirect message and delete FD1_RCP, the variable with the lesser value. importance given MOS_OPEN, MOS_DUM, FD1_RCP, and FD2_RCP in the model, I also delete INVEST_LOG from the model. Thus, the incipiently best subset consists of FD2_RCP, MOS_OPEN, and MOS_DUM. variable subset (FD2_RCP, MOS_OPEN, and MOS_DUM); the output is in Table 8.14. MOS_OPEN and FD2_RCP have comparable Wald chi-square values, 81.8072 and 85.7923, respectively, which are obviously greater than the Wald cutoff value 4. The Wald chi-square value for MOD_DUM is half of that of MOS_OPEN, and not comparable to the other values. However, MOS_DUM is staying in the model because it is empirically needed (recall Figures 8.9 and 8.10). I acknowledge that MOS_DUM and MOS_OPEN share information, which could be affecting the reliability of their Wald chi-square values. The actual amount of shared information is 42%, which indicates there is a minimal effect on the reliability of their Wald chi-square values. MOS_OPEN, MOS_DUM) and the starter five-variable subset (MOS_OPEN, MOS_DUM, FD1_RCP, FD2_RCP, INVEST_LOG). The G/df values are 62.02 (= 186.058/3 from Table 8.14) and 40.21 (= 201.031/5 from Table 8.13) for the former and latter subsets, respectively. Based on the decision rule in Section 8.10.4, I declare the three-variable subset is better than the five-variable sub- set. Thus, I expect good predictions of TXN_ADD based on the three-vari- able model defined in Equation (8.16): Table 8.14 Logistic Model for TXN_ADD with Best Incipient Subset Intercept and All All Intercept Only Variables Variables -2LL 3,606.488 3,420.430 186.058 with 3 df (p = 0.0001) Parameter Standard Wald Pr > Variable Estimate Error Chi-Square Chi-Square INTERCEPT 0.5164 0.1935 7.1254 0.0076 FD2_RCP 1.4942 0.1652 81.8072 0.0001 MOS_OPEN -0.3507 0.0379 85.7923 0.0001 MOS_DUM 0.9249 0.1329 48.4654 0.0001 126 Statistical and Machine-Learning Data Mining Predicted logit of TXN_ADD = Predicted LGT_TXN = 0.5164 + 1.4942*FD2_RCP - 0.3507*MOS_OPEN + 0.9249*MOS_DUM (8.16) 8.14 Visual Indicators of Goodness of Model Predictions In this section, I provide visual indicators of the quality of model predictions."
13,Explain why the predictions aggregated at the score group level are considered good,"5. For each score group, calculate the mean (smooth) residual and mean (smooth) predicted response, producing a set of paired smooth points (smooth residual, smooth predicted response). 127 7. as a reference line for determining whether a general trend exists in the scatter of smooth points. If the smooth residual plot looks like the ideal or null plot (i.e., has a random scatter about the zero line with about half of the points above the line and the remaining points below), then it is concluded that there is no general trend in the smooth residuals. Hence,the predictions aggregated at the score group level are considered good.","The LRM itself is a variable as it is a sum of weighted variables with the logistic regression coefficients serving as the weights. As such, the logit model predic- tion (e.g., the predicted LGT_TXN) is a variable that has a mean, a variance, and all the other descriptive measures afforded any variable. Also, the logit model prediction can be graphically displayed as afforded any variable. Accordingly, I present three valuable plotting techniques, which reflect the EDA-prescribed “graphic detective work” for assessing the goodness of model predictions. The plot of smooth residual by score groups is defined as the plot consisting of the mean residual against the mean predicted response by score groups, which are identified by the unique values created by preselected variables, typically the predictor variables in the model under consideration. For exam- ple, for the three-variable model, there are 18 score groups: three values of FD2_RCP multiplied by six values of MOS_OPEN. The two values of MOS_ DUM are not unique as they are part of the values of MOS_OPEN. groups and its interpretation are as follows: 1. 8.2.2. as outlined in Section 8.2.2. response minus predicted probability of response. selected variables.5. For each score group, calculate the mean (smooth) residual and mean (smooth) predicted response, producing a set of paired smooth points (smooth residual, smooth predicted response). 127 7. as a reference line for determining whether a general trend exists in the scatter of smooth points. If the smooth residual plot looks like the ideal or null plot (i.e., has a random scatter about the zero line with about half of the points above the line and the remaining points below), then it is concluded that there is no general trend in the smooth residuals. Thus, the predictions aggregated at the score group level are considered good. The desired implication is that on average the predictions at the individual level are also good. dom scatter. This is at best a subjective task as it depends on the model builder’s unwitting nature to see what is desired. To aid in an objective examination of the smooth residual plot, use the general association test discussed in Chapter 2 to determine whether the smooth residual plot is equivalent to the null plot.9. When the smooth residual plot is declared null, look for a local pat- tern. It is not unusual for a small wave of smooth points to form a local pattern, which has no ripple effect to create a general trend in an otherwise-null plot. A local pattern indicates a weakness or weak spot in the model in that there is a prediction bias for the score groups identified by the pattern. I construct the plot of smooth residual by score groups to determine the quality of the predictions of the three-variable (FD2_RCP, MOS_OPEN, MOS_DUM) model. The smooth residual plot in Figure 8.11 is declared to be equivalent to the null-plot-based general association test. Thus, the overall quality of prediction is considered good. That is, on average, the predicted TXN_ADD is equal to the actual TXN_ADD."
14,Explain why if model implementation on a solicitation database can include exception rules (e,"Easily seen, but not easily understood (at this point in the analysis), is the local pattern defined by four score groups (labeled 1 through 4) in the lower right-hand side of the plot. The local pattern explicitly shows that the smooth residuals are noticeably negative. The local pattern indicates a weak spot in the model as its predictions for the individuals in the four score groups have, on average, a positive bias, that is, their predicted TXN_ADD tends to be larger than their actual TXN_ADD. als in a weak spot, then model performance can be enhanced. For example, response models typically have a weak spot as prediction bias stems from limited information on new customers and outdated information on expired customers. Hence,if model implementation on a solicitation database can include exception rules (e.","9. When the smooth residual plot is declared null, look for a local pat- tern. It is not unusual for a small wave of smooth points to form a local pattern, which has no ripple effect to create a general trend in an otherwise-null plot. A local pattern indicates a weakness or weak spot in the model in that there is a prediction bias for the score groups identified by the pattern. I construct the plot of smooth residual by score groups to determine the quality of the predictions of the three-variable (FD2_RCP, MOS_OPEN, MOS_DUM) model. The smooth residual plot in Figure 8.11 is declared to be equivalent to the null-plot-based general association test. Thus, the overall quality of prediction is considered good. That is, on average, the predicted TXN_ADD is equal to the actual TXN_ADD.Easily seen, but not easily understood (at this point in the analysis), is the local pattern defined by four score groups (labeled 1 through 4) in the lower right-hand side of the plot. The local pattern explicitly shows that the smooth residuals are noticeably negative. The local pattern indicates a weak spot in the model as its predictions for the individuals in the four score groups have, on average, a positive bias, that is, their predicted TXN_ADD tends to be larger than their actual TXN_ADD. als in a weak spot, then model performance can be enhanced. For example, response models typically have a weak spot as prediction bias stems from limited information on new customers and outdated information on expired customers. Thus, if model implementation on a solicitation database can include exception rules (e.g., new customers are always targeted—assigned to the top decile) and expired customers are placed in the middle deciles, then the overall quality of prediction is improved. Statistical and Machine-Learning Data Mining Legend: A = smooth point for score group. A AA 0.1 A A A A l 0.0 A A A A A A A oth Residua Smo –0.1 1 2 –0.2 4 3 –0.3 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 Smooth Predicted TXN_ADD FIguRe 8.11 Plot of smooth residual by score group for three-variable (FD2_RCP, MOS_OPEN, and MOS_ DUM) model. smooth residual by score groups/three-variable model are as follows: (1) For the smooth residuals, the minimum and maximum values and the range are -0.26, 0.16, and 0.42, respectively; and (2) the standard deviation of the smooth residuals is 0.124. The plot of smooth actual versus predicted by decile groups is defined as the plot consisting of the mean actual response against the mean predicted response by decile groups. Decile groups are 10 equal-size classes, which are based on Logistic Regression: The Workhorse of Response Modeling 129 the predicted response values from the LRM under consideration. Decile groupings are not an arbitrary partitioning of the data as most database models are implemented at the decile level and consequently are built and validated at the decile level. dicted by decile groups and its interpretation are as follows: 1. as outlined in Section 8.2.2. data by the predicted response values. Then, divide the scored- ranked data into 10 equal-size classes. The first class has the largest mean predicted response, labeled “top”; the next class is labeled “2,” and so on. The last class has the smallest mean predicted response, labeled “bottom.” 4. For each decile group, calculate the mean (smooth) actual response and mean (smooth) predicted response, producing a set of 10 smooth points, (smooth actual response, smooth predicted response). group.6. Draw the 45° line on the plot. This line serves as a reference for assessing the quality of predictions at the decile group level. If the smooth points are either on or hug the 45° line in their proper order (top to bottom, or bottom to top), then predictions, on average, are considered good. 45° line. To aid in an objective examination of the smooth plot, use the correlation coefficient between the smooth actual and predicted response points. The correlation coefficient serves as an indicator of the amount of scatter about the 45° straight line. The larger the cor- relation coefficient is, the less scatter there will be and the better the overall quality of prediction. points tends to produce a big r value, which serves as a gross esti- mate of the individual-level r value. The correlation coefficient based on smooth actual and predicted response points is a gross measure of the individual-level predictions of the model. It is best served as a comparative indicator in choosing the better model. Decile Groups for Case Study I construct plot of the smooth actual versus predicted by decile groups based on Table 8.15 to determine the quality of the three-variable model 130 Statistical and Machine-Learning Data Mining Table 8.15 Smooth Points by Deciles from Model Based on FD_RCP, MOS_OPEN, and MOS_DUM TXN_ADD Predicted TXN_ADD DECILE N MEAN MEAN MIN MAX Top 492 0.069 0.061 0.061 0.061 2 493 0.047 0.061 0.061 0.061 3 493 0.037 0.061 0.061 0.061 4 492 0.089 0.080 0.061 0.085 5 493 0.116 0.094 0.085 0.104 6 493 0.085 0.104 0.104 0.104 7 492 0.142 0.118 0.104 0.121 8 493 0.156 0.156 0.121 0.196 9 493 0.185 0.198 0.196 0.209 Bottom 492 0.270 0.263 0.209 0.418 Total 4,926 0.119 0.119 0.061 0.418 predictions. The smooth plot in Figure 8.12 has a minimal scatter of the 10 smooth points about the 45° lines, with two noted exceptions. Decile groups 4 and 6 appear to be the farthest away from the line (in terms of perpendicular distance). Decile groups 8, 9, and bot are on top of each other, which indicate the predictions are the same for these groups. The indication is that the model cannot discriminate among the least-respond- ing individuals. But, because implementation of response models typi- cally excludes the lower three or four decile groups, their spread about the 45° line and their (lack of) order are not as critical a feature in assessing the quality of the prediction. Thus, the overall quality of prediction is con- sidered good. decile groups/three-variable model is the correlation coefficient between the smooth points, rsm. actual, sm. predicted: decile group, is 0.972. The plot of smooth actual versus predicted by score groups is defined as the plot consisting of the mean actual against the mean predicted response by the score groups. Its construction and interpretation are virtually identical to the plot for smooth actual versus predicted by decile groups. The painlessly obvious difference is that decile groups are replaced by score groups, which are defined in the discussion in Section 8.14.1 on the plot of the smooth resid- ual by score groups. plot of the smooth actual versus predicted by score groups: Logistic Regression: The Workhorse of Response Modeling 131 Legend: Values top, 2, ..., 9, bot are decile groups. top 0.25 0.20 2 3 al TXN_ADD 0.15 tu 4 Ac oth 6 Smo 0.10 7 5 bot 0.05 9 8 0.00 0.00 0.05 0.10 0.15 0.20 0.25 0.30 Smooth Predicted TXN_ADD FIguRe 8.12 Plot of smooth actual versus predicted by decile group for three-variable (FD2_RCP, MOS_ OPEN, and MOS_DUM) model. dicted logit to the predicted probability of response. actual response and predicted response."
15,Explain why the original estimated response rate of class B is unchanged after the smoothing process,"5. The middle leaf consists of only class B, defined by a large class size of 2,828 with a reliable response rate of 6.6%. Apparently, the low response rate of class B is not comparable to any class (original, tem- porary, or newly merged) response rate to warrant a merging. Hence,the original estimated response rate of class B is unchanged after the smoothing process.","The remaining six presumably reliable classes (A, C, D, E, and H) have sizes between 219 and 368. follows: 1. The top box, the root of the tree, represents the sample of 4,926 with response rate 11.9%. original 14 classes into 3 merged (smoothed) classes, as displayed in the CHAID tree with three leaf boxes. classes and the two reliable classes A and E, represents a newly merged class with a reliable response rate of 24.7% based on a class size of 1,018. In this situation, the smoothing process increases the reliability of the small classes with two-step averaging. The first step combines all the small classes into a temporary class, which by itself produces a reliable average response rate of 22.7% based on a class size of 383. In the second step, which does not always occur in smoothing, the temporary class is further united with the already-reliable classes A and E because the latter classes have comparable response rates to the temporary class response rate. response rate of the seven small classes and classes A and E. When double smoothing does not occur, the temporary class is the final class. 143 4. The increased reliability that smoothing of a categorical variable offers can now be clearly illustrated. Consider class M with its unre- liable estimated response rate of 42% based on class size 19. The smoothing process puts class M in the larger, more reliable left- most leaf with a response rate of 24.7%. The implication is that class M now has a more reliable estimate of response rate, namely, the response rate of its newly assigned class, 24.7%. Thus, the smooth- ing has effectively adjusted the original estimated response rate of class M downward, from a positively biased 42% to a reliable 24.7%. class J is upward, from a negatively biased 19% to 24.7%. It is not surprising that the two reliable classes, A and E, remain noticeably unchanged, from 25% and 26% to 24.7%, respectively.5. The middle leaf consists of only class B, defined by a large class size of 2,828 with a reliable response rate of 6.6%. Apparently, the low response rate of class B is not comparable to any class (original, tem- porary, or newly merged) response rate to warrant a merging. Thus, the original estimated response rate of class B is unchanged after the smoothing process. This presents no concern over the reliability of class B because its class size is largest from the outset.6. The rightmost leaf consists of large classes C, D, H, and I and the small class N for an average reliable response rate of 13.9% with class size 1,080. The smoothing process adjusts the response rate of class N downward, from 16% to a smooth 13.9%. The same adjustment occurs for class C. The remaining classes D, H, and I experience an upward adjustment. labeled 1, 2, and 3, corresponding to the leaves from left to right, respectively (see bottom of Figure 8.17). I also create two dummy variables for CH_TYPE: 1. CH_FTY_1 = 1 if FD_TYPE = A, E, F, G, J, K, L, or M; otherwise, CH_FTY_1 = 0; 2. CH_FTY_2 = 1 if FD_TYPE = B; otherwise, CH_FTY_2 = 0. reference class. If an individual has values CH_FTY_1 = 0 and CH_ FTY_2 = 0, then the individual has implicitly CH_TYPE = 3 and one of the original classes (C, D, H, I, or N). I assess the importance of the CHAID-based smoothed variable CH_ TYPE by performing a logistic regression analysis on TXN_ADD with both CH_FTY_1 and CH_FTY_2, as the set dummy variable must be 144 Statistical and Machine-Learning Data Mining together in the model; the output is in Table 8.19. The G/df value is 108.234 (= 216.468/2), which is greater than the standard G/df value of 4. Thus, CHFTY_1 and CH_FTY_2 together are declared important predictor vari- ables of TXN_ADD. I try to improve the predictions of the three-variable (MOS_OPEN, MOS_DUM, and FD2_RCP) model with the inclusion of the smoothed variable CH_TYPE. I perform the LRM on TXN_ADD with MOS_OPEN, MOS_DUM, FD2_RCP, and CH_FTY_1 and CH_FTY_2; the output is in Table 8.20. The Wald chi-square value for FD2_RCP is less than 4. Thus, I delete FD2_RCP from the model and rerun the model with the remaining four variables. model produces comparable Wald chi-square values for the four variables; the output is in Table 8.21. The G/df value equals 64.348 (= 257.395/4), which Table 8.19 G and df for CHAID-Smoothed FD_TYPE Variable –2LL G df p-value INTERCEPT 3,606.488 CH_FTY_1 & 3,390.021 216.468 2 0.0001 CH_FTY_2 Table 8.20 Logistic Model: EDA Model Variables plus CH_TYPE Variables Intercept and Intercept Only All Variables All Variables –2LL 3,606.488 3,347.932 258.556 with 5 df (p = 0.0001) Variable Parameter Standard Wald Pr > Estimate Error Chi-Square Chi-Square INTERCEPT -0.7497 0.2464 9.253 0.0024 CH_FTY_1 0.6264 0.1175 28.4238 0.0001 CH_FTY_2 -0.6104 0.1376 19.6737 0.0001 FD2_RCP 0.2377 0.2212 1.1546 0.2826 MOS_OPEN -0.2581 0.0398 42.0054 0.0001 MOS_DUM 0.7051 0.1365 26.6804 0.0001 Logistic Regression: The Workhorse of Response Modeling 145 Table 8.21 Logistic Model: Four-Variable EDA Model Intercept and Intercept Only All Variables All Variables –2LL 3,606.488 3,349.094 257.395 with 4 df (p = 0.0001) Parameter Wald Variable Estimate Standard Error Chi-Square Pr > Chi-Square INTERCEPT –0.9446 0.1679 31.6436 0.0001 CH_FTY_1 0.6518 0.1152 32.0362 0.0001 CH_FTY_2 –0.6843 0.1185 33.3517 0.0001 MOS_OPEN –0.2510 0.0393 40.8141 0.0001 MOS_DUM 0.7005 0.1364 26.3592 0.0001 is slightly larger than the G/df (62.02) of the three-variable (MOS_OPEN, MOS_DUM, FD2_RCP) model. This is not a strong indication that the four- variable model has more predictive power than the three-variable model. analysis of EDA versus non-EDA in Section 8.15, to determine whether the four-variable (4var-) EDA model is better than the three-variable (3var-) EDA model. I need the smooth plot descriptive statistics for the latter model as I already have the descriptive statistics for the former model. group: 4var- versus 3var-eDa Models The plot of smooth residual by score group for the 4var-EDA model in Figure 8.18 is equivalent to the null plot based on the general association test. Thus, the overall quality of the predictions of the model is considered good. It is worthy of notice that there is a far-out smooth point, labeled FO, in the middle of the top of the plot. This smooth residual point corresponds to a score group consisting of 56 individuals (accounting for 1.1% of the data), indicating a weak spot. model plot are as follows: For the smooth residual, the minimum and maxi- mum values and the range are -0.198, 0.560, and 0.758, respectively; the standard deviation of the smooth residual is 0.163. FO smooth point are worthy of notice because such statistics are known to be sensitive to far-out points, especially when they are smoothed and account for a very small percentage of the data. For the FO-adjusted smooth residual, the minimum and maximum values and the range are -0.198, 0.150, and 0.348, respectively; the standard deviation of the smooth residuals is 0.093. Statistical and Machine-Learning Data Mining Legend: A = smooth point for score group. 2 F0 0.5 0.4 0.3 l 0.2 A A oth Residua Smo 0.1 A 3 A A 0.0 A A A A A A –0.1 A A 4 1 –0.2 0.0 0.1 0.2 0.3 0.4 Smooth Predicted TXN_ADD FIguRe 8.18 Smooth residual by score group plot for 4var-EDA (MOS_OPEN, MOS_DUM, CH_FTY_1, CH_ FTY_2) model. cates that the former model produces smaller smooth residuals. The 3var- EDA model smooth residual range is noticeably smaller than that of the latter model: 44.6% (= (0.758 - 0.42)/0.758) smaller. The 3var-EDA model smooth resid- ual standard deviation is noticeably smaller than that of the 4var-EDA model: 23.9% (= (0.163 - 0.124/0.163) smaller. The implication is that the CHAID-based dummy variables carrying the information of FD_TYPE are not important Logistic Regression: The Workhorse of Response Modeling 147 enough to produce better predictions than that of the 3var-EDA model. In other words, the 3var-EDA model has a better quality of prediction. rule for the FO score group/weak spot, the implication is the 4var-EDA model has a better quality of predictions as the model produces smaller smooth residuals. The 4var-EDA model FO-adjusted smooth residual range is notice- ably smaller than that of the 3var-EDA model: 17.1% (= (0.42 - 0.348)/0.42) smaller. The 4var-EDA model FO-adjusted smooth residual standard devia- tion is noticeably smaller than that of the 3var-EDA model: 25.0% (= (0.124 - 0.093)/0.124). by Decile groups: 4var- versus 3var-eDa Models The plot of smooth actual versus predicted by decile groups for the 4var- EDA model in Figure 8.19 indicates good hugging of scatter about the 45° lines, despite the following two exceptions. First, there are two pairs of decile groups (6 and 7, 8 and 9), where the decile groups in each pair are adjacent to each other. This indicates that the predictions are different for decile groups within each pair, which should have the same response rate. Second, the bot decile group is very close to the line, but out of order. three or four decile groups, their spread about the 45° line and their (lack of) order is not as critical a feature in assessing the quality of predictions. between smooth actual and predicted points, rsm. actual, sm. predicted: decile group, is 0.989. 4var-EDA model smooth actual plots indicates the latter model produces a meagerly tighter hug about the 45° line. The 4var-EDA model correlation coefficient is hardly noticeably larger than that of the three-variable model: 1.76% (= (0.989-0.972)/0.972) smaller. The implication is that both models have equivalent quality of prediction at the decile level. by Score groups: 4var- versus 3var-eDa Models The score groups for plot of the smooth actual versus predicted by score groups for the 4var-EDA model in Figure 8.20 are defined by the variables in the 3var-EDA model to make an uncomplicated comparison. The plot indi- cates a very nice hugging about the 45° line, except for one far-out smooth point, labeled FO, which was initially uncovered by the smooth residual plot in Figure 8.18. The score-group-based correlation coefficient between all smooth points, rsm. actual, sm. predicted:score group, is 0.784; the score- group-based correlation without the far-out score group FO, rsm. actual, sm. predicted: 148 Statistical and Machine-Learning Data Mining Legend: Values top, 2, ..., 9, bot are decile groups. top 0.25 2 0.20 al TXN_ADD 0.15 3 tu 4 Ac oth Smo 0.10 5 7 6 bot 0.05 9 8 0.00 0.00 0.05 0.10 0.15 0.20 0.25 0.30 Smooth Predicted TXN_ADD FIguRe 8.19 Smooth actual versus predicted by decile group plot for 4var-EDA (MOS_OPEN, MOS_DUM, CH_FTY_1, CH_FTY_2) model."
16,Explain why the odds ratio does not explicitly reflect the variation of the other predictor variables,"The logistic regression model estimates the logit of Y— the log of the odds of an individual responding yes as defined in Equation (11.1), from which an individual’s probability of responding yes is obtained in Equation (11.2): Logit Y = b0 + b1*X1 + b2*X2 + X3 + bn*Xn (11.1) exp(Logit Y) Prob(Y = 1) = (11.2) 1 + exp(Logit Y) An individual’s predicted probability of responding yes is calculated by “plugging in” the values of the predictor variables for that individual in * This chapter is based on an article with the same title in Journal of Targeting, Measurement and Analysis for Marketing, 6, 2, 1997. Used with permission. 196 Statistical and Machine-Learning Data Mining Equations (11.1) and (11.2). The b’s are the logistic regression coefficients. The coefficient b0 is referred to as the Intercept and has no predictor variable with which it is multiplied.The odds ratio is the traditional measure of assessing the effect of a predictor variable on the response variable (actually on the odds of response = 1) given that the other predictor variables are “held constant.” The phrase “given that … ” implies that the odds ratio is the average effect of a predictor variable on response when the effects of the other predictor variables are “partialled out” of the relationship between response and the predictor variable. Hence,the odds ratio does not explicitly reflect the variation of the other predictor variables.","39. Weiss, S.M., and Indurkhya, N. , Predictive Data Mining, Morgan Kaufman, San Francisco, 1998. 11 CHAID for Interpreting a Logistic Regression Model* 11.1 Introduction The logistic regression model is the standard technique for building a response model. Its theory is well established, and its estimation algorithm is available in all major statistical software packages. The literature on the the- oretical aspects of logistic regression is large and rapidly growing. However, little attention is paid to the interpretation of the logistic regression response model. The purpose of this chapter is to present a data mining method based on CHAID (chi-squared automatic interaction detection) for interpreting a logistic regression model, specifically to provide a complete assessment of the effects of the predictor variables on response. I state briefly the definition of the logistic regression model. Let Y be a binary response (dependent) variable, which takes on yes/no values (typically coded 1/0, respectively), and X1, X2, X3, Xn be the predictor (independent) variables.The logistic regression model estimates the logit of Y— the log of the odds of an individual responding yes as defined in Equation (11.1), from which an individual’s probability of responding yes is obtained in Equation (11.2): Logit Y = b0 + b1*X1 + b2*X2 + X3 + bn*Xn (11.1) exp(Logit Y) Prob(Y = 1) = (11.2) 1 + exp(Logit Y) An individual’s predicted probability of responding yes is calculated by “plugging in” the values of the predictor variables for that individual in * This chapter is based on an article with the same title in Journal of Targeting, Measurement and Analysis for Marketing, 6, 2, 1997. Used with permission. 196 Statistical and Machine-Learning Data Mining Equations (11.1) and (11.2). The b’s are the logistic regression coefficients. The coefficient b0 is referred to as the Intercept and has no predictor variable with which it is multiplied.The odds ratio is the traditional measure of assessing the effect of a predictor variable on the response variable (actually on the odds of response = 1) given that the other predictor variables are “held constant.” The phrase “given that … ” implies that the odds ratio is the average effect of a predictor variable on response when the effects of the other predictor variables are “partialled out” of the relationship between response and the predictor variable. Thus, the odds ratio does not explicitly reflect the variation of the other predictor variables. The odds ratio for a predictor variable is obtained by exponentiat- ing the coefficient of the predictor variable. That is, the odds ratio for Xi equals exp(bi), where exp is the exponential function, and bi is the coefficient of Xi. A woodworker’s tool supplier, who wants to increase response to her cata- log in an upcoming campaign, needs a model for generating a list of her most responsive customers. The response model, which is built on a sample drawn from a recent catalog mailing with a 2.35% response rate, is defined by the following variables: 1. The response variable is RESPONSE, which indicates whether a cus- tomer made a purchase (yes = 1, no = 0) to the recent catalog mailing.2. The predictor variables are (a) CUST_AGE, the customer age in years; (b) LOG_LIFE, the log of total purchases in dollars since the custom- er’s first purchase, that is, the log of lifetime dollars; and (c) PRIOR_ BY, the dummy variable indicating whether a purchase was made in the 3 months prior to the recent catalog mailing (yes = 1, no = 0). variables produces the output in Table 11.1. The “Parameter Estimate” col- umn contains the logistic regression coefficients, from which the RESPONSE model is defined in Equation (11.3). = –8.43 + 0.02*CUST_AGE + 0.74*LOG_LIFE + 0.82*PRIOR_BY (11.3) 11.3.1 Odds ratio The following discussion illustrates two weaknesses in the odds ratio."
17,Explain why Xi is an important predictor variable,"¶ The choice of “very small” is arbitrary, but convention sets it at 5% or less. Statistical and Machine-Learning Data Mining b. If the p value is very small, typically less than 5%, then the sample evidence supports the decision to reject NH in favor of accepting AH. It is concluded that bi (or cMSE_Xi) has some nonzero value, and Xi contributes a significant reduction in prediction error. Hence,Xi is an important predictor variable.","† A suite of classical assumptions is required for the proper testing of the least squares estimate of bi. See any good mathematical statistics textbook, such as Ryan, T.P., Modern Regression Methods, Wiley, New York, 1997.¶ The choice of “very small” is arbitrary, but convention sets it at 5% or less. Statistical and Machine-Learning Data Mining b. If the p value is very small, typically less than 5%, then the sample evidence supports the decision to reject NH in favor of accepting AH. It is concluded that bi (or cMSE_Xi) has some nonzero value, and Xi contributes a significant reduction in prediction error. Thus, Xi is an important predictor variable.The decision rule makes it clear that the p value is an indicator of the likeli- hood that the variable has some predictive importance, not an indicator of how much importance (AH does not specify a value of bi). Thus, a smaller p value implies a greater likelihood of some predictive importance, not a greater predictive importance. This is contrary to the common misinterpretation of the p value: The smaller the p value is, the greater the predictive importance of the associated variable will be. Relying solely on the p value for declaring important predictor variables is problematic in big data applications, which are characterized by “large” samples drawn from populations with unknown spread of the X’s. The p value is affected by the sample size (as sample size increases, the p value decreases) and by the spread of Xi (as spread of Xi increases, the p value decreases)* [1]. Accordingly, a small p value may be due to a large sample or a large spread of the X’s. Thus, in big data applications, a small p value is only an indicator of a potentially important predictor variable. is currently unresolved. Big data are nonexperimental data for which there are no procedures for determining the correct large sample size. associated variables are often declared important when in fact they are not important; this reduces the stability of a model†,‡ [2]. A procedure that adjusts the p values when working with big data is needed.§ Until such procedures are established, the recommended ad hoc approach is as follows: In big data applications, variables with small p values must undergo a final assessment of importance based on their actual reduction in prediction * The size of bi also affects the p value: As bi increases, the p value decreases. This factor cannot be controlled by the analyst."
18,"Explain why a smaller p value implies a greater likelihood of some predictive importance, not a greater predictive importance","¶ The choice of “very small” is arbitrary, but convention sets it at 5% or less. Statistical and Machine-Learning Data Mining b. If the p value is very small, typically less than 5%, then the sample evidence supports the decision to reject NH in favor of accepting AH. It is concluded that bi (or cMSE_Xi) has some nonzero value, and Xi contributes a significant reduction in prediction error. Thus, Xi is an important predictor variable.The decision rule makes it clear that the p value is an indicator of the likeli- hood that the variable has some predictive importance, not an indicator of how much importance (AH does not specify a value of bi). Hence,a smaller p value implies a greater likelihood of some predictive importance, not a greater predictive importance.","† A suite of classical assumptions is required for the proper testing of the least squares estimate of bi. See any good mathematical statistics textbook, such as Ryan, T.P., Modern Regression Methods, Wiley, New York, 1997.¶ The choice of “very small” is arbitrary, but convention sets it at 5% or less. Statistical and Machine-Learning Data Mining b. If the p value is very small, typically less than 5%, then the sample evidence supports the decision to reject NH in favor of accepting AH. It is concluded that bi (or cMSE_Xi) has some nonzero value, and Xi contributes a significant reduction in prediction error. Thus, Xi is an important predictor variable.The decision rule makes it clear that the p value is an indicator of the likeli- hood that the variable has some predictive importance, not an indicator of how much importance (AH does not specify a value of bi). Thus, a smaller p value implies a greater likelihood of some predictive importance, not a greater predictive importance. This is contrary to the common misinterpretation of the p value: The smaller the p value is, the greater the predictive importance of the associated variable will be. Relying solely on the p value for declaring important predictor variables is problematic in big data applications, which are characterized by “large” samples drawn from populations with unknown spread of the X’s. The p value is affected by the sample size (as sample size increases, the p value decreases) and by the spread of Xi (as spread of Xi increases, the p value decreases)* [1]. Accordingly, a small p value may be due to a large sample or a large spread of the X’s. Thus, in big data applications, a small p value is only an indicator of a potentially important predictor variable. is currently unresolved. Big data are nonexperimental data for which there are no procedures for determining the correct large sample size. associated variables are often declared important when in fact they are not important; this reduces the stability of a model†,‡ [2]. A procedure that adjusts the p values when working with big data is needed.§ Until such procedures are established, the recommended ad hoc approach is as follows: In big data applications, variables with small p values must undergo a final assessment of importance based on their actual reduction in prediction * The size of bi also affects the p value: As bi increases, the p value decreases. This factor cannot be controlled by the analyst.‡ Falsely fail to reject NH. The effect of a “small” sample is that a variable can be declared unimportant when, in fact, it is important."
19,"Explain why for a predictive model, predictions for the same individual, obtained from repeated implementations of the (reliable) model, are expected to vary closely","† A model should be monitored for consistent results after each implementation. If the results of the model show signs of degradation, then the model should either be recalibrated (update the regression coefficients: keep the same variables in the model but use fresh data) or retrained (update the model: add new variables to the original variables of the model and use fresh data). 226 Statistical and Machine-Learning Data Mining fall within narrow limits. Hence,for a predictive model, predictions for the same individual, obtained from repeated implementations of the (reliable) model, are expected to vary closely.","4. Hayes, W.L., Statistics for the Social Sciences, Holt, Rinehart and Winston, Austin, TX, 1972. The Average Correlation: A Statistical Data Mining Measure for Assessment of Competing Predictive Models and the Importance of the Predictor Variables 13.1 Introduction The purpose of this chapter is to introduce the number one statistic, the mean, and the runner-up, the correlation coefficient, which when used together as discussed here yield the average correlation, providing a fruitful statistical data mining measure. The average correlation, along with the correlation coefficient, provides a quantitative criterion for assessing (1) competing pre- dictive models and (2) the importance of the predictor variables. Two essential characteristics of a predictive model are its reliability and validity, terms that are often misunderstood and misused. Reliability refers to a model* yielding consistent results.† For a predictive model, the proverbial question is how dependable (reliable) the model is for predictive purposes. able predictions. It is normal for an individual to vary in performance, as chance influences are always in operation, but performance is expected to * A “model” can be predictive (statistical regression), explanatory (principal components analysis, PCA), or predictive and explanatory (structural equation).† A model should be monitored for consistent results after each implementation. If the results of the model show signs of degradation, then the model should either be recalibrated (update the regression coefficients: keep the same variables in the model but use fresh data) or retrained (update the model: add new variables to the original variables of the model and use fresh data). 226 Statistical and Machine-Learning Data Mining fall within narrow limits. Thus, for a predictive model, predictions for the same individual, obtained from repeated implementations of the (reliable) model, are expected to vary closely. intended to measure with respect to a given criterion (e.g., a predictive model criterion is small prediction errors). One indispensable element of a valid model is that is has high reliability. If the reliability of the model is low, so goes the validity of the model. Reliability is a necessary, but not sufficient, condition for a valid model. Thus, a predictive model is valid to the extent the model is efficient (precise and accurate) in predicting at a given time. I give a clarifying explanation of efficiency in the next section. model does not possess a standard level of validity as it may be highly valid at one point in time, but not at another, as the environment about the initial modeling process is likely to change. The inference is that models have to be kept in a condition of good efficiency. See Footnote 2 for a brief discussion of maintaining a model in good standing regarding validity. validity. Face validity is a term used to characterize the notion of a model “looking like it is going to work.” It is a subjective criterion valuable to model users. Face validity gives users, especially those who may not have the spe- cialized background to build a model but do have practical knowledge of how models work, what they want of a model. Thus, if the model does not look right for the required objective, the confidence level of the model utility drops, as does the acceptance and implementation of the model.Content validity refers to the variables in a model in that the content of the individual variables and the ensemble of the variables are relevant to the purpose of the model. As a corollary, the model should not have irrelevant, unnecessary variables. Such variables can be eliminated by assessing pre- liminarily the correlation of each variable with the dependent variable. The correlation coefficient for this elimination process is not foolproof as it at times allows variables with nonrelevant content to “sneak in” the model. mining the quality of the variable selection used for defining the best subset of the predictor variables. In other words, the model builder must assess subjectively the content of the variables. An objective discussion of content validity is beyond the scope of this chapter.* The literature† does not address the “left side” of the model equation, the dependent variable. The dependent variable is often expressed in manage- ment terms (i.e., nonoperational terms). The model builder needs the explicit backstory about the management objective to create a valid definition of the * An excellent reference is Carmines, E.G., and Zeller, R.A., Reliability and Viability Assessment, Sage, Thousand Oaks, CA, 1991."
20,"Explain why I strongly argue that the imputation of big data applications must be used with restraint, and their findings must be used judiciously","Nothing can take the place of the missing data. Allison noted, “The best solution to the missing data problem is not to have any missing data” [3]. gerous,” seductive because it gives a false sense of confidence that the data are complete and dangerous because it can produce misguided analyses and untrue models [4]. filling in missing data. In big data applications, the problem of missing data is severe as it is common for at least one variable to have 30%–90% of its values missing. Hence,I strongly argue that the imputation of big data applications must be used with restraint, and their findings must be used judiciously.","2. X_dum = 0 if X is not missing; X_dum = 1 if X is missing. card cases. The disadvantage is that the analysis can become unwieldy when there are many variables with missing data. In addition, filling in the miss- ing value with a zero is arbitrary, which is unsettling for some data analysts. defined as any process that fills in missing data to produce a complete data- set. The simplest and most popular imputation method is mean-value imputa- tion. The mean of the nonmissing values of the variable of interest is used to fill in the missing data. Consider individuals 2 and 8 in Table 16.1. Their missing ages are replaced with the mean AGE of the file, namely, 40 years (rounded from 39.6). The advantage of this method is undoubtedly its ease of use. The means may be calculated within classes, which are predefined by other variables related to the study at hand. replaced by the predicted values from a regression analysis. The dependent variable Y is the variable whose missing values need to be imputed. The predictor variables, the X’s, are the matching variables. Y is regressed on the X’s using a complete-case analysis dataset. If Y is continuous, then ordinary least squares (OLS) regression is appropriate. If Y is categorical, then the logistic regression model (LRM) is used. For example, I wish to impute AGE for individual 8 in Table 16.1. I regress AGE on GENDER and INCOME (the 270 Statistical and Machine-Learning Data Mining matching variables) based on the complete-case dataset consisting of five individuals (IDs 1, 3, 4, 6, and 10). The OLS regression imputation model is defined in Equation (16.1): AGE _ I M P u t E D = 2 5 . 8 - 2 0 . 5*GE N DE R + 0 . 0 0 0 2*I NCOM E (16 .1) Plugging in the values of GENDER (= 1) and INCOME (= $125,000) for indi- vidual 8, the imputed AGE is 53 years. Missing data methods presuppose that the missing data are “missing at ran- dom” (MAR). Rubin formalized this condition into two separate assump- tions [2]: 1. Missing at random (MAR) means that what is missing does not depend on the missing values but may depend on the observed values. does not depend on either the observed values or the missing values. of individuals with only complete data can be regarded as a simple ran- dom subsample from the original data. Note that the second assump- tion MCAR represents a stronger condition than the MAR assumption. can be tested to some extent by comparing the information from complete cases to the information from incomplete cases. A procedure often used is to compare the distribution of the variable of interest, say, Y, based on nonmiss- ing data with the distribution of Y based on missing data. If there are signifi- cant differences, then the assumption is considered not met. If no significant differences are indicated, then the test offers no direct evidence of assump- tion violation. In this case, the assumption is considered cautiously to be satisfied.* The MAR assumption is impossible to test for validity. (Why?) It is accepted wisdom that missing data solutions at best perform satis- factorily, even when the amount of missing data is moderate and the miss- ing data assumption has been met. The potential of the two new imputation methods, maximum likelihood and multiple imputation, which offer sub- stantial improvement over the complete-case analysis, is questionable as * This test proves the necessary condition for MCAR. It remains to be shown that there is no relationship between missingness on a given variable and the values of that variable. 271 their assumptions are easily violated. Moreover, their utility in big data applications has not been established.Nothing can take the place of the missing data. Allison noted, “The best solution to the missing data problem is not to have any missing data” [3]. gerous,” seductive because it gives a false sense of confidence that the data are complete and dangerous because it can produce misguided analyses and untrue models [4]. filling in missing data. In big data applications, the problem of missing data is severe as it is common for at least one variable to have 30%–90% of its values missing. Thus, I strongly argue that the imputation of big data applications must be used with restraint, and their findings must be used judiciously.In the spirit of the EDA (exploratory data analysis) tenet—that failure is when you fail to try—I advance the proposed data mining/EDA CHAID imputation method as a hybrid mean-value/regression-based imputation method that explicitly accommodates missing data without imposing additional assump- tions. The salient features of the new method are the EDA characteristics: 1. Flexibility: Assumption-free CHAID work especially well with big data containing large amounts of missing data. imputation methods and machine-learning algorithm for data struc- ture identification. I introduce a couple of terms required for the discussion of CHAID impu- tation. Imputation methods require the sample to be divided into groups or classes, called imputation classes, which are defined by variables called matching variables. The formation of imputation classes is an important step to ensure the reliability of the imputation estimates. As the homogeneity of the classes increases, so does the accuracy and stability of the estimates. It is assumed that the variance (with respect to the variable whose missing val- ues are to be imputed) within each class is small. rate and distinct groups, which are defined by the predictor variables, such that the variance of the dependent variable is minimized within the groups 272 Statistical and Machine-Learning Data Mining and maximized across the groups. CHAID was originally developed as a method of detecting “combination” or interaction variables. In database mar- keting today, CHAID primarily serves as a market segmentation technique. value/regression-based imputation. definition, CHAID creates optimal homogeneous groups, which can be used effectively as trusty imputation classes.* Accordingly, CHAID provides a reliable method of mean-value/regression-based imputation. CHAID is a tree-structured, assumption-free modeling alternative to OLS regression. It provides reliable estimates without the assumption of specifying the true structural form of the model (i.e., knowing the correct independent variables and their correct reexpressed forms) and without regard for the weighty classical assumptions of the underly- ing OLS model. Thus, CHAID with its trusty imputation classes pro- vides reliable regression tree imputation for a continuous variable. binary and polychotomous LRM without the assumption of speci- fying the true structural form of the model. Thus, CHAID with its trusty imputation classes provides reliable classification tree imputa- tion for a categorical variable. ability to use most of the analysis sample. The analysis sample for CHAID is not as severely reduced by the pattern of missing values in the matching variables, as is the case for regression-based models, because CHAID can accommodate missing values for the matching variables in its analysis.† The regression-based imputation methods cannot make such an accommodation. Consider a sample of 29,132 customers from a cataloguer’s database. The fol- lowing is known about the customers: their ages (AGE_CUST), GENDER, total lifetime dollars (LIFE_DOL), and whether a purchase was made within the past 3 months (PRIOR_3). Missing values for each variable are denoted by “???.” * Some analysts may argue about the optimality of the homogeneous groups but not the trustworthiness of the imputation classes."
21,Explain why there are the following questions: 1,"* Validation of any response or profit model built from any modeling technique (e.g., discriminant analysis, logistic regression, neural network, genetic algorithms, or CHAID [chi-squared automatic interaction detection]). 319 Table 19.1 Response Decile Analysis Decile Cumulative Number of Number of Response Response Decile Individuals Responders Rate Rate Cum Lift Top 18,110 88 0.49% 0.49% 186 2 18,110 58 0.32% 0.40% 154 3 18,110 50 0.28% 0.36% 138 4 18,110 63 0.35% 0.36% 137 5 18,110 44 0.24% 0.33% 128 6 18,110 48 0.27% 0.32% 123 7 18,110 39 0.22% 0.31% 118 8 18,110 34 0.19% 0.29% 112 9 18,110 23 0.13% 0.27% 105 Bottom 18,110 27 0.15% 0.26% 100 Total 181,100 474 0.26% Table 19.2 Cum Lifts for Three Validations Decile First Sample Second Sample Third Sample Top 186 197 182 2 154 153 148 3 138 136 129 4 137 129 129 5 128 122 122 6 123 118 119 7 118 114 115 8 112 109 110 9 105 104 105 Bottom 100 100 100 19.4 Three Questions With many decile analysis validations, the expected sample-to-sample variation within each decile points to the uncertainty of the Cum Lift esti- mates. If there is an observed large variation for a given decile, there is less 320 Statistical and Machine-Learning Data Mining confidence in the Cum Lift for that decile; if there is an observed small varia- tion, there is more confidence in the Cum Lift. Hence,there are the following questions: 1.","* This chapter is based on an article with the sa me title in Journal of Targeting, Measurement and Analysis for Marketing, 6, 2, 1997. Used with permission. 318 Statistical and Machine-Learning Data Mining 19.3 Illustration As marketers use the Cum Lift measure from a decile analysis to assess the goodness of a model, the validation of the model* consists of compar- ing the Cum Lifts from the calibration and holdout decile analyses based on the model. It is expected that shrinkage in the Cum Lifts occurs: Cum Lifts from the holdout sample are typically smaller (less optimistic) than those from the calibration sample from which they were originated. The Cum Lifts on a fresh holdout sample, which does not contain the calibra- tion idiosyncrasies, provide a more realistic assessment of the quality of the model. The calibration Cum Lifts inherently capitalize on the idiosyn- crasies of the calibration sample due to the modeling process, which favors large Cum Lifts. If both the Cum Lift shrinkage and the Cum Lift values themselves are acceptable, then the model is considered successfully vali- dated and ready to use; otherwise, the model is reworked until success- fully validated. based on a sample of 181,100 customers with an overall response rate of 0.26%. (Recall from Chapter 18 that the Cum Lift is a measure of predictive power; it indicates the expected gain from a solicitation implemented with a model over a solicitation implemented without a model.) The Cum Lift for the top decile is 186; this indicates that when soliciting to the top decile— the top 10% of the customer file identified by the RM model—there is an expected 1.86 times the number of responders found by randomly solicit- ing 10% of the file (without a model). Similar to that for the second decile, the Cum Lift 154 indicates that when soliciting to the top two deciles—the top 20% of the customer file based on the RM model—there is an expected 1.54 times the number of responders found by randomly soliciting 20% of the file. which two additional decile analysis validations are performed. Not sur- prisingly, the Cum Lifts for a given decile across the three validations are somewhat different. The reason for this is the expected sample-to-sample variation, attributed to chance. There is a large variation in the top decile (range is 15 = 197 - 182) and a small variation in decile 2 (range is 6 = 154 - 148).* Validation of any response or profit model built from any modeling technique (e.g., discriminant analysis, logistic regression, neural network, genetic algorithms, or CHAID [chi-squared automatic interaction detection]). 319 Table 19.1 Response Decile Analysis Decile Cumulative Number of Number of Response Response Decile Individuals Responders Rate Rate Cum Lift Top 18,110 88 0.49% 0.49% 186 2 18,110 58 0.32% 0.40% 154 3 18,110 50 0.28% 0.36% 138 4 18,110 63 0.35% 0.36% 137 5 18,110 44 0.24% 0.33% 128 6 18,110 48 0.27% 0.32% 123 7 18,110 39 0.22% 0.31% 118 8 18,110 34 0.19% 0.29% 112 9 18,110 23 0.13% 0.27% 105 Bottom 18,110 27 0.15% 0.26% 100 Total 181,100 474 0.26% Table 19.2 Cum Lifts for Three Validations Decile First Sample Second Sample Third Sample Top 186 197 182 2 154 153 148 3 138 136 129 4 137 129 129 5 128 122 122 6 123 118 119 7 118 114 115 8 112 109 110 9 105 104 105 Bottom 100 100 100 19.4 Three Questions With many decile analysis validations, the expected sample-to-sample variation within each decile points to the uncertainty of the Cum Lift esti- mates. If there is an observed large variation for a given decile, there is less 320 Statistical and Machine-Learning Data Mining confidence in the Cum Lift for that decile; if there is an observed small varia- tion, there is more confidence in the Cum Lift. Thus, there are the following questions: 1. With many decile analysis validations, how can an average Cum Lift (for a decile) be defined to serve as an honest estimate of the Cum Lift? In addition, how many validations are needed? 2. With many decile analysis validations, how can the variability of an honest estimate of Cum Lift be assessed? That is, how can the stan- dard error (a measure of precision of an estimate) of an honest esti- mate of the Cum Lift be calculated? 3. With only a single validation dataset, can an honest Cum Lift esti- mate and its standard error be calculated? The answers to these questions and more lie in the bootstrap methodology. The bootstrap is a computer-intensive approach to statistical inference [1]. extensively the sample at hand* [2]. By random selection with replacement from the sample, some individuals occur more than once in a bootstrap sam- ple, and some individuals occur not at all. Each same-size bootstrap sample will be slightly different from the others. This variation makes it possible to induce an empirical sampling distribution† of the desired statistic, from which estimates of bias and variability are determined. tistic. For well-known statistics, such as the mean, the standard deviation, regression coefficients, and R-squared, the bootstrap provides an alternative to traditional parametric methods. For statistics with unknown properties, such as the median and Cum Lift, traditional parametric methods do not exist; thus, the bootstrap provides a viable alternative over the inappropriate use of traditional methods, which yield questionable results. does not rely on unrealistic parametric assumptions. Consider testing the significance of a variable§ in a regression model built using ordinary least * Other resampling methods include, for example, the jackknife, infinitesimal jackknife, delta method, influence function method, and random subsampling.† A sampling distribution can be considered as the frequency curve of a sample statistic from an infinite number of samples. Bootstrapping in Marketing: A New Approach for Validating Models 321 squares estimation. Say the error terms do not have a normal distribution, a clear violation of the ordinary least squares assumptions [3]. The sig- nificance testing may yield inaccurate results due to the model assump- tion not being met. In this situation, the bootstrap is a feasible approach in determining the significance of the coefficient without concern for any assumptions. As a nonparametric method, the bootstrap does not rely on theoretical derivations required in traditional parametric methods. I review the well-known parametric approach to the construction of confi- dence intervals (CIs) to demonstrate the utility of the bootstrap as an alter- native technique. Consider the parametric construction of a confidence interval for the popu- lation mean. I draw a random sample A of five numbers from a population."
22,"Explain why 95% confidence refers to the procedure for constructing the interval, not the observed interval itself","For sample A, SE equals 8.31, and the 95% CI for the population mean is between 4.31 (= (20.60 - 1.96*8.31) and 36.89 (= (20.60 + 1.96*8.31). This is commonly, although not quite exactly, stated as follows: There is 95% confidence that the population mean lies between 4.31 and 36.89. The sta- tistically correct statement for the 95% CI for the unknown population mean is as follows: If one repeatedly calculates such intervals from, say, 100 independent random samples, 95% of the constructed intervals would contain the true population mean. Do not be fooled: Once the confidence interval is constructed, the true mean is either in the interval or not in the interval. Hence,95% confidence refers to the procedure for constructing the interval, not the observed interval itself.","Sample A consists of (23, 4, 46, 1, 29). The sample mean is 20.60, the sample median is 23, and the sample standard deviation is 18.58. that the theoretical sampling distribution of the sample mean is normally distributed with an analytically defined standard error [4]. Thus, the 100(1 - a)% CI for the mean is: Sa M p Le M e a N Va Lu e ± |Z a/2|*STa NDa RD eRRoR where Sample Mean value is simply the mean of the five numbers in the sample. CI. The |Za/2| values are 1.96, 1.64, and 1.28 for 95%, 90%, and 80% CIs, respectively. Standard Error (SE) of the sample mean has the ana- lytic formula: SE = the sample standard deviation divided by the square root of the sample size.For sample A, SE equals 8.31, and the 95% CI for the population mean is between 4.31 (= (20.60 - 1.96*8.31) and 36.89 (= (20.60 + 1.96*8.31). This is commonly, although not quite exactly, stated as follows: There is 95% confidence that the population mean lies between 4.31 and 36.89. The sta- tistically correct statement for the 95% CI for the unknown population mean is as follows: If one repeatedly calculates such intervals from, say, 100 independent random samples, 95% of the constructed intervals would contain the true population mean. Do not be fooled: Once the confidence interval is constructed, the true mean is either in the interval or not in the interval. Thus, 95% confidence refers to the procedure for constructing the interval, not the observed interval itself. This parametric approach for the construction of confidence intervals for statistics, such as the median and the Cum Lift, does not exist because the theoretical sampling distributions 322 Statistical and Machine-Learning Data Mining (which provide the standard errors) of the median and Cum Lift are not known. If confidence intervals for the median and Cum Lift are desired, then approaches that rely on a resampling methodology like the bootstrap can be used. The key assumption of the bootstrap* is that the sample is the best estimate† of the unknown population. Treating the sample as the population, the ana- lyst repeatedly draws same-size random samples with replacement from the original sample. The analyst estimates the sampling distribution of the desired statistic from the many bootstrap samples and is able to calculate a bias-reduced bootstrap estimate of the statistic and a bootstrap estimate of the SE of the statistic. 2. Treat sample as population. dom selection with replacement of size n, the size of the original sample. it BS1. 7. From steps 1 to 6, there are BS1, BS2, ..., BSm. BSest(Y) = 2*SAM_EST - mean(BSi).* This bootstrap method is the normal approximation. Others are percentile, B-C percentile, and percentile-t. † Actually, the sample distribution function is the nonparametric maximum likelihood estimate of the population distribution function."
23,Explain why it is instructive to compare the performance of the bootstrap with the theoretically correct parametric chi-square test,"* This illustration draws on Sample B comes from Mosteller, F., and Tukey, J.W., Data Analysis and Regression, Addison-Wesley, Reading, MA, 139–143, 1977. Statistical and Machine-Learning Data Mining Table 19.3 100 Bootstrapped StDs 1.3431 0.60332 1.7076 0.6603 1.4614 1.43 1.4312 1.2965 1.9242 0.71063 1.3841 1.7656 0.73151 0.70404 0.643 1.6366 1.8288 1.6313 0.61427 0.76485 1.3417 0.69474 2.2153 1.2581 1.4533 1.353 0.479 0.62902 2.1982 0.73666 0.66341 1.4098 1.8892 2.0633 0.73756 0.69121 1.2893 0.67032 1.7316 0.60083 1.4493 1.437 1.3671 0.49677 0.70309 0.51897 0.65701 0.59898 1.3784 0.7181 1.4312 1.3985 0.6603 1.2972 0.47854 2.0658 1.7825 0.63281 1.8755 0.39384 0.69194 0.6343 1.31 1.3491 0.70079 1.3754 0.29609 1.5522 0.62048 1.8657 1.3919 1.6596 1.3726 2.0877 1.6659 1.4372 0.72111 1.4356 0.83327 1.4056 1.7404 1.796 1.7957 1.3994 1.399 1.3653 1.3726 1.2665 1.2874 1.8172 1.322 0.56569 0.74863 1.4085 1.6363 1.3802 0.60194 1.9938 0.57937 0.74117 1.3476 0.6345 1.7188 10. The bootstrap 95% confidence interval for the population standard deviation is 0.50 < population standard deviation < 2.47. ulation (NP). Hence,it is instructive to compare the performance of the bootstrap with the theoretically correct parametric chi-square test.","5. I calculate StD on this bootstrap (BS) sample to obtain a pseudo- value, BS1 = 1.3478. BSest(StD) = 2*SAM_EST - mean(BSi) = 2*1.3435 - 1.2034 = 1.483 9. I calculate the bootstrap estimate of the standard error of StD: SEBS(StD) = standard deviation (BSi) = 0.5008.* This illustration draws on Sample B comes from Mosteller, F., and Tukey, J.W., Data Analysis and Regression, Addison-Wesley, Reading, MA, 139–143, 1977. Statistical and Machine-Learning Data Mining Table 19.3 100 Bootstrapped StDs 1.3431 0.60332 1.7076 0.6603 1.4614 1.43 1.4312 1.2965 1.9242 0.71063 1.3841 1.7656 0.73151 0.70404 0.643 1.6366 1.8288 1.6313 0.61427 0.76485 1.3417 0.69474 2.2153 1.2581 1.4533 1.353 0.479 0.62902 2.1982 0.73666 0.66341 1.4098 1.8892 2.0633 0.73756 0.69121 1.2893 0.67032 1.7316 0.60083 1.4493 1.437 1.3671 0.49677 0.70309 0.51897 0.65701 0.59898 1.3784 0.7181 1.4312 1.3985 0.6603 1.2972 0.47854 2.0658 1.7825 0.63281 1.8755 0.39384 0.69194 0.6343 1.31 1.3491 0.70079 1.3754 0.29609 1.5522 0.62048 1.8657 1.3919 1.6596 1.3726 2.0877 1.6659 1.4372 0.72111 1.4356 0.83327 1.4056 1.7404 1.796 1.7957 1.3994 1.399 1.3653 1.3726 1.2665 1.2874 1.8172 1.322 0.56569 0.74863 1.4085 1.6363 1.3802 0.60194 1.9938 0.57937 0.74117 1.3476 0.6345 1.7188 10. The bootstrap 95% confidence interval for the population standard deviation is 0.50 < population standard deviation < 2.47. ulation (NP). Thus, it is instructive to compare the performance of the bootstrap with the theoretically correct parametric chi-square test. The bootstrap confidence interval is somewhat wider than the chi-square/NP interval in Figure 19.1. The BS confidence interval covers values between 0.50 and 2.47, which also includes the values within the NP confidence interval (0.93, 2.35). indicate the bootstrap methodology provides results that are consistent with the outcomes of the parametric techniques. Thus, the bootstrap is a reliable approach to inferential statistics in most situations. point estimate of the standard deviation. The original sample estimate is 1.3435, and the bootstrap estimate is 1.483. There is a 10.4% (1.483/1.3435) bias reduction in the estimate. 325 NP 0.93xxxxxxxxxxxxxx2.35 BS .50xxxxxxxxxxxxxxxxxxx2.47 FIgure 19.1 Bootstrap versus normal estimates. Continuing with the RM model illustration, I execute the 10-step bootstrap procedure to perform a bootstrap decile analysis validation. I use 50 boot- strap samples,* each of a size equal to the original sample size of 181,100. dard error of 10. Accordingly, for the top decile, the 95% bootstrap confi- dence interval is 163 to 203. The second decile has a bootstrap Cum Lift of 151, and a bootstrap 95% confidence interval is between 137 and 165. Similar readings in Table 19.4 can be made for the other deciles. Specifically, this bootstrap validation indicates that the expected Cum Lift is 135 using the RM model to select the top 30% of the most responsive individuals from a randomly drawn sample of size 181,100 from the target population or data- base. Moreover, the Cum Lift is expected to lie between 127 and 143 with 95% confidence. Similar statements can be made for other depths of file from the bootstrap validation in Table 19.4. be easily obtained from the bootstrap estimates and confidence intervals for decile Cum Lifts. The conversion formula involves the following: Bootstrap decile response rate equals bootstrap decile Cum Lift divided by 100, then multiplied by overall response rate. For example, for the third decile, the bootstrap response rate is 0.351% (= (135/100)*0.26%); the lower and upper confidence interval end points are 0.330% (= (127/100)*0.26%) and 0.372% (= (143/100)*0.26%), respectively. Quantifying the predictive certainty, that is, constructing prediction con- fidence intervals is likely to be more informative to data analysts and their management than obtaining a point estimate alone. A single calculated value of a statistic such as Cum Lift can be regarded as a point estimate that pro- vides a best guess of the true value of the statistic. However, there is an obvi- ous need to quantify the certainty associated with such a point estimate.The decision maker wants the margin of error of the estimate. Plus or minus, * I experience high precision in bootstrap decile validation with just 50 bootstrap samples. Statistical and Machine-Learning Data Mining Table 19.4 Bootstrap Response Decile Validation (Bootstrap Sample Size n = 181,000) Bootstrap Cum Bootstrap 95% Bootstrap Decile Lift Se CI Top 183 10 (163, 203) 2 151 7 (137, 165) 3 135 4 (127, 143) 4 133 3 (127, 139) 5 125 2 (121, 129) 6 121 1 (119, 123) 7 115 1 (113, 117) 8 110 1 (108, 112) 9 105 1 (103, 107) Bottom 100 0 (100, 100) what value should be added/subtracted to the estimated value to yield an interval for which there is a reasonable confidence that the true (Cum Lift) value lies? If the confidence interval (equivalently, the standard error or mar- gin of error) is too large for the business objective at hand, what can be done? The answer rests on the well-known, fundamental relationship between sample size and confidence interval length: Increasing (decreasing) sample size increases (decreases) confidence in the estimate. Equivalently, increas- ing (decreasing) sample size decreases (increases) the standard error [5]. The sample size-confidence length relationship can be used to increase confi- dence in the bootstrap Cum Lift estimates in two ways: 1. If ample additional customers are available, add them to the original validation dataset until the enhanced validation dataset size pro- duces the desired standard error and confidence interval length. the bootstrap sample size until the enhanced bootstrap dataset size produces the desired standard error and confidence interval length. 225,000 from the original sample size of 181,100 to produce a slight decrease in the standard error from 4 to 3 for the cumulative top three deciles. This simulated validation in Table 19.5 indicates that a Cum Lift of 136 centered in a slightly shorter 95% confidence interval between 130 and 142 is expected when using the RM model to select the top 30% of the most responsive indi- viduals from a randomly selected sample of size 225,000 from the database. from 135 to 136) because their calculations are based on new larger samples. 327 Table 19.5 Bootstrap Response Decile Validation (Bootstrap Sample Size n = 225,000) Bootstrap Decile Cum Lift Bootstrap Se 95% Bootstrap CI Top 185 5 (163, 203) 2 149 3 (137, 165) 3 136 3 (127, 143) 4 133 2 (127, 139) 5 122 1 (121, 129) 6 120 1 (119, 123) 7 116 1 (113 ,117) 8 110 0.5 (108, 112) 9 105 0.5 (103, 107) Bottom 100 0 (100, 100) Bootstrap Cum Lift estimates rarely show big differences when the enhanced dataset size increases. (Why?) In the next section, I continue the discussion on the sample size-confidence length relationship as it relates to a bootstrap assessment of model implementation performance. Implementation performance Statisticians are often asked, “How large a sample do I need to have confi- dence in my results?” The traditional answer, which is based on paramet- ric theoretical formulations, depends on the statistic in question, such as response rate or mean profit, and on additional required input. These addi- tions include the following: 1. The expected value of the desired statistic. the decision maker is willing to take by wrongly rejecting the null hypothesis; this may involve claiming a relationship exists when, in fact, it does not. 1 minus the probability the decision maker is willing to take by wrongly accepting the null hypothesis; this may involve claiming that a relationship does not exist when, in fact, it does. Statistical and Machine-Learning Data Mining Regardless of who has built the marketing/database model, once it is ready for implementation, the question is essentially the same: How large a sample do I need to implement a solicitation based on the model to obtain a desired performance quantity? The database answer, which in this case does not require most of the traditional input, depends on one of two per- formance objectives. Objective 1 is to maximize the performance quantity for a specific depth of file, namely, the Cum Lift. Determining how large the sample should be—actually the smallest sample necessary to obtain the Cum Lift value—involves the concepts discussed in the previous section that correspond to the relationship between confidence interval length and sample size. The following procedure answers the sample size question for objective 1. interval containing Cum Lift values closest to the desired value, based on the decile analysis validation at hand. If the corresponding confidence interval length is acceptable, then the size of the valida- tion dataset is the required sample size. Draw a random sample of that size from the database. the validation sample size by adding individuals or bootstrapping a larger sample size until the confidence interval length is acceptable. which indicates that a smaller sample can be used to save time and cost of data retrieval, then decrease the validation sample size by deleting individuals or bootstrapping a smaller sample size until the confidence interval length is acceptable. Draw a random sample of that size from the database. to discriminate among the quality of individuals who are selected to con- tribute to the performance quantity. This is particularly worth doing when a solicitation is targeted to an unknown, yet believed to be, relatively homo- geneous population of finely graded individuals in terms of responsiveness or profitability. of individuals within a decile is different across the deciles within the pre- selected depth-of-file range. This is accomplished by (1) determining the sample size that produces decile confidence intervals that do not overlap and (2) ensuring an overall confidence level that the individual decile confidence intervals are jointly valid; that is, the true Cum Lifts are “in” their confidence intervals. The former condition is accomplished by increasing sample size. 329 The latter condition is accomplished by employing the Bonferroni method, which allows the analyst to assert a confidence statement that multiple con- fidence intervals are jointly valid. combine k confidence intervals that individually have confidence levels 1 - a1, 1 - a2, . ., 1 - ak. The analyst wants to make a joint confidence statement with confidence level 1 - aJ. The Bonferroni method states that the joint confidence level 1 - aJ is greater than or equal to 1 - a1 - a2..., -ak. The joint confidence level is a conservative lower bound on the actual confidence level for a joint confidence statement. The Bonferroni method is conservative in the sense that it provides confidence intervals that have confidence levels larger than the actual level. intervals: 95%, 90%, 85%, and 80%. confidence that the two true decile Cum Lifts lie between their respective confidence intervals. There is at least 85% confidence that the three true decile Cum Lifts lie between their respective confidence intervals. And, there is at least 80% confidence that the four true decile Cum Lifts lie between their respective confidence intervals."
24,Explain why the model builder starts rebuilding a LRM on the appended training data or the new sample,"1. If the bootstrapped LRMs are stable (in form and predictions), then the model builder picks one of the candidate (bootstrapped) LRMs as the final model. either (a) add more data to the original data or (b) take a new larger sample to serve as the training data. Hence,the model builder starts rebuilding a LRM on the appended training data or the new sample.","The standard use of the Hosmer-Lemeshow (HL) test, confirming a fit- ted LRM is the correct model, is not reliable because it does not distinguish between nonlinearity and noise in checking the model fit [1]. Also, the HL test is sensitive to multiple datasets. The bootstrap validation method provides (1) a measure of how sensitive LRM predictions are to small changes (random perturbations) in the data and, consequently, (2) a procedure to select the best LRM, if it exists. Bootstrap samples are randomly different by way of sampling with replacement from 337 338 Statistical and Machine-Learning Data Mining the original (training) data. The model builder performs repetitions of logis- tic regression modeling based on various bootstrap samples.1. If the bootstrapped LRMs are stable (in form and predictions), then the model builder picks one of the candidate (bootstrapped) LRMs as the final model. either (a) add more data to the original data or (b) take a new larger sample to serve as the training data. Thus, the model builder starts rebuilding a LRM on the appended training data or the new sample. LRM. If the tasks are not effective, then the data are not homoge- neous and step 3 should be conducted. Chapter 27, which addresses overfitting. Delivering homogeneous data is tantamount to eliminating the components of an overfitted model. The model builder may have to stretch his or her thinking from issues of overfitting to issues of homogeneity, but the effort will be fruitful. method, the bootstrap validation method is not an elixir. Due diligence dic- tates that a fresh dataset—perhaps one that includes ranges outside the origi- nal ranges of the predictor variables Xi—must be used on the final LRM. In sum, I confidently use the bootstrap validation method to yield reliable and robust results along with the final LRM tested on a fresh dataset. This how-to chapter introduces a bootstrap validation method for the ever- popular logistic regression model. The method provides 1) a measure of how sensitive LRM predictions are to small changes (random perturbations) in the data, and consequently 2) a procedure to select the best LRM, if it exists Reference 1. Hosmer, D.W., and Lemeshow, S., Applied Logistic Regression, Wiley, New York, 1989. Visualization of Marketing Models*Data Mining to Uncover Innards of a Model 21.1 Introduction Visual displays—commonly known as graphs—have been around for a long time but are currently at their peak of popularity. The popularity is due to the massive amounts of data flowing from the digital environment and the accompanying increase in data visualization software, which provides a bet- ter picture of big data than ever before. Visual displays are commonly used in the exploratory phase of data analysis and model building. An area of untapped potential for visual displays is “what the final model is doing” on implementation of its intended task of predicting. Such displays would increase the confidence in the model builder, while engendering confidence in the marketer, an end user of marketing models. The purpose of this chap- ter is to introduce two data mining graphical methods—star graphs and profile curves—for uncovering the innards of a model: a visualization of the characteristic and performance levels of the individuals predicted by the model. The first visual display had its pictorial debut about 2000 BC when the Egyptians created a real estate map describing data such as property out- lines and owner. The Greek Ptolemy created the first world map circa AD 150 using latitudinal and longitudinal lines as coordinates to represent the earth on a flat surface. In the fifteenth century, Descartes realized that Ptolemy’s geographic map making could serve as a graphical method to identify the * This chapter is based on the following: Ratner, B., Profile curves: a method of multivariate comparison of groups, The DMA Research Council Journal, 28–45 1999. Used with permission. 340 Statistical and Machine-Learning Data Mining relationship between numbers and space, such as patterns [1]. Thus, the com- mon graph was born: a horizontal line (X-axis) and a vertical line (Y-axis) intersecting perpendicularly to create a visual space that occupies numbers defined by an ordered pair of X-Y coordinates. The original Descartes graph, which has been embellished with more than 500 years of knowledge and technology, is the genesis of the discipline of data visualization, which is experiencing an unprecedented growth due to the advances in microprocessor technology and a plethora of visualization software. a limited basis from the seventeenth century to the mid-eighteenth century, then with much more enthusiasm toward the end of the eighteenth century.[2,3] At the end of the eighteenth century, Playfield initiated work in the area of statistical graphics with the invention of the bar diagram (1786) and pie chart (1801).* Following Playfield’s progress with graphical methods, Fourier presented the cumulative frequency polygon, and Quetelet created the fre- quency polygon and histogram. [4] In 1857, Florence Nightingale — who was a self-educated statistician — unknowingly reinvented the pie chart, which she used in her report to the royal commission to force the British army to maintain nursing and medical care to soldiers in the field. [5] In 1977, Tukey started a revolution of numbers and space with his seminal book Exploratory Data Analysis (EDA) [6]. Tukey explained, by setting forth in careful and elaborate detail, the unappreciated value of numerical, counting, and graphical detective work performed by simple arithmetic and easy-to-draw pictures. Almost three decades after reinventing the concept of graph mak- ing as a means of encoding numbers for strategic viewing, Tukey’s “graphic” offsprings are everywhere. They include the box-and-whiskers plot taught in early grade schools and easily generated computer-animated and interactive displays in three-dimensional space and with 64-bit color used as a staple in business presentations. Tukey, who has been called the “Picasso of statistics,” has visibly left his imprint on the visual display methodology of today [6]. mining methods, including smooth and fitted graphs and other Tukey-esque displays for identifying structure of data and fitting of models. Geometry- based graphical methods, which show how the dependent variable varies over the pattern of internal model variables (variables defining the model), are in an area that has not enjoyed growth and can benefit from a Tukey- esque innovation [7]. In this chapter, I introduce two data mining methods to show what the final model is doing, that is, to visualize the individuals iden- tified by the model in terms of variables of interest—internal model vari- ables or external model variables (variables not defining the model)—and the levels of performance of those individuals. I illustrate the methods using a response model, but they equally apply to a profit model."
25,Explain why the SRC in the logistic regression model is defined in Equation (22,"SRC for Xi = (StdDevXi/StdDevY)*Raw reg., coefficient for Xi (22.2) For the logistic regression model, the problem of calculating the standard deviation of the dependent variable, which is logit Y, not Y, is complicated. vide inconsistent results [1]. The simplest is the one used in the SAS system, although it is not without its problems. The StdDev for the logit Y is taken as 1.8138 (the value of the standard deviation of the standard logistic distribution). Hence,the SRC in the logistic regression model is defined in Equation (22.","9. Andrews, D.F., Plots of high-dimensional data, Biometrics, 28, 1972. Demographic Variable about the Deciles Title1 ‘table’; data table; input decile age income educ gender; cards; 1 63 155 18 0.05 2 51 120 16 0.10 3 49 110 14 0.20 4 46 111 13 0.25 5 42 105 13 0.40 6 41 095 12 0.55 7 39 088 12 0.70 8 37 091 12 0.80 9 25 070 12 1.00 10 25 055 12 1.00 ; run; proc print; run; proc standard data = table out = tablez mean = 4 std = 1; var Visualization of Marketing Models 357 age income educ gender; Title1 ‘table stdz’; proc print data = tablez; run; proc format; value dec_fmt 1. = ’top’ 2 = ’ 2 ‘ 3 = ’ 3 ‘ 4 = ’ 4 ‘ 5 = ’ 5 ‘ 6. = ’ 6 ‘ 7 = ’ 7 ‘ 8 = ’ 8 ‘ 9 = ’ 9 ‘ 10 = ’bot’; run; proc greplay nofs igout = work.gseg; delete all; run;quit; goptions reset = all htext = 1.05 device = win targetdevice = winprtg ftext = swissb lfactor = 3 hsize = 2 vsize = 8; proc greplay nofs igout = work.gseg; delete all; run; goptions reset = all device = win targetdevice = winprtg ftext = swissb lfactor = 3; title1 ‘AGE by Decile’; proc gchart data = tablez; format decile dec_fmt. ; star decile/fill = empty discrete sumvar = age slice = outside value = none noheading ; run;quit; Title1 ‘EDUCATON by Decile’; proc gchart data = tablez; format decile dec_fmt. ; star decile/fill = empty discrete sumvar = educ slice = outside value = none noheading; run;quit; Title1 ‘INCOME by Decile’; proc gchart data = tablez; format decile dec_fmt. ; star decile/fill = empty discrete sumvar = income slice = outside value = none noheading; 358 Statistical and Machine-Learning Data Mining run;quit; Title1 ‘GENDER by Decile’; proc gchart data = tablez; format decile dec_fmt.; star decile/fill = empty discrete sumvar = gender slice = outside value = none noheading; run;quit; proc greplay nofs igout = work.gseg tc = sashelp.templt template = l2r2s; treplay 1:1 2:2 3:3 4:4; run;quit; Appendix 2 SAS Code for Star Graphs for Each Decile about the Demographic Variables data table; input decile age income educ gender; cards; 1 63 155 18 0.05 2 51 120 16 0.10 3 49 110 14 0.20 4 46 111 13 0.25 5 42 105 13 0.40 6 41 095 12 0.55 7 39 088 12 0.70 8 37 091 12 0.80 9 25 070 12 1.00 10 25 055 12 1.00 ; run; proc standard data = table out = tablez mean = 4 std = 1; var age income educ gender; Title2 ‘table stdz’; proc print data = tablez; Visualization of Marketing Models 359 run; proc transpose data = tablez out = tablezt prefix = dec_; var age income educ gender; run; proc print data = tablezt; run; proc standard data = tablezt out = tableztz mean = 4 std = 1; Var dec_1 - dec_10; title2’tablezt stdz’; proc print data = tableztz; run; proc transpose data = tablez out = tablezt prefix = dec_; var age income educ gender; run; proc print data = tablezt; run; proc greplay nofs igout = work.gseg; delete all; run;quit; goptions reset = all htext = 1.05 device = win target = winprtg ftext = swissb lfactor = 3 hsize = 4 vsize = 8; Title1 ‘top decile’; proc gchart data = tableztz; star name/fill = empty sumvar = dec_1 slice = outside value = none noheading; run;quit; Title1 ’2nd decile’; proc gchart data = tableztz; star name/fill = empty sumvar = dec_2 slice = outside value = none noheading; run;quit; 360 Statistical and Machine-Learning Data Mining Title1 ’3rd decile’; proc gchart data = tableztz; star name/fill = empty sumvar = dec_3 slice = outside value = none noheading; run;quit; Title1 ’4th decile’; proc gchart data = tableztz; star name/fill = empty sumvar = dec_4 slice = outside value = none noheading; run;quit; Title1 ’5th decile’; proc gchart data = tableztz; star name/fill = empty sumvar = dec_5 slice = outside value = none noheading; run;quit; Title1 ’6th decile’; proc gchart data = tableztz; star name/fill = empty sumvar = dec_6 slice = outside value = none noheading; run;quit; Title1 ’7th decile’; proc gchart data = tableztz; star name/fill = empty sumvar = dec_7 slice = outside value = none noheading; run;quit; Title1 ’8th decile’; proc gchart data = tableztz; star name/fill = empty sumvar = dec_8 slice = outside value = none noheading; run;quit; Title1 ’9th decile’; proc gchart data = tableztz; star name/fill = empty sumvar = dec_9 slice = outside value = none noheading; run;quit; Title1 ‘bottom decile’; Visualization of Marketing Models 361 proc gchart data = tableztz; star name/fill = empty sumvar = dec_10 slice = outside value = none noheading; run;quit; goptions hsize = 0 vsize = 0; Proc Greplay Nofs TC = Sasuser.Templt; Tdef L2R5 Des = ’Ten graphs: five across, two down’ 1/llx = 0 lly = 51 ulx = 0 uly = 100 urx = 19 ury = 100 lrx = 19 lry = 51 2/llx = 20 lly = 51 ulx = 20 uly = 100 urx = 39 ury = 100 lrx = 39 lry = 51 3/llx = 40 lly = 51 ulx = 40 uly = 100 urx = 59 ury = 100 lrx = 59 lry = 51 4/llx = 60 lly = 51 ulx = 60 uly = 100 urx = 79 ury = 100 lrx = 79 lry = 51 5/llx = 80 lly = 51 ulx = 80 uly = 100 urx = 100 ury = 100 lrx = 100 lry = 51 6/llx = 0 lly = 0 ulx = 0 uly = 50 urx = 19 ury = 50 lrx = 19 lry = 0 7/llx = 20 lly = 0 ulx = 20 uly = 50 urx = 39 ury = 50 lrx = 39 lry = 0 8/llx = 40 lly = 0 362 Statistical and Machine-Learning Data Mining ulx = 40 uly = 50 urx = 59 ury = 50 lrx = 59 lry = 0 9/llx = 60 lly = 0 ulx = 60 uly = 50 urx = 79 ury = 50 lrx = 79 lry = 0 10/llx = 80 lly = 0 ulx = 80 uly = 50 urx = 100 ury = 50 lrx = 100 lry = 0; Run; Quit; Proc Greplay Nofs Igout = Work.Gseg TC = Sasuser.Templt Template = L2R5; Treplay 1:1 2:2 3:3 4:4 5:5 6:6 7:7 8:8 9:9 10:10; run;quit; Appendix 3 SAS Code for Profile Curves: All Deciles Title1’table’; data table; input decile age income educ gender; cards; 1 63 155 18 0.05 2 51 120 16 0.10 3 49 110 14 0.20 4 46 111 13 0.25 5 42 105 13 0.40 6 41 095 12 0.55 7 39 088 12 0.70 8 37 091 12 0.80 9 25 070 12 1.00 10 25 055 12 1.00 ; Visualization of Marketing Models 363 run; data table; Set table; x1 = age;x2 = income;x3 = educ;x4 = gender; proc print; run; data table10; sqrt2 = sqrt(2); array f {10}; do t = -3.14 to 3.14 by.05; do i = 1 to 10; set table point = i; f(i) = x1/sqrt2 + x4*sin(t) + x3*cos(t) + x2*sin(2*t); end; output; label f1 = ’00’x; end; stop; run; goptions reset = all device = win target = winprtg ftext = swissb lfactor = 3; Title1 ‘Figure 21.6 Profile Curves: All Deciles’; proc gplot data = table10; plot f1*t = ’T’ f2*t = ’2’ f3*t = ’3’ f4*t = ’4’ f5*t = ’5’ f6*t = ’6’ f7*t = ’7’ f8*t = ’8’ f9*t = ’9’ f10*t = ’B’ /overlay haxis = -3 -2 -1 0 1 2 3 nolegend vaxis = -150 to 250 by 50; run;quit; This page intentionally left blank 22 The Predictive Contribution Coefficient: A Measure of Predictive Importance 22.1 Introduction Determining the most important predictor variables in a regression model is a vital element in the interpretation of the model. A general rule is to view the predictor variable with the largest standardized coefficient (SRC) as the most important variable, the predictor variable with the next-largest SRC as the next important variable, and so on. This rule is intuitive, is easy to apply, and provides practical information for understanding how the model works. purpose of this chapter is twofold: first, to discuss why the decision rule is theoretically amiss yet works well in practice; second, to present an alterna- tive measure—the predictive contribution coefficient (PCC)—that offers greater utile information than the standardized coefficient as it is an assumption-free measure founded in the data mining paradigm. Let Y be a continuous dependent variable, and X1, X2, … , Xi, … , Xn be the predictor variables. The linear regression model is defined in Equation (22.1): Y = b0 + b1*X1 + b2*X2 + … + bi*Xi + … + bn*Xn (22.1) The b’s are the raw regression coefficients, which are estimated by the method of ordinary least squares. Once the coefficients are estimated, an individual’s predicted Y value is calculated by “plugging in” the values of the predictor variables for that individual in the equation. question: How does Xi affect the prediction of Y? The answer is the predicted Y experiences an average change of bi with a unit increase in Xi when the 365 366 Statistical and Machine-Learning Data Mining other X’s are held constant. A common misinterpretation of the raw regres- sion coefficient is that the predictor variable with the largest value (the sign of the coefficient is ignored) has the greatest effect on the predicted Y. Unless the predictor variables are measured in the same units, the raw regression coefficient values can be so unequal that they cannot be compared with each other. The raw regression coefficients must be standardized to import the different units of measurement of the predictor variables to allow a fair comparison. This discussion is slightly modified for the situation of a binary dependent variable: The linear regression model is the logistic regression model, and the method of maximum likelihood is used to estimate the regression coefficients, which are linear in the logit of Y. The standardized regression coefficients (SRCs) are just plain numbers, like “points,” allowing material comparisons among the predictor variables. a conversion factor, a ratio of a unit measure of Xi variation to a unit mea- sure of Y variation. The SRC in an ordinary regression model is defined in Equation (22.2), where StdDevXi and StdDevY are the standard deviations for Xi and Y, respectively.SRC for Xi = (StdDevXi/StdDevY)*Raw reg., coefficient for Xi (22.2) For the logistic regression model, the problem of calculating the standard deviation of the dependent variable, which is logit Y, not Y, is complicated. vide inconsistent results [1]. The simplest is the one used in the SAS system, although it is not without its problems. The StdDev for the logit Y is taken as 1.8138 (the value of the standard deviation of the standard logistic distribution).Thus, the SRC in the logistic regression model is defined in Equation (22.3). (22.3) The SRC can also be obtained directly by performing the regression analysis on standardized data. Recall that standardizing the dependent variable Y and the predictor variables Xi’s creates new variables zY and zXi, such that their means and standard deviations are equal to zero and one, respectively. The coefficients obtained by regressing zY on the zXi’s are, by definition, the SRCs. tant predictors, in ranked order, needs to be annotated before being answered. characteristic of reduction in prediction error. The usual answer is provided by the decision rule: The variable with the largest SRC is the most important variable; the variable with the next-largest SRC is the next important variable, and so on. This decision rule is correct with the unnoted caveat that the predictor variables are uncorrelated. There is a rank order correspondence between the SRC (the sign of the coefficient is ignored) and the reduction in prediction The Predictive Contribution Coefficient 367 error in a regression model with only uncorrelated predictor variables. There is one other unnoted caveat for the proper use of the decision rule: The SRC for a dummy predictor variable (defined by only two values) is not reliable as the standard deviation of a dummy variable is not meaningful. variables, which challenge the utility of the decision rule. Yet, the rule pro- vides continuously useful information for understanding how the model works without raising sophistic findings. The reason for its utility is there is an unknown working assumption at play: The reliability of the ranking based on the SRC increases as the average correlation among the predictor variables decreases (see Chapter 13). Thus, for well-built models, which nec- essarily have a minimal correlation among the predictor variables, the deci- sion rule remains viable in virtually all regression applications. However, there are caveats: Dummy variables cannot be reliably ranked, and compos- ite variables (derived from other predictor variables) and the elemental vari- ables (defining the composite variables) are highly correlated inherently and thus cannot be ranked reliably. Consider RESPONSE (0 = no; 1 = yes), PROFIT (in dollars), AGE (in years), GENDER (1 = female, 0 = male) and INCOME (in thousand dollars) for 10 individuals in the small data in Table 22.1. I standardized the data to produce the standardized variables with the notation used previously: zRESPONSE, zPROFIT, zAGE, zGENDER, and zINCOME (data not shown). the standardized data. Specifically, I regress PROFIT on INCOME, AGE, and Table 22.1 Small Data ID# Response Profit Age Gender Income 1 1 185 65 0 165 2 1 174 56 0 167 3 1 154 57 0 115 4 0 155 48 0 115 5 0 150 49 0 110 6 0 119 40 0 99 7 0 117 41 1 96 8 0 112 32 1 105 9 0 107 33 1 100 10 0 110 37 1 95 368 Statistical and Machine-Learning Data Mining Table 22.2 PROFIT Regression Output Based on Raw Small Data Parameter Standard Standardized Variable DF Estimate Error t Value Pr > t Estimate INTERCEPT 1 37.4870 16.4616 2.28 0.0630 0.0000 INCOME 1 0.3743 0.1357 2.76 0.0329 0.3516 AGE 1 1.3444 0.4376 3.07 0.0219 0.5181 GENDER 1 –11.1060 6.7221 –1.65 0.1496 –0.1998 Table 22.3 PROFIT Regression Output Based on Standardized Small Data Parameter Standard Standardized Variable DF Estimate Error t Value Pr 1+1 Estimate INTERCEPT 1 –4.76E–16 0.0704 0.0000 1.0000 0.0000 zINCOME 1 0.3516 0.1275 2.7600 0.0329 0.3516 zAGE 1 0.5181 0.1687 3.0700 0.0219 0.5181 zGENDER 1 –0.1998 0.1209 –1.6500 0.1496 –0.1998 GENDER and regress zPROFIT on zINCOME, zAGE, and zGENDER. The raw regression coefficients and SRCs based on the raw data are in the “Parameter Estimate” and “Standardized Estimate” columns in Table 22.2, respectively. 1.3444, and -11.1060, respectively. The SRCs for INCOME, AGE, and GENDER are 0.3516, 0.5181, and -0.1998, respectively. are in the “Parameter Estimate” and “Standardized Estimate” columns in Table 22.3, respectively. As expected, the raw regression coefficients—which are now the SRCs—are equal to the values in the “Standardized Estimate” column; for zINCOME, zAGE, and zGENDER, the values are 0.3516, 0.5181, and -0.1998, respectively. which is a gross 0.71. Thus, the SRC provides a questionable ranking of the predictor variables for PROFIT. AGE is the most important predictor variable, followed by INCOME; the ranked position of GENDER is undetermined. the standardized data. Specifically, I regress RESPONSE on INCOME, AGE, and GENDER and regress RESPONSE* on zINCOME, zAGE, and zGEN- DER. The raw and standardized logistic regression coefficients based on the raw data are in the “Parameter Estimate” and “Standardized Estimate” col- umns in Table 22.4, respectively. The raw logistic regression coefficients for INCOME, AGE, and GENDER are 0.0680, 1.7336, and 14.3294, respectively. The Predictive Contribution Coefficient 369 Table 22.4 RESPONSE Regression Output Based on Raw Small Data Parameter Standard Wald Standardized Variable DF Estimate Error Chi-Square Pr > ChiSq Estimate INTERCEPT 1 –99.5240 308.0000 0.1044 0.7466 INCOME 1 0.0680 1.7332 0.0015 0.9687 1.0111 AGE 1 1.7336 5.9286 0.0855 0.7700 10.5745 GENDER 1 14.3294 82.4640 0.0302 0.8620 4.0797 Table 22.5 RESPONSE Regression Output Based on Standardized Small Data Parameter Standard Wald Standardized Variable DF Estimate Error Chi-Square Pr > Chi-Sq Estimate INTERCEPT 1 –6.4539 31.8018 0.0412 0.8392 zINCOME 1 1.8339 46.7291 0.0015 0.9687 1.0111 zAGE 1 19.1800 65.5911 0.0855 0.7700 10.5745 zGENDER 1 7.3997 42.5842 0.0302 0.8620 4.0797 The standardized logistic regression coefficients for zINCOME, zAGE, and zGENDER are 1.8339, 19.1800, and 7.3997, respectively (Table 22.5). Although this is a trivial example, it still serves valid PCC calculations. standardized data are in the “Parameter Estimate” and “Standardized Estimate” columns in Table 22.5, respectively. Unexpectedly, the raw logis- tic regression coefficients—which are now the standardized logistic regres- sion coefficients—do not equal the values in the “Standardized Estimate” column. If the ranking of predictor variables is required of the standard- ized coefficient, this inconsistency presents no problem as the raw and standardized values produce the same rank order. If information about the expected increase in the predicted Y is required, I prefer the SRCs in the “Parameter Estimate” column as it follows the definition of SRC. tor variables is a gross 0.71. Thus, the SRC provides questionable ranking of the predictor variables for RESPONSE. AGE is the most important predictor vari- able, followed by INCOME; the ranked position of GENDER is undetermined. The PCC is a development in the data mining paradigm. It has the hall- marks of the data mining paradigm: flexibility because it is an assumption- free measure that works equally well with ordinary and logistic regression 370 Statistical and Machine-Learning Data Mining models; practicality and innovation because it offers greater utile informa- tion than the SRC; and above all, simplicity because it is easy to understand and calculate, as is evident from the following discussion. defined in Equation (22.4): zY = b0 + b1*zX1 + b2*zX2 + … + bi*zXi + … + bn*zXn (22.4) The PCC for zXi, PCC(zXi), is a measure of the contribution of zXi relative to the contribution of the other variables to the predictive scores of the model.PCC(zXi) is defined as the average absolute ratio of the zXi score-point con- tribution (zXi*bi) to the score-point contribution (total predictive score minus zXis score-point) of the other variables. Briefly, the PCC is read as follows: The larger the PCC(zXi) value is, the more significant the part zXi has in the predictions of the model and the more important zXi is as a predictor vari- able. Exactly how the PCC works and what are its benefits over the SRC are discussed in the next section. Now, I provide justification for the PCC. the SRC, which itself, as discussed, is not a perfect measure to rank predictor variables. The effects of any “impurities” (biases) carried by the SRC on the PCC are presumably negligible due to the “wash-cycle” calculations of the PCC. The six-step cycle, which is described in the next section, crunches the actual values of the SRC such that any original bias effects are washed out. sion model are the most important predictors. The predictive contribution decision rule is that the variable with the largest PCC is the most important variable; the variable with the next-largest PCC is the next important vari- able, and so on. The predictor variables can be ranked from most to least important based on the descending order of the PCC. Unlike the decision rule for reduction in prediction error, there are no presumable caveats for the predictive contribution decision rule. Correlated predictor variables, includ- ing composite and dummy variables, can be thus ranked. Consider the logistic regression model based on the standardized data in Table 22.5. I illustrate in detail the calculation of PCC(zAGE) with the neces- sary data in Table 22.6. the data. For individual ID 1, the values of the standardized predic- tor variable, in Table 22.6, which are multiplied by the corresponding The P redictive C Table 22.6 ontr Necessary Data ibu zAGE Score- OTHERVARS tio Total Predicted Point Score-Point zAGE_ n C ID # zAGE zINCOME zGENDER Score Contribution Contribution othvars oe 1 1.7354 1.7915 –0.7746 24.3854 33.2858 –8.9004 3.7398 ffic 2 0.9220 1.8657 –0.7746 8.9187 17.6831 –8.7643 2.0176 ien 3 1.0123 –0.0631 –0.7746 7.1154 19.4167 –12.3013 1.5784 t 4 0.1989 –0.0631 –0.7746 –8.4874 3.8140 –12.3013 0.3100 5 0.2892 –0.2485 –0.7746 –7.0938 5.5476 –12.6414 0.4388 6 –0.5243 –0.6565 –0.7746 –23.4447 –10.0551 –13.3897 0.7510 7 –0.4339 –0.7678 1.1619 –7.5857 –8.3214 0.7357 11.3105 8 –1.2474 –0.4340 1.1619 –22.5762 –23.9241 1.3479 17.7492 9 –1.1570 –0.6194 1.1619 –21.1827 –22.1905 1.0078 22.0187 10 –0.7954 –0.8049 1.1619 –14.5883 –15.2560 0.6677 22.8483 371 372 Statistical and Machine-Learning Data Mining standardized logistic regression coefficients in Table 22.5, produce the TOTAL PREDICTED score of 24.3854. in the data. For individual ID 1, the zAGE SCORE-POINT contribu- tion is 33.2858 (= 1.7354*19.1800). individual in the data. For individual ID 1, the other variables SCORE-POINT contribution is -8.9004 (= 24.3854 - 33.2858). OTHVARS is defined as the absolute ratio of zAGE SCORE-POINT contribution to the other variables SCORE-POINT contribution. For ID 1, zAGE_OTHVARS is 3.7398 (= absolute value of 33.2858/-8.9004). values: 2.8787. The zAGE_OTHVARS distribution is typically skewed, which suggests that the median is more appropriate than the mean for the average. step process for zINCOME and zGENDER (not shown). GENDER, and last is INCOME. Their PCC values are 2.8787, 0.3810, and 0.0627, respectively. “large” PCC value of 2.8787. The implication is AGE is clearly driving the predictions of the model. When a predictor variable has a large PCC value, it is known as a key driver of the model."
26,"Explain why I attempt to reex- press the paired variables (XX, Y) to straighten the curvilinear relation- ship","If a reason for the existence of outlier point (1, 20) cannot be posited, then the relationship is assumed curvilinear, as depicted in the scatterplot in Figure 26.2. In this case, there is no outlier, and the model builder seeks to reexpress the paired variables (XX, Y) to straighten the curvilinear relation- ship and then observe the resultant scatterplot for new outliers. Statistical and Machine-Learning Data Mining Table 26.2 Correlation Coefficients: N = 101 Prob > r under H0: Rho=0 XX GenIQvar Y −0.41618 0.84156 <.0001 <.0001 Plot of Y*XX. Legend: A = 1 obs, B = 2 obs, ..., Y = 25 obs 20 A 19 18 17 16 15 14 13 12 Y 11 10987654 Y 3 Y 2 Y 1 Y 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 XX FIgure 26.2 Scatterplot of (XX, Y) depicting a nonlinear relationship.I have doubts about point (1, 20) as an outlier. Hence,I attempt to reex- press the paired variables (XX, Y) to straighten the curvilinear relation- ship.","The correlation coefficient of (XX, Y) is -0.41618 (see Table 26.2). The correla- tion coefficient value implies the strength of the linear relationship between XX and Y only if the corresponding scatterplot indicates an underlying lin- ear relationship between XX and Y. Under the momentary assumption, the relationship between XX and Y is not linear; thus, the correlation coefficient value of -0.41618 is meaningless.If a reason for the existence of outlier point (1, 20) cannot be posited, then the relationship is assumed curvilinear, as depicted in the scatterplot in Figure 26.2. In this case, there is no outlier, and the model builder seeks to reexpress the paired variables (XX, Y) to straighten the curvilinear relation- ship and then observe the resultant scatterplot for new outliers. Statistical and Machine-Learning Data Mining Table 26.2 Correlation Coefficients: N = 101 Prob > r under H0: Rho=0 XX GenIQvar Y −0.41618 0.84156 <.0001 <.0001 Plot of Y*XX. Legend: A = 1 obs, B = 2 obs, ..., Y = 25 obs 20 A 19 18 17 16 15 14 13 12 Y 11 10987654 Y 3 Y 2 Y 1 Y 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 XX FIgure 26.2 Scatterplot of (XX, Y) depicting a nonlinear relationship.I have doubts about point (1, 20) as an outlier. Thus, I attempt to reex- press the paired variables (XX, Y) to straighten the curvilinear relation- ship. I apply the GenIQ Model to paired variables (XX, Y). The GenIQ reexpression of (XX, Y) is (GenIQvar, Y), where GenIQvar is the GenIQ data-mined transformation of XX. (The specification of the transfor- mation is discussed in Section 26.3.2) The scatterplot of (GenIQvar, Y) (Figure 26.3) shows no outliers. More important, the scatterplot indicates a linear relationship between GenIQvar and Y and the meaningful cor- relation coefficient of (GenIQvar, Y), 0.84156 (see Table 26.2) quantifies the linear relationship as strong. an explanation in light of the four mass points where GenIQ is −1.00, on the horizontal axis, labeled GenIQvar. The four points are in a vertical trend, which is viewed as a variation about a “true” transformed mass point. The scatterplot shows the quintessential straight line defined by two points: the true transformed point, positioned at the middle of the four Y’s, (–1.00, 2.5), and the point (1.00, 20), labeled A. A Data Mining Method for Moderating Outliers 413 Plot of Y*GenIQvar. Legend: A = 1 obs, B = 2 obs, ..., Y = 25 obs 20 A 19 18 17 16 15 14 13 12 Y 11 10987654 Y3Y2Y1Y –1.00 –0.75 –0.50 –0.25 0.00 0.25 0.50 0.75 1.00 GenIQvar FIgure 26.3 Scatterplot of (GenIQvar, Y). XX × + XX –0.922 GenIQvar 1 – XX GenIQvar Computer Code × XX x1 = XX; x2 = XX; x1 = x1 * x2; x2 = 1; x1 = x2 – x1; x2 = –0.921982; x3 = XX; x4 = XX; x3 = x3 * x4; x2 = x2 + x3; If x1 NE 0 Then x1 = x2/x1; Else x1 = 1; GenIQvar = x1; FIgure 26.4 GenIQ Model for moderating the outlier. Statistical and Machine-Learning Data Mining 26.3.2 The genIQ Model for Moderating the Outlier The GenIQ Model moderates the outlier point (1, 20) by reexpressing all 101 points. Comparing scatterplots in Figures 26.1 and 26.3, the outlier is repo- sitioned from top left to top right in the scatterplot, and the remaining four mass points are repositioned from the bottom left to the bottom right in the scatterplot. The GenIQ Model transformation is defined by the GenIQvar tree and GenIQvar computer code (Figure 26.4). serve as a multivariate method for moderating virtually all outliers in the data. This is possible due to the nature of the GenIQ decile table fitness func- tion. A discussion of how optimizing the decile table fitness function brings forth a moderation of virtually all outliers is beyond the scope of this chap- ter. Suffice it to say that such optimization is equivalent to straightening the data by repositioning the outliers into a multilinear pattern, thus moderating the outliers instead of discarding them. The common remedy for handling outliers is to determine and discard them. I presented an alternative data mining method for moderating outliers instead of discarding them. I illustrated the data mining feature of the GenIQ Model as the alternative method for handling outliers. Overfitting: Old Problem, New Solution 27.1 Introduction Overfitting, a problem akin to model inaccuracy, is as old as model build- ing itself, as it is part of the modeling process. An overfitted model is one that approaches reproducing the training data on which the model is built. The effect of overfitting is an inaccurate model. The purpose of this chapter is to introduce a new solution, based on the data mining feature of the GenIQ Model, to the old problem of overfitting. I illustrate how the GenIQ Model identifies the complexity of the idiosyncrasies and subsequently instructs for deletion of the individuals that contribute to the complexity in the data under consideration. Overfitting, a problem akin to model inaccuracy, is as old as model building itself, as it is part of the modeling process. An overfitted model is one that approaches reproducing the training data on which the model is built—by capitalizing on the idiosyncrasies of the training data. The model reflects the com- plexity of the idiosyncrasies by including extra variables, interactions, and variable constructs. It follows that a key characteristic of an overfitted model is the model has too many variables: An overfitted model is too complex. picture of the predominant pattern in the data; the model memorizes the training data instead of capturing the desired pattern. Individuals of valida- tion data (drawn from the population of the training data) are strangers who are unacquainted with the training data and cannot expect to fit into the model’s perfect picture of the predominant pattern.When the accuracy of a model based on the validation data is “out of the neighborhood” of the accuracy of the model based on the training data, the problem is one of overfitting. As the fit of the model increases by including 415 416 Statistical and Machine-Learning Data Mining more information (seemingly to be a good thing), the predictive performance of the model on the validation data decreases. This is the paradox of overfitting. val about the prediction error is large. to Help remember the Concept A model is built to represent training data, not to reproduce training data. feel at home with the model. The visitor encounters an uncomfortable fit in the model because he or she probabilistically does not look like a typical data point from the training data. The misfit visitor takes a poor prediction. The model is overfitted. underfitted model is too simple. An underfitted model can be considered a poorly rendered picture of the predominant pattern; without recollection of the training data, the model captures poorly the desired pattern. Individuals of validation data are strangers who have no familiarity with the training data and cannot expect to fit into the model’s portraiture of the predominant pattern. affect error bias. Bias is the difference between the predicted score and the true score. Underfitted models have large error bias; predicted scores are wildly far from the true scores. Figure 27.1 is a graphical depiction of over- and underfitted models.* Consider the two models: the simple model g(x) in the left-hand graph and the zigzag model in right-hand graph in Figure 27.1. Clearly, I want a model that best represents the predominant pattern of the parabola as depicted by the data points indicated by circles. I fit the points with the straight-line model g(x), using only one variable (too few variables). The model is visibly too simple. It does not do a good job of fitting the data and would not do well in predicting for new data points. This model is underfitted."
27,Explain why every model has a fitness value reflecting how well it solves the problem,"2. Select the set of functions (e.g., the set of arithmetic operators [addi- tion, subtraction, multiplication, division]; log and exponential) and variables (predictors X1, X2, …, Xn and numerical values) that are believed to be related to the problem at hand (the dependent vari- able Y).* An initial population of random models is generated using the preselected set of functions and variables. the model to a training set, a sample of individuals along with their values on the predictor variables X1, X2, …, Xn and the dependent variable Y. Hence,every model has a fitness value reflecting how well it solves the problem.","** The focus of this chapter is optimization, but genetic modeling has been applied to a variety of problems: optimal control, planning, sequence induction, empirical discovery and fore-casting, symbolic integration, and discovering mathematical identities. 433 better than a model with a lower fitness value and survives and reproduces at a high rate. Models that are less fit survive and reproduce, if at all, at a lower rate. contain probabilistically some valuable genetic material. Recombining the parts of highly fit “parent” models produces probabilistically offspring mod- els that are better fit at solving the problem than either parent. Offspring models then become the parents of the next generation, repeating the recom- bination process. After many generations, an evolved model is declared the best-so-far solution of the problem. 1. Define the fitness function. The fitness function allows for identify- ing good or bad models, after which refinements are made with the goal of producing the best model.2. Select the set of functions (e.g., the set of arithmetic operators [addi- tion, subtraction, multiplication, division]; log and exponential) and variables (predictors X1, X2, …, Xn and numerical values) that are believed to be related to the problem at hand (the dependent vari- able Y).* An initial population of random models is generated using the preselected set of functions and variables. the model to a training set, a sample of individuals along with their values on the predictor variables X1, X2, …, Xn and the dependent variable Y. Thus, every model has a fitness value reflecting how well it solves the problem. operators. The genetic operators are applied to models in the current population selected with a probability based on fitness (i.e., the fitter the model, the more likely the model is to be selected). the new population. by genetically recombining randomly chosen parts of two parent models from the current population. rent population into the new population. declared the best-of-generation model, which is the solution, or an approxi- mate solution, to the problem. Statistical and Machine-Learning Data Mining 29.4 Genetic Modeling: An Illustration Consider the process of building a response model, for which the dependent variable RESPONSE assumes two values: yes and no. I designate the best model as one with the highest R-squared* value. Thus, the fitness function is the formula for R-squared. (Analytical note: I am using the R-squared mea- sure only for illustrative purposes; it is not the fitness function of the GenIQ Model. The GenIQ Model fitness function is discussed in Section 29.8.) I have to select functions and variables that are related to the problem at hand (e.g., predicting RESPONSE). Selection is based on theoretical rationale or empirical expertise; sometimes, function selection is based on a rapid- pace trial and error.I have two variables, X1 and X2, to use as predictors of RESPONSE. Thus, the variable set contains X1 and X2. I add the numerical value “b” to the vari- able set based on prior experience. I define the function set to contain the four arithmetic operations and the exponential function (exp), also based on prior experience. ased function roulette wheel (Figure 29.1) and an unbiased function-variable roulette wheel (Figure 29.2). The slices of the function wheel are of equal size, namely, 20%. The slices of the function-variable wheel are of equal size, namely, 12.5%. Note that the division symbol % is used to denote a “protected” divi- sion. This means that division by zero, which is undefined, is set to the value 1. of the wheel falls on slice “+.” Next, I spin the function-variable wheel, and the pointer lands on slice X1. With two following spins of the function-vari- able wheel, the pointer lands on slices “X1” and “b,” successively. I decide to stop evolving the model at this point. The resultant random model 1 is depicted in Figure 29.3 as a rooted point-label tree. tion wheel once, then by spinning the function-variable wheel twice. The pointer lands on slices “+,” X1, and X1, successively. Similarly, I generate three additional random models, models 3, 4, and 5 in Figures 29.5, 29.6, and 29.7, respectively. Thus, I have generated the initial population of five random models (genetic population size is five). Table 29.1 to indicate how well it solves the problem of predicting RESPONSE."
28,"Explain why I accept the expected constant change of the partial linear-RC for DOLLAR_2, 0","3. At this point, the pending implication is that there are four levels of expected change in the logit RESPONSE associated by DOLLAR_2 across its range from 43 to 1293. ship of the smooth predicted logit RESPONSE (column 8) versus the smooth DOLLAR_2 (column 5), in Figure 31.2, indicates there is a single expected constant change across the DOLLAR_2 range, as the variation among slice-level changes is reasonably due to sample variation. This last examination supports the decided implication that the linearity assumption of the partial linear-RC for DOLLAR_2 is valid. Hence,I accept the expected constant change of the partial linear-RC for DOLLAR_2, 0.","2. Similarly, for slices 3, 4, and 5, the expected constant changes in the logit RESPONSE within the corresponding intervals are 0.0016, 0.0018, and 0.0015, respectively. Note that for slice 5, the maximum DOLLAR_2 value, in column 3, is 1,293.3. At this point, the pending implication is that there are four levels of expected change in the logit RESPONSE associated by DOLLAR_2 across its range from 43 to 1293. ship of the smooth predicted logit RESPONSE (column 8) versus the smooth DOLLAR_2 (column 5), in Figure 31.2, indicates there is a single expected constant change across the DOLLAR_2 range, as the variation among slice-level changes is reasonably due to sample variation. This last examination supports the decided implication that the linearity assumption of the partial linear-RC for DOLLAR_2 is valid. Thus, I accept the expected constant change of the partial linear-RC for DOLLAR_2, 0.00210 (from Equation 31.5). tion-free estimate of the partial linear-RC for DOLLAR_2, par- tial quasi-RC(linear), which is defined as the regression coefficient of the simple ordinary regression of the smooth logit predicted RESPONSE on the smooth DOLLAR_2, columns 8 and 5, respec- tively. The partial quasi-RC(linear) for DOLLAR_2 is 0.00159 (details not shown). Statistical and Machine-Learning Data Mining Symbol is Value of Slice. 5 t –3.3 gi d Lote –3.4 4 Estima 3 –3.5 2 1 –3.6 25 50 75 100 125 150 175 200 225 250 Dollars Spent w/2 yrs FIguRe 31.2 Visual display of LRM partial quasi-RC(logit) for DOLLAR_2. analyst, who is intimate with the data, can decide on. They are (1) accept the partial quasi-RC after asserting the variation among slice-level changes in the partial quasi-RC plot as nonrandom; (2) accept the partial linear-RC (0.00210) after the partial quasi-RC plot validates the linearity assumption; and (3) accept the trusty partial quasi-RC(linear) estimate (0.00159) after the partial quasi-RC plot validates the linearity assumption. Of course, the default alter- native is to accept outright the partial linear-RC without testing the linearity assumption. Note that the small difference in magnitude between the trusty and the “true” estimates of the partial linear-RC for DOLLAR_2 is not typi- cal, as the next discussion shows. to correspond to the distinct values of LSTORD_M, in Table 31.7. the smooth predicted logit RESPONSE and the smooth LSTORD_M, in Figure 31.3, is clearly nonlinear with expected changes in the logit RESPONSE: -0.0032, -0.1618, -0.1067, -0.0678, and 0.0175. does not hold. There is not a constant expected change in the logit RESPONSE as implied by the prescribed interpretation of the partial linear-RC for LSTORD_M, -0.0798 (in Equation 31.5). oeffic TabLe 31.7 ien Calculations for LRM Partial Quasi-RC(logit): LSTORD_M t-Fr med_ ee M min_ m2ax_ med_ LSTORD_M_ change_ med_ med_ change_ quasi-RC Slice LSTORD_M LSTORD_M LSTORD_M_r r+1 LSTORD_M lgt_r lgt_r+1 lgt (logit) ode 1 1 1 . . . 2 1 3 1 2 1 –3.2332 –3.2364 –0.0032 –0.0032 3 3 3 2 3 1 –3.2364 –3.3982 –0.1618 –0.1618 4 3 4 3 4 1 –3.3982 –3.5049 –0.1067 –0.1067 5 4 5 4 5 1 –3.5049 –3.5727 –0.0678 –0.0678 6 5 12 5 6 1 –3.5727 –3.5552 0.0175 0.0175 485 486 Statistical and Machine-Learning Data Mining Symbol is Value of Slice. 1 2 tgi d Lo –3.4 3 te Estima 4 6 5 –3.6 1 2 3 4 5 6 No. Months Since Last Order FIguRe 31.3 Visual display of LRM partial quasi-RC(logit) for LSTORD_M. 3. The secondary implication is that the structural form of LSTORD_M is not correct. The S-shaped nonlinear pattern suggests that qua- dratic or cubic reexpressions of LSTORD_M be tested for model inclusion.4. Satisfyingly, the partial quasi-RC(linear) value of -0.0799 (from the simple ordinary regression of the smooth predicted logit RESPONSE on the smooth LSTORD_M) equals the partial linear-RC value of -0.0798. The implications are as follows: (1) The partial linear-RC provides the average constant change in the logit RESPONSE across the LSTORD_M range of values from 1 to 66; (2) the partial quasi-RC provides more accurate reading of the changes with respect to the six presliced intervals across the LSTORD_M range. CELL and AGE_Y support the linearity assumption of the partial linear- RC. Thus, the partial quasi-RC(linear) and the partial linear-RC values should be equivalent. In fact, they are: For RFM_CELL, the partial quasi- RC(linear) and the partial-linear RC are -0.2007 and -0.1995, respectively; for AGE_Y, the partial quasi-RC(linear) and the partial-linear RC are 0.5409 and 0.5337, respectively. ology perform quite well on the linear predictions based on multiple predic- tor variables. Suffice it to say, by converting the logits into probabilities—as Interpretation of Coefficient-Free Models 487 was done in the simple logistic regression illustration in Section 31.3.3—the quasi-RC approach performs equally well with nonlinear predictions based on multiple predictor variables. The linear regression paradigm, with nearly two centuries of theoretical development and practical use, has made the equation form—the sum of weighted predictor variables (Y = b0 + b1X1 + b2X2 + … + bnXn)—the icon of predictive models. This is why the new machine-learning techniques of the last half-century are evaluated by the coefficients they produce. If the new coefficients impart comparable information to the regression coefficient, then the new technique passes the first line of acceptance. If not, the technique is all but summarily rejected. Ironically, some machine-learning methods offer better predictions without the use of coefficients. The burden of acceptance of the coefficient-free model lies with the extraction of something familiar and trusting. The quasi-RC procedure provides data analysts and marketers the comfort and security of coefficient-like information for evaluating and using the coefficient-free machine-learning models. quasi-RC method. One of the most popular coefficient-free models is the regression tree (e.g., CHAID [chi-squared automatic interaction detection]."
29,Explain why the partial quasi-RC(linear) and the partial linear-RC values should be equivalent,"4. Satisfyingly, the partial quasi-RC(linear) value of -0.0799 (from the simple ordinary regression of the smooth predicted logit RESPONSE on the smooth LSTORD_M) equals the partial linear-RC value of -0.0798. The implications are as follows: (1) The partial linear-RC provides the average constant change in the logit RESPONSE across the LSTORD_M range of values from 1 to 66; (2) the partial quasi-RC provides more accurate reading of the changes with respect to the six presliced intervals across the LSTORD_M range. CELL and AGE_Y support the linearity assumption of the partial linear- RC. Hence,the partial quasi-RC(linear) and the partial linear-RC values should be equivalent.","3. At this point, the pending implication is that there are four levels of expected change in the logit RESPONSE associated by DOLLAR_2 across its range from 43 to 1293. ship of the smooth predicted logit RESPONSE (column 8) versus the smooth DOLLAR_2 (column 5), in Figure 31.2, indicates there is a single expected constant change across the DOLLAR_2 range, as the variation among slice-level changes is reasonably due to sample variation. This last examination supports the decided implication that the linearity assumption of the partial linear-RC for DOLLAR_2 is valid. Thus, I accept the expected constant change of the partial linear-RC for DOLLAR_2, 0.00210 (from Equation 31.5). tion-free estimate of the partial linear-RC for DOLLAR_2, par- tial quasi-RC(linear), which is defined as the regression coefficient of the simple ordinary regression of the smooth logit predicted RESPONSE on the smooth DOLLAR_2, columns 8 and 5, respec- tively. The partial quasi-RC(linear) for DOLLAR_2 is 0.00159 (details not shown). Statistical and Machine-Learning Data Mining Symbol is Value of Slice. 5 t –3.3 gi d Lote –3.4 4 Estima 3 –3.5 2 1 –3.6 25 50 75 100 125 150 175 200 225 250 Dollars Spent w/2 yrs FIguRe 31.2 Visual display of LRM partial quasi-RC(logit) for DOLLAR_2. analyst, who is intimate with the data, can decide on. They are (1) accept the partial quasi-RC after asserting the variation among slice-level changes in the partial quasi-RC plot as nonrandom; (2) accept the partial linear-RC (0.00210) after the partial quasi-RC plot validates the linearity assumption; and (3) accept the trusty partial quasi-RC(linear) estimate (0.00159) after the partial quasi-RC plot validates the linearity assumption. Of course, the default alter- native is to accept outright the partial linear-RC without testing the linearity assumption. Note that the small difference in magnitude between the trusty and the “true” estimates of the partial linear-RC for DOLLAR_2 is not typi- cal, as the next discussion shows. to correspond to the distinct values of LSTORD_M, in Table 31.7. the smooth predicted logit RESPONSE and the smooth LSTORD_M, in Figure 31.3, is clearly nonlinear with expected changes in the logit RESPONSE: -0.0032, -0.1618, -0.1067, -0.0678, and 0.0175. does not hold. There is not a constant expected change in the logit RESPONSE as implied by the prescribed interpretation of the partial linear-RC for LSTORD_M, -0.0798 (in Equation 31.5). oeffic TabLe 31.7 ien Calculations for LRM Partial Quasi-RC(logit): LSTORD_M t-Fr med_ ee M min_ m2ax_ med_ LSTORD_M_ change_ med_ med_ change_ quasi-RC Slice LSTORD_M LSTORD_M LSTORD_M_r r+1 LSTORD_M lgt_r lgt_r+1 lgt (logit) ode 1 1 1 . . . 2 1 3 1 2 1 –3.2332 –3.2364 –0.0032 –0.0032 3 3 3 2 3 1 –3.2364 –3.3982 –0.1618 –0.1618 4 3 4 3 4 1 –3.3982 –3.5049 –0.1067 –0.1067 5 4 5 4 5 1 –3.5049 –3.5727 –0.0678 –0.0678 6 5 12 5 6 1 –3.5727 –3.5552 0.0175 0.0175 485 486 Statistical and Machine-Learning Data Mining Symbol is Value of Slice. 1 2 tgi d Lo –3.4 3 te Estima 4 6 5 –3.6 1 2 3 4 5 6 No. Months Since Last Order FIguRe 31.3 Visual display of LRM partial quasi-RC(logit) for LSTORD_M. 3. The secondary implication is that the structural form of LSTORD_M is not correct. The S-shaped nonlinear pattern suggests that qua- dratic or cubic reexpressions of LSTORD_M be tested for model inclusion.4. Satisfyingly, the partial quasi-RC(linear) value of -0.0799 (from the simple ordinary regression of the smooth predicted logit RESPONSE on the smooth LSTORD_M) equals the partial linear-RC value of -0.0798. The implications are as follows: (1) The partial linear-RC provides the average constant change in the logit RESPONSE across the LSTORD_M range of values from 1 to 66; (2) the partial quasi-RC provides more accurate reading of the changes with respect to the six presliced intervals across the LSTORD_M range. CELL and AGE_Y support the linearity assumption of the partial linear- RC. Thus, the partial quasi-RC(linear) and the partial linear-RC values should be equivalent. In fact, they are: For RFM_CELL, the partial quasi- RC(linear) and the partial-linear RC are -0.2007 and -0.1995, respectively; for AGE_Y, the partial quasi-RC(linear) and the partial-linear RC are 0.5409 and 0.5337, respectively. ology perform quite well on the linear predictions based on multiple predic- tor variables. Suffice it to say, by converting the logits into probabilities—as Interpretation of Coefficient-Free Models 487 was done in the simple logistic regression illustration in Section 31.3.3—the quasi-RC approach performs equally well with nonlinear predictions based on multiple predictor variables. The linear regression paradigm, with nearly two centuries of theoretical development and practical use, has made the equation form—the sum of weighted predictor variables (Y = b0 + b1X1 + b2X2 + … + bnXn)—the icon of predictive models. This is why the new machine-learning techniques of the last half-century are evaluated by the coefficients they produce. If the new coefficients impart comparable information to the regression coefficient, then the new technique passes the first line of acceptance. If not, the technique is all but summarily rejected. Ironically, some machine-learning methods offer better predictions without the use of coefficients. The burden of acceptance of the coefficient-free model lies with the extraction of something familiar and trusting. The quasi-RC procedure provides data analysts and marketers the comfort and security of coefficient-like information for evaluating and using the coefficient-free machine-learning models. quasi-RC method. One of the most popular coefficient-free models is the regression tree (e.g., CHAID [chi-squared automatic interaction detection].The regression tree has a unique equation form of “if … then” rules, which has rendered its interpretation virtually self-explanatory and has freed itself from a burden of acceptance. The need for coefficient-like information was never sought. In contrast, most machine-learning methods, like artificial neural networks, have not enjoyed an easy first line of acceptance. Even their proponents have called artificial neural networks black boxes. Ironically, artificial neural networks do have coefficients (actually, interconnection weights between input and output layers), but no formal effort has been made to translate them into coefficient-like information. The genetic GenIQ Model has no outright coefficients. Numerical values are sometimes part of the genetic model, but they are not coefficient-like in any way, just genetic material evolved as necessary for accurate prediction. nonlinear regression model. In the next section, I illustrate how the quasi- RC technique works and how to interpret its results for a quintessential everymodel, the nonregression-based, nonlinear, and coefficient-free GenIQ Model as presented in Chapter 29. As expected, the quasi-RC technique works with artificial neural network models and CHAID or CART regres- sion tree models. Statistical and Machine-Learning Data Mining 31.5.1 Illustration of Quasi-RC for a Coefficient-Free Model Again, consider the illustration in Chapter 30 of cataloger ABC, who requires a response model to be built on a recent mail campaign. I select the best number 3 GenIQ Model (in Figure 30.3) for predicting RESPONSE based on four predictor variables: 1. DOLLAR_2: dollars spent within last 2 years 2. PROD_TYP: number of different products 3. RFM_CELL: recency/frequency/money cells (1 = best to 5 = worst) 4. AGE_Y: knowledge of customer’s age (1 = if known; 0 = if not known) The GenIQ partial quasi-RC(prob) table and plot for DOLLAR_2 are in Table 31.8 and Figure 31.4, respectively. The plot of the relationship between the smooth predicted probability RESPONSE (GenIQ-converted probability score) and the smooth DOLLAR_2 is clearly nonlinear, which is considered reasonable, due to the inherently nonlinear nature of the GenIQ Model. The implication is that partial quasi-RC(prob) for DOLLAR_2 reliably reflects the expected changes in probability RESPONSE. The interpretation of the partial quasi-RC(prob) for DOLLAR_2 is as follows: For slice 2, which has minimum and maximum DOLLAR_2 values of 50 and 59, respectively, the partial quasi-RC(prob) is 0.000000310. This means that for each unit change in DOLLAR_2 between 50 and 59, the expected constant change in the prob- ability RESPONSE is 0.000000310. Similarly, for slices 3, 4, ¼ , 10, the expected constant changes in the probability RESPONSE are 0.000001450, 0.000001034, ¼ , 0.000006760, respectively.* The GenIQ partial quasi-RC(prob) table and plot for PROD_TYP are pre- sented in Table 31.9 and Figure 31.5, respectively. Because PROD_TYP assumes distinct values between 3 and 47, albeit more than a handful, I use 20 slices to take advantage of the granularity of the quasi-RC plotting. The interpretation of the partial quasi-RC(prob) for PROD_TYP can follow the literal rendition of “for each and every unit change” in PROD_TYP as done for DOLLAR_2. However, as the quasi-RC technique provides alternatives, the following interpretations are also available: 1. The partial quasi-RC plot of the relationship between the smooth predicted probability RESPONSE and the smooth PROD_TYP sug- gests two patterns. For pattern 1, for PROD_TYP values between 6 and 15, the unit changes in probability RESPONSE can be viewed as sample variation masking an expected constant change in * Note that the maximum values for DOLLAR_2 in Tables 31.6 and 31.8 are not equal. This is because they are based on different M-spread common regions as the GenIQ Model and LRM use different variables. o TabLe 31.8 effi Calculations for GenIQ Partial Quasi-RC(prob): DOLLAR_2 cien med_ med_ t-F min_ max_ DOLLAR DOLLAR_2_ change_ quasi-RC re Slice DOLLAR_2 DOLLAR_2 _2_r r+1 DOLLAR_2 med_prb_r med_prb_r+1 change_prb (prob) e M 1 0 50 . . . 2 50 59 40 50 10 0.031114713 0.031117817 0.000003103 0.000000310 els 3 59 73 50 67 17 0.031117817 0.031142469 0.000024652 0.000001450 4 73 83 67 79 12 0.031142469 0.031154883 0.000012414 0.000001034 5 83 94 79 89 10 0.031154883 0.031187925 0.000033043 0.000003304 6 94 110 89 102 13 0.031187925 0.031219393 0.000031468 0.000002421 7 110 131 102 119 17 0.031219393 0.031286803 0.000067410 0.000003965 8 131 159 119 144 25 0.031286803 0.031383536 0.000096733 0.000003869 9 159 209 144 182 38 0.031383536 0.031605964 0.000222428 0.000005853 10 209 480 182 253 71 0.031605964 0.032085916 0.000479952 0.000006760 489 490 Statistical and Machine-Learning Data Mining Symbol is Value of Slice. 10 0.0320 d Probability 9 0.0315 8 Estimate 6 7 3 4 5 1 2 0.0310 0 50 100 150 200 250 300 Dollars Spent w/2 yrs FIguRe 31.4 Visual display of GenIQ partial quasi-RC(prob) for DOLLAR_2. can be determined by the average of the unit changes in probability RESPONSE corresponding to PROD_TYP values between 6 and 15. change in probability RESPONSE is increasing in a nonlinear man- ner, which follows the literal rendition for each and every unit change in PROD_TYP. RC(prob) table or plot for PROD_TYP as much ado about sample variation, then the partial quasi-RC(linear) estimate can be used. Its value of 0.00002495 is obtained from the regression coefficient from the simple ordinary regression of the smooth predicted RESPONSE on the smooth PROD_TYP (columns 8 and 5, respectively). sented in Table 31.10 and Figure 31.6, respectively. The partial quasi-RC of the relationship between the smooth predicted probability RESPONSE and the smooth RFM_CELL suggests an increasing expected change in prob- ability. Recall that RFM_CELL is treated as an interval-level variable with a “reverse” scale: 1 = best to 5 = worst; thus, the RFM_CELL has clearly expected nonconstant change in probability. The plot has double smooth points at both RFM_CELL = 4 and RFM_CELL = 5, for which the double- smoothed predicted probability RESPONSE is taken as the average of the reported probabilities. For RFM_CELL = 4, the twin points are 0.031252 Interp TabLe 31.9 ret Calculations for GenIQ Partial Quasi-RC(prob): PROD_TYP atio min_ max_ med_ med_ change_ n of C PROD_ PROD_ PROD_ PROD_ PROD_ med_ med_prb_ quasi_RC Slice TYP TYP TYP_r TYP_r+1 TYP prb_r r+1 change_prob (prob) oe 1 3 6 . . . 2 6 7 6 7 1 0.031103 0.031108 0.000004696 0.000004696 ien 3 7 8 7 7 0 0.031108 0.031111 0.000003381 . 4 8 8 7 8 1 0.031111 0.031113 0.000001986 0.000001986 re 5 8 8 8 8 0 0.031113 0.031113 0.000000000 . 6 8 9 8 8 0 0.031113 0.031128 0.000014497 . 7 9 9 8 9 1 0.031128 0.031121 –0.000006585 –0.000006585 els 8 9 9 9 9 0 0.031121 0.031136 0.000014440 . 9 10 9 10 1 0.031136 0.031142 0.000006514 0.000006514 10 10 11 10 10 0 0.031142 0.031150 0.000007227 . 11 11 10 11 1 0.031150 0.031165 0.000015078 0.000015078 12 11 12 11 12 1 0.031165 0.031196 0.000031065 0.000031065 13 12 13 12 12 0 0.031196 0.031194 –0.000001614 . 13 14 12 13 1 0.031194 0.031221 0.000026683 0.000026683 15 14 15 13 14 1 0.031221 0.031226 0.000005420 0.000005420 16 15 16 14 15 1 0.031226 0.031246 0.000019601 0.000019601 17 16 19 15 17 2 0.031246 0.031305 0.000059454 0.000029727 18 19 22 17 20 3 0.031305 0.031341 0.000036032 0.000012011 19 22 26 20 24 4 0.031341 0.031486 0.000144726 0.000036181 20 26 47 24 30 6 0.031486 0.031749 0.000262804 0.000043801 491 492 Statistical and Machine-Learning Data Mining Symbol is Value of Slice. 20 0.0315 19 d Probability 18 17 0.0313 16 Estimate 13 14 15 10 12 6 7 8 9 1 23 45 0.0310 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 No. Different Products Purchased Note: Slices 1–5 are bunched up. Visual display of GenIQ partial quasi-RC(prob) for PROD_TYP. is 0.311945. Similarly, for RFM_CELL = 5, the double-smoothed predicted probability RESPONSE is 0.31204. The interpretation of the partial quasi- RC(prob) for RFM_CELL can follow the literal rendition for each and every unit change in RFM_CELL. sented in Table 31.11 and Figure 31.7, respectively. The partial quasi-RC plot of the relationship between the smooth predicted probability RESPONSE and the smooth RFM_CELL is an uninteresting expected linear change in probability. The plot has double - smoothed points at both AGE_Y = 1, for which the double-smoothed predicted probability RESPONSE is taken as the average of the reported probabilities. For AGE_Y = 1, the twin points are 0.031234 and 0.031192; thus, the double-smoothed predicted probabil- ity RESPONSE is 0.31213. The interpretation of the partial quasi-RC(prob) for AGE_Y can follow the literal rendition for each and every unit change in AGE_Y. In sum, this illustration shows how the quasi-RC methodology works on a nonregression-based, nonlinear, and coefficient-free model. The quasi-RC procedure provides data analysts and marketers with the sought-after com- fort and security of coefficient-like information for evaluating and using coefficient-free machine-learning models like GenIQ. oeffic TabLe 31.10 ient Calculations for GenIQ Partial Quasi-RC(prob): RFM_CELL -Fre min_ max_ med_ med_ e M RFM_ RFM_ RFM_ RFM_ change_ med_prb_ quasi-RC Slice CELL CELL CELL_r CELL_r+1 RFM_CELL med_prb_r r+1 change_prb (prob) ode 1 1 3 . . . 2 3 4 2 3 1 0.031773 0.031252 –0.000521290 –0.000521290 3 4 4 3 4 1 0.031252 0.031137 –0.000114949 –0.000114949 4 4 4 4 4 0 0.031137 0.031270 0.000133176 . 4 5 4 5 1 0.031270 0.031138 –0.000131994 –0.000131994 6 5 5 5 5 0 0.031138 0.031278 0.000140346 . 494 Statistical and Machine-Learning Data Mining Symbol is Value of Slice. 1 0.0316 d Probability 0.0314 6 2 4 Estimate 0.0312 3 5 0.0310 2 3 4 5 RFM Cells (1 = best to 5 = worse) FIguRe 31.6 Visual display of GenIQ partial quasi-RC(prob) for RFM_CELL. The redoubtable regression coefficient enjoys everyday use in marketing analysis and modeling. Model builders and marketers use the regres- sion coefficient when interpreting the tried-and-true regression model. workings of the linear statistical adjustment. This removes the effects of the other variables from the dependent variable and the predictor variable, producing a linear relationship between the dependent variable and the predictor variable. regression coefficient to evaluate new modeling methods. This leads to a quandary as some of the newer methods have no coefficients. As a counter- step, I presented the quasi-regression coefficient (quasi-RC), which provides information similar to the regression coefficient for evaluating and using coefficient-free models. Moreover, the quasi-RC serves as a trusty assump- tion-free alternative to the regression coefficient when the linearity assump- tion is not met. regression models to highlight the importance of the satisfaction of linearity assumption for accurate reading of the regression coefficient itself, as well as its effect on the predictions of the model. With these illustrations, I outlined Interpretation of C oefficient- TabLe 31.11 Free M Calculations for GenIQ Partial Quasi-RC(prob): AGE_Y med_ od min_ max_ med_ AGE_ change_ med_ med_prb_ quasi-RC els Slice AGE_Y AGE_Y AGE_Y_r Y_r+1 AGE_Y prb_r r+1 change_prb (prob) 1 0 1 . . . 1 1 1 1 0 0.031177 0.031192 0.000014687 . 1 1 1 1 0 0.031192 0.031234 0.000041677 . 496 Statistical and Machine-Learning Data Mining Symbol is Value of Slice. 3 0.03122 d Probability 0.03120 2 Estimate 0.03118 1 0.03116 0 1 AGE_Y FIguRe 31.7 Visual Display of GenIQ Partial Quasi RC(prob) for AGE_Y the method for calculating the quasi-RC. Comparison between the actual regression coefficient and the quasi-RC showed perfect agreement, which advances the trustworthiness of the new measure."
